<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>保姆级情感分类模型开发 | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/39.ac23927e.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/26.a658bfa8.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/61.68581797.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/65.f7077be9.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/77.0018ed7f.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/86.4180d39a.js"><link rel="prefetch" href="/assets/js/87.04e4ab18.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link router-link-active">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link router-link-active">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>保姆级情感分类模型开发</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#一、文本预处理-训练测试集的准备" class="sidebar-link">一、文本预处理&amp;训练测试集的准备</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#_1-数据集" class="sidebar-link">1.数据集</a></li><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#_2-文本数值化" class="sidebar-link">2.文本数值化</a></li><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#_3-划分训练集和测试集" class="sidebar-link">3.划分训练集和测试集</a></li></ul></li><li><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#二、搭建模型跑起来" class="sidebar-link">二、搭建模型跑起来</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#_1-模型的结构设计" class="sidebar-link">1.模型的结构设计</a></li><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91.html#_2-模型的搭建" class="sidebar-link">2. 模型的搭建</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="保姆级情感分类模型开发"><a href="#保姆级情感分类模型开发" class="header-anchor">#</a> 保姆级情感分类模型开发</h1> <p>Author: 郭必扬</p> <p>Time: 2019.04.26</p> <blockquote><p>情感分析是上手NLP的最简单的任务之一，它就是一个简单的文本分类问题，判断一段文本的情感极性。最简单的就是二分类，判断是积极的还是消极的；更难一点的就是三分类，除了积极消极还有无情感倾向的；更加复杂的就比如情感打分，例如电影打1~5分，这就是五分类。但本质上都一样，无非类别太多更难以学习罢了。</p> <p>IMDB是一个专业的电影评论网站，类似国内的豆瓣，IMDB的电影评论数据是大家经常使用来练手的情感分析数据集，也是各种比赛，如Kaggle，和各种学者做研究常用的数据集。</p> <p>本文尝试用这个数据做一个情感二分类，作为一个NLP的练手。具体涉及到：</p> <ol><li>文本预处理；</li> <li>预训练词向量的加载；</li> <li>采用RNNs训练模型</li></ol></blockquote> <p>数据集地址：http://ai.stanford.edu/~amaas/data/sentiment/</p> <p>本文采用Keras作为框架在进行模型搭建。</p> <h2 id="一、文本预处理-训练测试集的准备"><a href="#一、文本预处理-训练测试集的准备" class="header-anchor">#</a> 一、文本预处理&amp;训练测试集的准备</h2> <h3 id="_1-数据集"><a href="#_1-数据集" class="header-anchor">#</a> 1.数据集</h3> <h4 id="_1关于数据集"><a href="#_1关于数据集" class="header-anchor">#</a> ①关于数据集</h4> <p>其实，keras自带了IMDB的已经进行很好的预处理的数据集，可以一行代码下载，不需要进行任何的处理就可以训练，而且效果比较好。但是，这样就太没意思了。在真实场景中，我们拿到的都是脏脏的数据，我们必须自己学会读取、清洗、筛选、分成训练集测试集。而且，从我自己的实践经验来看，<strong>数据预处理的本事才是真本事，模型都好搭，现在的各种框架已经让搭建模型越来越容易，但是数据预处理只能自己动手</strong>。所有往往实际任务中，数据预处理花费的时间、精力是最多的，而且直接影响后面的效果。</p> <p>另外，我们要知道，对文本进行分析，首先要将文本数值化。因为计算机不认字的，只认数字。所以最后处理好的文本应该是数值化的形式。而<strong>keras自带的数据集全都数值化了，而它并不提供对应的查询字典让我们知道每个数字对应什么文字</strong>，这让我们只能训练模型，看效果，无法拓展到其他语料上，也无法深入分析。综上，我上面推荐的数据集，是原始数据集，都是真实文本，当然，为了方便处理，也已经被斯坦福的大佬分好类了。但是怎么数值化，需要我们自己动手。</p> <p>下载后解压，会看到有两个文件夹，<code>test</code>和<code>train</code>：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624615419472-image.png" alt=""></p> <p>我们点进<code>train</code>中，会发现正样本和负样本已经分好类了：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624615427248-image.png" alt=""></p> <p><code>neg</code>和<code>pos</code>分别是负样本和正样本，<code>unsup</code>是未标注的样本，可用后续需要采用。其他的都自己去看看吧。</p> <p>打开<code>pos</code>文件，看看里面啥样：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624615434265-image.png" alt=""></p> <p>都是一个个文本。</p> <p>注意到，这些文本一般都不短...</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624615441757-image.png" alt=""></p> <p>数据集中，共有5w条文本，test集和train集各半，每个集合中，pos和neg也是各半。</p> <p>当然，他们划分的train和test，你不一定真的要这样用。例如<strong>本文中，我为了方便，就吧train集合当做我所有的数据，在这2.5w条数据中再按照7:3划分train set和test set.</strong></p> <h4 id="_2导入数据集的代码"><a href="#_2导入数据集的代码" class="header-anchor">#</a> ②导入数据集的代码</h4> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> os
datapath <span class="token operator">=</span> <span class="token string">r'datasets\aclImdb_v1\train'</span>
pos_files <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>datapath<span class="token operator">+</span><span class="token string">'/pos'</span><span class="token punctuation">)</span>
neg_files <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>datapath<span class="token operator">+</span><span class="token string">'/neg'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>pos_files<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>neg_files<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>输出：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token number">12500</span>
<span class="token number">12500</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>所以我们总共有12500个正样本和12500个负样本。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
pos_all <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
neg_all <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> pf<span class="token punctuation">,</span>nf <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>pos_files<span class="token punctuation">,</span>neg_files<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>datapath<span class="token operator">+</span><span class="token string">'/pos'</span><span class="token operator">+</span><span class="token string">'/'</span><span class="token operator">+</span>pf<span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        s <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
        pos_all<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>datapath<span class="token operator">+</span><span class="token string">'/neg'</span><span class="token operator">+</span><span class="token string">'/'</span><span class="token operator">+</span>nf<span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        s <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
        neg_all<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>pos_all<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>neg_all<span class="token punctuation">)</span><span class="token punctuation">)</span>
X_orig <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pos_all<span class="token operator">+</span>neg_all<span class="token punctuation">)</span>
Y_orig <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">12500</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">12500</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;X_orig:&quot;</span><span class="token punctuation">,</span>X_orig<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Y_orig:&quot;</span><span class="token punctuation">,</span>Y_orig<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>上面代码的主要作用是把一个个样本放进正负样本对应的列表中，同时配上对应的label。代码很好理解。</p> <p>输出：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token number">12500</span>
<span class="token number">12500</span>
X_orig<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">25000</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
Y_orig<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">25000</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="_2-文本数值化"><a href="#_2-文本数值化" class="header-anchor">#</a> 2.文本数值化</h3> <h4 id="_1文本数值化的思路"><a href="#_1文本数值化的思路" class="header-anchor">#</a> ①文本数值化的思路</h4> <p>前面提到过，NLP问题比CV问题更难的一部分原因，就是文本都是离散化的数据，不像图像数据都是连续的数值数据，所以我们要想办法把一系列文本转化成一系列数字。</p> <p>这里的方法很多，我们这里采用的方法是，给词汇表中每一个词一个index，用index代替那个词。如一个语料库共有1w个词，那么就设置1w个index，每个词直接替换程index就行。</p> <p>但是，很多问题中，词汇量巨大，但是可能大部分词都是低频词，对训练模型的贡献很小，反而会严重拖累模型的训练。所以，一般我们可以分析一下文本词汇的词频分布特征，选取词频占大头的一批词就行了。</p> <p>例如，在本文的任务中，数据集共涉及到的词汇量有8~9w，这样训练起来会很慢。经过分析，发现大概2w个词就已经覆盖了绝大部分篇幅，所以我就选取词典大小为2w。然后，对文本数值化的时候，那些低频词就直接过滤掉了，只留下高频词。这样，模型训练起来效率就会大大提高。</p> <p><strong>词向量</strong></p> <p>如果你接触过词向量，那么一定会想到可以使用词向量吧文本转化成数值类型。不错，我们在本文中也会这么做。但是，如果直接吧文本转化成词向量，输入进模型的话，我们可能无法继续调优（fine-tune），词向量相当于是对文本的特征的一种表示，本身性质已经很好了。但是对于特定任务场景，我们一般都希望可以在训练好的词向量的基础上，继续用对应领域的数据对词向量进一步进行优化。所以，今天我们会探索，如果在加入词向量后，可以接着fine-tune。</p> <h4 id="_2文本数值化-词向量导入的代码"><a href="#_2文本数值化-词向量导入的代码" class="header-anchor">#</a> ②文本数值化，词向量导入的代码</h4> <p>keras自带的文本预处理的工具十分好用，具体可参加我单独写的一个短文：https://beyondguo.github.io/2019-03-18-Keras-Text-Preprocessing/</p> <p>我们设置词典大小为20000，文本序列最大长度为200.</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> text_to_word_sequence<span class="token punctuation">,</span>one_hot<span class="token punctuation">,</span>Tokenizer
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences
<span class="token keyword">import</span> time
vocab_size <span class="token operator">=</span> <span class="token number">20000</span>
maxlen <span class="token operator">=</span> <span class="token number">200</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Start fitting the corpus......&quot;</span><span class="token punctuation">)</span>
t <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span> <span class="token comment"># 要使得文本向量化时省略掉低频词，就要设置这个参数</span>
tik <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
t<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>X_orig<span class="token punctuation">)</span> <span class="token comment"># 在所有的评论数据集上训练，得到统计信息</span>
tok <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
word_index <span class="token operator">=</span> t<span class="token punctuation">.</span>word_index <span class="token comment"># 不受vocab_size的影响</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'all_vocab_size'</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_index<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Fitting time: &quot;</span><span class="token punctuation">,</span><span class="token punctuation">(</span>tok<span class="token operator">-</span>tik<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'s'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Start vectorizing the sentences.......&quot;</span><span class="token punctuation">)</span>
v_X <span class="token operator">=</span> t<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>X_orig<span class="token punctuation">)</span> <span class="token comment"># 受vocab_size的影响</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Start padding......&quot;</span><span class="token punctuation">)</span>
pad_X <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>v_X<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>maxlen<span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'post'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Finished!&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p><strong>上面的代码可以第一次读会比较难理解，这里稍微解释一下：</strong></p> <p><code>Tokenizer</code>是一个类，可以接收一个vocab_size的参数，也就是词典大小。设置了词典大小后，在后面生成文本的向量的时候，会把那些低频词（词频在20000开外的）都筛掉。</p> <p>定义了<code>Tokenizer</code>的一个实例<code>t</code>，然后调用方法<code>t.fit_on_texts(X_orig)</code>的作用，就是把我们所有的预料丢进去，让<code>t</code>去统计，它会帮你统计词频，给每个词分配index，形成字典等等。</p> <p>想获取index和词的对照字典的话，就使用<code>t.word_index</code>方法。注意，获取字典的时候，不会筛掉那些低频词，是所有词的一个字典。</p> <p>然后，想把一个句子、段落，转化成对应的index表示的向量怎么办呢？Tokenizer也提供了便捷的方法，不用你自己去慢慢查表，直接使用<code>t.texts_to_sequences(X_orig)</code>方法，就可以获取每句话的index组成的向量表示。注意，这里，就已经吧低频词给过滤掉了，比如一句话有100个词，其中有30个低频词，那么经过这个函数，得到的就是长度为70的一个向量。</p> <p>得到每个句子的向量后，会发现大家长度各有不同，长的长短的短，这样在后面的RNNs训练时，就不方便批处理。所以，我们还需要对句子进行一个padding（填白，补全），把所有句子弄程统一长度，短的补上0，长的切掉。用的方法就是<code>pad_sequences</code>。</p> <p>上面代码的输出是：</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code>Start fitting the corpus<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
all_vocab_size <span class="token number">88582</span>
Fitting time<span class="token punctuation">:</span>  <span class="token number">9.10555362701416</span> s
Start vectorizing the sentences<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
Start padding<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
Finished!
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到，我们2.5w个文本，几百万词，丢进去统计，效率还是挺高的，不到10秒就统计好了。</p> <p>刚刚说了，获取字典的时候，不会筛掉那些低频词，是所有词的一个字典。但后面我们需要只保留那些高频词的一个字典，所以需要进行这样一个操作，形成一个高频词字典：</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> copy
x <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>word_counts<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
s <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>key<span class="token operator">=</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span>p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
small_word_index <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>word_index<span class="token punctuation">)</span> <span class="token comment"># 防止原来的字典也被改变了</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Removing less freq words from word-index dict...&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> item <span class="token keyword">in</span> s<span class="token punctuation">[</span><span class="token number">20000</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    small_word_index<span class="token punctuation">.</span>pop<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Finished!&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>small_word_index<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_index<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>输出：</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code>Removing less freq words <span class="token keyword">from</span> word<span class="token operator">-</span>index <span class="token builtin">dict</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
Finished!
<span class="token number">20000</span>
<span class="token number">88582</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><strong>词向量的导入：</strong></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> gensim
model_file <span class="token operator">=</span> <span class="token string">'../big_things/w2v/GoogleNews-vectors-negative300.bin'</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Loading word2vec model......&quot;</span><span class="token punctuation">)</span>
wv_model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span>model_file<span class="token punctuation">,</span>binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这里采用Google发布的使用GoogleNews进行训练的一个300维word2vec词向量。这个读者可以自行去网上下载。如果无法下载，可以到公众号留言申请。</p> <p>现在，我们需要把这个词向量，跟我们本任务中的词汇的index对应起来，也就是构建一个<code>embedding matrix</code>这样就可以通过index找到对应的词向量了。方法也很简单：</p> <p>先随机初始化一个embedding matrix，这里需要注意的是，我们的词汇量vocab_size虽然是20000，但是训练的时候还是会碰到不少词不在词汇表里，也在词向量也查不到，那这些词怎么处理呢？我们就需要单独给这些未知词（UNK）一个index，在keras的文本预处理中，会默认保留index=0给这些未知词。</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code>embedding_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>vocab_size<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># +1是要留一个给index=0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Transfering to the embedding matrix......&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># sorted_small_index = sorted(list(small_word_index.items()),key=lambda x:x[1])</span>
<span class="token keyword">for</span> word<span class="token punctuation">,</span>index <span class="token keyword">in</span> small_word_index<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        word_vector <span class="token operator">=</span> wv_model<span class="token punctuation">[</span>word<span class="token punctuation">]</span>
        embedding_matrix<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> word_vector
    <span class="token keyword">except</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Word: [&quot;</span><span class="token punctuation">,</span>word<span class="token punctuation">,</span><span class="token string">&quot;] not in wvmodel! Use random embedding instead.&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Finished!&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Embedding matrix shape:\n&quot;</span><span class="token punctuation">,</span>embedding_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>通过上面的操作，所有的index都对应上了词向量，那些不在word2vec中的词和index=0的词，词向量就是随机初始化的值。</p> <h3 id="_3-划分训练集和测试集"><a href="#_3-划分训练集和测试集" class="header-anchor">#</a> 3.划分训练集和测试集</h3> <p>划分训练集和测试集，当然使用经典的sklearn的<code>train_test_split</code>了。</p> <p>废话少说，直接上代码：</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed <span class="token operator">=</span> <span class="token number">1</span>
random_indexs <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>permutation<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>pad_X<span class="token punctuation">)</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> pad_X<span class="token punctuation">[</span>random_indexs<span class="token punctuation">]</span>
Y <span class="token operator">=</span> Y_orig<span class="token punctuation">[</span>random_indexs<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span>Y<span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;X_train:&quot;</span><span class="token punctuation">,</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y_train:&quot;</span><span class="token punctuation">,</span>y_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;X_test:&quot;</span><span class="token punctuation">,</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y_test:&quot;</span><span class="token punctuation">,</span>y_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>y_train<span class="token punctuation">)</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>y_train<span class="token punctuation">)</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>输出：</p> <div class="language-Python line-numbers-mode"><pre class="language-python"><code><span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span>
 <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span><span class="token punctuation">]</span>
X_train<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">20000</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">)</span>
y_train<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">20000</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
X_test<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">)</span>
y_test<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token number">9982</span>
<span class="token number">10018</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>训练样本2w，测试样本5k.</p> <p>唯一值得注意的一点就是，由于前面我们加载数据集的时候，正样本和负样本都聚在一块，所以我们在这里要把他们<strong>随机打乱</strong>一下，用的就是numpy的<code>random.permutation</code>方法。这些都是惯用伎俩了。</p> <blockquote><p>恭喜！您已阅读本文80%的内容！</p></blockquote> <h2 id="二、搭建模型跑起来"><a href="#二、搭建模型跑起来" class="header-anchor">#</a> 二、搭建模型跑起来</h2> <p>做完了数据的预处理，后面的东西，就都是小菜一碟了。那么多框架是干嘛的？就是为了让你用尽可能少的代码把那些无聊的事情给做了！Keras尤其如此。</p> <h3 id="_1-模型的结构设计"><a href="#_1-模型的结构设计" class="header-anchor">#</a> 1.模型的结构设计</h3> <p>处理NLP问题，最常用的模型的就是RNN系列，LSTM和GRU随便用。然后，一般还会在前面加一个embedding层。</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624615502290-image.png" alt=""></p> <p>之前我一直以为embedding层就是把预训练好的词向量加进去，实际上不是。即使没有训练好的词向量，我们也可以使用embedding层。因为我们可以用我们的训练数据，来训练出词的embedding，只不过这个embedding不同于word2vec的那种表达词的含义的embedding，更多的是针对特定场景下的一个embedding。（不知道这样说有没有说清楚...）</p> <p>所以，我们直接配置一个embedding层，不提供词向量都可以训练。如果提供了词向量，这样可以加速我们的训练，相当于我们已经有一个训练好的参数，提供给了模型，模型无非就需要接着改一改即可，而不是从一个随机的状态来慢慢训练。</p> <h3 id="_2-模型的搭建"><a href="#_2-模型的搭建" class="header-anchor">#</a> 2. 模型的搭建</h3> <p>Talk is cheap, the code below is also cheap:</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> keras
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token punctuation">,</span>Model
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Input<span class="token punctuation">,</span>Dense<span class="token punctuation">,</span>GRU<span class="token punctuation">,</span>LSTM<span class="token punctuation">,</span>Activation<span class="token punctuation">,</span>Dropout<span class="token punctuation">,</span>Embedding
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Multiply<span class="token punctuation">,</span>Concatenate<span class="token punctuation">,</span>Dot

inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
use_pretrained_wv <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token keyword">if</span> use_pretrained_wv<span class="token punctuation">:</span>
    wv <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>VOCAB_SIZE<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>wv_dim<span class="token punctuation">,</span>input_length<span class="token operator">=</span>MAXLEN<span class="token punctuation">,</span>weights<span class="token operator">=</span><span class="token punctuation">[</span>embedding_matrix<span class="token punctuation">]</span><span class="token punctuation">)</span>  	<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    wv <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>VOCAB_SIZE<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>wv_dim<span class="token punctuation">,</span>input_length<span class="token operator">=</span>MAXLEN<span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

h <span class="token operator">=</span> LSTM<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">(</span>wv<span class="token punctuation">)</span>
y <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>h<span class="token punctuation">)</span>
m <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>inputs<span class="token punctuation">,</span>output<span class="token operator">=</span>y<span class="token punctuation">)</span>
m<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>

m<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
m<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>validation_split<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>从上面的代码可以知道，想要把预训练的word2vec词向量加入到模型中，就是把词向量作为embedding层的参数（weights），具体我们需要先构建一个<code>embedding matrix</code>，这个我们在前面已经构建好了，然后传进embedding层即可。</p> <p>运行！输出：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         (None, 200)               0         
_________________________________________________________________
embedding_7 (Embedding)      (None, 200, 128)          2560128   
_________________________________________________________________
lstm_7 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 129       
=================================================================
Total params: 2,691,841
Trainable params: 2,691,841
Non-trainable params: 0
_________________________________________________________________
Train on 17000 samples, validate on 3000 samples
Epoch 1/3
17000/17000 [==============================] - 178s 10ms/step - loss: 0.6711 - acc: 0.5692 - val_loss: 0.6701 - val_acc: 0.5697
Epoch 2/3
17000/17000 [==============================] - 168s 10ms/step - loss: 0.5964 - acc: 0.6479 - val_loss: 0.5072 - val_acc: 0.7940
Epoch 3/3
17000/17000 [==============================] - 169s 10ms/step - loss: 0.5104 - acc: 0.7171 - val_loss: 0.4976 - val_acc: 0.7943
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>可以发现，参数的大部分，都是embedding层的参数。所以，读者可以尝试一下将词向量参数固定，可以发现训练速度会快得多。但是效果可能会略差一些。</p> <p><strong>建议读者对比一下：</strong></p> <p>①不使用word2vec作为embedding的参数</p> <p>②使用word2vec作为embedding的参数并固定参数</p> <p>③使用word2vec作为embedding的参数并继续fine-tune</p> <p>相信会有一些有意思的发现。</p> <p>但是你可能没时间（<s>多半是懒！</s>），所以这里我也告诉大家我的实验结果：</p> <p>①效果最差，时间最长</p> <p>②效果最好，时间较长</p> <p>③效果中等，时间最快</p> <hr> <blockquote><p>本文带着读者详细的了解了使用keras进行文本预处理，如何将词向量加入到训练模型中提升性能，动手的读者更可以体会到不同词向量使用方法的差别。</p> <p>这里，我们差不多直观上感受到了NLP是啥感觉，<strong>后面的文章</strong>，会主要探讨一下<strong>Attention机制</strong>在这个基础上的应用，然后我们还会尝试<strong>使用CNN</strong>来做一下同样的任务，看看效果如何。相信我们会有新的发现！</p></blockquote></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/39.ac23927e.js" defer></script>
  </body>
</html>
