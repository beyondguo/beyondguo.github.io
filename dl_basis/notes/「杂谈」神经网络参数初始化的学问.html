<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>「杂谈」神经网络参数初始化的学问 | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/26.a658bfa8.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/39.ac23927e.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/61.68581797.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/65.f7077be9.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/77.0018ed7f.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/86.4180d39a.js"><link rel="prefetch" href="/assets/js/87.04e4ab18.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link router-link-active">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link router-link-active">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>「杂谈」神经网络参数初始化的学问</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#一、直接把参数都初始化为0" class="sidebar-link">一、直接把参数都初始化为0</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#二、随机初始化" class="sidebar-link">二、随机初始化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#_2放大版随机初始化" class="sidebar-link">②放大版随机初始化</a></li><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#_3增大迭代次数" class="sidebar-link">③增大迭代次数</a></li><li class="sidebar-sub-header"><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#_4缩小版随机初始化" class="sidebar-link">④缩小版随机初始化</a></li></ul></li><li><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#" class="sidebar-link">/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#三、he-initialization" class="sidebar-link">三、He Initialization</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/dl_basis/notes/%E3%80%8C%E6%9D%82%E8%B0%88%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%AD%A6%E9%97%AE.html#总结一下" class="sidebar-link">总结一下：</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="「杂谈」神经网络参数初始化的学问"><a href="#「杂谈」神经网络参数初始化的学问" class="header-anchor">#</a> 「杂谈」神经网络参数初始化的学问</h1> <blockquote><p>我们已经知道，神经网络的参数主要是权重（weights）：W， 和偏置项（bias）：b。
训练神经网络的时候需先给定一个初试值，才能够训练，然后一点点地更新，但是<strong>不同的初始化方法，训练的效果可能会截然不同</strong>。本文主要记录一下不同的初始化的方法，以及相应的效果。</p></blockquote> <p>笔者正在学习的Andrew Ng的DeepLearning.ai提供了相应的模型框架和数据，我们这里要自己设置的就是不同的初值。</p> <p>数据可视化之后是这样的：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624598961457-image.png" alt=""></p> <p>我们需要做的就是把上面的红点和蓝点分类。</p> <h2 id="一、直接把参数都初始化为0"><a href="#一、直接把参数都初始化为0" class="header-anchor">#</a> 一、直接把参数都初始化为0</h2> <p>这是大家可以想到的最简单的方法，也确实很多其他的地方都采用0初值，那神经网络中这样做是否可行呢？
在python中，可以用***np.zeros((维度))*** 来给一个向量/矩阵赋值0，
于是，对于L层神经网络，可这样进行0-initialization：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>for l in range(1,L): #总共L层，l为当前层
    W = np.zeros((num_of_dim[l],num_of_dim[l-1])) # W的维度是（当前层单元数，上一层单元数）
    b = np.zeros((num_of_dim[l],1)) # b的维度是（当前层单元数，1）
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>通过这样的初值，我们run一下模型，得到的cost-iteration曲线以及在训练集、测试集上面的准确率如下：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624598999800-image.png" alt=""></p> <p>可以发现，<strong>压根就没训练</strong>！得到的模型跟瞎猜没有区别。</p> <h4 id="为什么呢"><a href="#为什么呢" class="header-anchor">#</a> 为什么呢？</h4> <p>我们看看神经网络的结构图：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599015156-image.png" alt=""></p> <p>这是一个3层神经网络，可以看出，神经网络结构是十分<strong>对称</strong>的，不管有几层。
当我们把所有的参数都设成0的话，那么上面的每一条边上的权重就都是0，那么神经网络就还是对称的，对于同一层的每个神经元，它们就一模一样了。
这样的后果是什么呢？我们知道，<strong>不管是哪个神经元，它的前向传播和反向传播的算法都是一样的，如果初始值也一样的话，不管训练多久，它们最终都一样，都无法打破对称（fail to break the symmetry）</strong>,那每一层就相当于只有一个神经元，<strong>最终L层神经网络就相当于一个线性的网络</strong>，如Logistic regression，线性分类器对我们上面的非线性数据集是“无力”的，所以最终训练的结果就瞎猜一样。</p> <p>因此，我们决不能把所有参数初始化为0，同样也不能初始化为任何相同的值，因为我们必须“<strong>打破对称性</strong>”！</p> <h2 id="二、随机初始化"><a href="#二、随机初始化" class="header-anchor">#</a> 二、随机初始化</h2> <p>好，不用0，咱们随机给一批值总可以吧。确实可以！咱们看看：
【下面的演示会试试多种参数或超参数，为了方便大家看，我分4步：①②③④】</p> <p>####①随机初始化
python中，随机初始化可以用 <em><strong>np.random.randn(维度)</strong></em> 来随机赋值：
于是前面的代码改成：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>for l in range(1,L): #总共L层，l为当前层
    W = np.random.randn(num_of_dim[l],num_of_dim[l-1]) # W的维度是（当前层单元数，上一层单元数）
    b = np.zeros((num_of_dim[l],1)) # b的维度是（当前层单元数，1）
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>这里有三点需要说明一下：</strong></p> <ol><li>b不用随机初始化，因为w随机之后，已经打破对称，b就一个常数，无所谓了</li> <li>random.rand()是在0~1之间随机，random.randn()是标准正态分布中随机，有正有负</li> <li>np.zeros(())这里是两个括号，random.randn()是一个括号，奇怪的很，就记着吧</li></ol> <p>那看看run出来的效果如何呢：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599026562-image.png" alt=""></p> <p><strong>效果明显比0初始化要好多了</strong>，cost最后降的也比较低，准确率也不错，92%。给分类效果可视化：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599036100-image.png" alt=""></p> <p>我们接着试试，如果把随机初始化的值放大一点会出现什么：</p> <h3 id="_2放大版随机初始化"><a href="#_2放大版随机初始化" class="header-anchor">#</a> ②放大版随机初始化</h3> <div class="language- line-numbers-mode"><pre class="language-text"><code>for l in range(1,L): #总共L层，l为当前层
    W = np.random.randn(num_of_dim[l],num_of_dim[l-1])*10 # W的维度是（当前层单元数，上一层单元数）
    b = np.zeros((num_of_dim[l],1)) # b的维度是（当前层单元数，1）
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>上面的代码中，我们给W最后多<strong>乘以10</strong>，run的效果：
<strong>【注意啊，乘以10不一定就是变大，因为我们的w的随机取值可正可负，所以乘以10之后，正数更大，负数更小】</strong></p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599045935-image.png" alt=""></p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599053039-image.png" alt=""></p> <p>咦~~ 真o心 ~~</p> <p>准确率明显降低了许多，到86%。</p> <p>####为什么把随机初始化的值放大就不好了呢？</p> <p>我们看看神经网络中常用的sigmoid函数：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599063592-image.png" alt=""></p> <p>这家伙，中间的斜率大，两边的斜率小还趋于零。所以当我们把随机的值乘以10了之后，我们的初值会往两边跑，那么我们的<strong>梯度下降就会显著变慢，可能迭代半天，才下降一点点。</strong></p> <p>这就是问题的症结。</p> <p>我们上面的实验，可以从图的横坐标看出，都是设定的一样的迭代次数（iteration number）：15000次，因此，<strong>在相同的迭代次数下，放大版的随机初始化的模型的学习就像一个“笨学生”，没别人学的多，因此效果就更差</strong>。</p> <p>为了验证我说的，我们可以试试吧迭代次数加大，看看我说的是不是对的：</p> <h3 id="_3增大迭代次数"><a href="#_3增大迭代次数" class="header-anchor">#</a> ③增大迭代次数</h3> <p>测试了好久。。。
然后打脸了。。。</p> <p>不过还是值得玩味~~</p> <p>我把迭代次数显示设为60000，也就是增大了4,5倍，结果cost function后来下降十分十分缓慢，最后效果还不如之前的。然后我再把<strong>迭代次数增加到了160000</strong>，相当于比一开始增大了10倍多，结果....</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599072182-image.png" alt=""></p> <p>可以看到，cost基本从20000次迭代之后就稳定了，怎么都降不下去了，实际上是在降低，但是十分十分十分X10地缓慢。难道这就是传说中的梯度消失？？？
所以结果并没有我想象地把迭代次数加大，就可以解决这个问题，实际上，可以看到，在训练集上准确度确实上升了，所以说明确实模型有所改进，只不过改进的太缓慢，相当于没有改进。</p> <p>仔细分析了一下，由于W太大或者太小，导致激活函数对w的倒数趋于零，那么计算cost对w的导数也会趋于零，所以下降如此缓慢也是可以理解。</p> <p>好，放大的效果如此差，我们缩小试试？</p> <h3 id="_4缩小版随机初始化"><a href="#_4缩小版随机初始化" class="header-anchor">#</a> ④缩小版随机初始化</h3> <p>还是回到迭代14000次，这次把<strong>w除以10</strong>看看：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599085309-image.png" alt=""></p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599093300-image.png" alt=""></p> <p>嘿~缩小结果甚至更差！连圈圈都没有了。</p> <p>上面这个图，说明学习到的模型太简单了，因为我们把<strong>w都除以10，实际上就接近0了</strong>，深度学习中我们认为<strong>参数越大，模型越复杂；参数越小，模型越简单</strong>。所以除以10之后，参数太小了，模型就too simple了，效果当然不好。</p> <p><strong>最后再试一次吧，再多的话大家都烦了我也烦了。</strong></p> <p>上面乘以10和除以10，效果都很差，那我们试一个中间的，比如：<strong>除以3</strong>（真的是随便试试）</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599103632-image.png" alt=""></p> <p>可见，只要找到一个恰当的值来缩小，是可以提高准确率的。但是，这里除以三是我拍脑门出来的，不能每次都这么一个个地试吧，<strong>有没有一个稳健的，通用的</strong>方法呢？</p> <p>有！接着看：</p> <h2 id=""><a href="#" class="header-anchor">#</a></h2> <h2 id="三、he-initialization"><a href="#三、he-initialization" class="header-anchor">#</a> 三、He Initialization</h2> <p>上面试了各种方法，放大缩小都不好，无法把握那个度。还好，总有大神为我们铺路，何凯明大佬提出了一种方法，我们称之为<strong>He Initialization</strong>，它就是在我们随机初始化了之后，<strong>乘以</strong>!
<img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599230018-image.png" alt=""></p> <p>这样就避免了参数的初始值过大或者过小，因此可以取得比较好的效果，代码也很简单，用***np.sqrt()***来求平方根：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>for l in range(1,L): #总共L层，l为当前层
    W = np.random.randn(num_of_dim[l],num_of_dim[l-1])**np.sqrt(2/num_of_dim[l-1]) # W的维度是（当前层单元数，上一层单元数）
    b = np.zeros((num_of_dim[l],1)) # b的维度是（当前层单元数，1）
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>取得的效果如下：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599239506-image.png" alt=""></p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599246127-image.png" alt=""></p> <p>啧啧啧，看这效果，看这优美的损失曲线，看着卓越的准确率... ...</p> <p><strong>以后就用你了，He Initialization ！</strong></p> <p>其实吧，He Initialization是推荐针对使用<strong>ReLU</strong>激活函数的神经网络使用的，不过对其他的激活函数，效果也不错。</p> <p>还有其他的类似的一些好的初始化方法，例如：</p> <p>推荐给sigmoid的<strong>Xavier Initialization</strong>：随机化之后<strong>乘以</strong></p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599262151-image.png" alt=""></p> <h2 id="总结一下"><a href="#总结一下" class="header-anchor">#</a> 总结一下：</h2> <ul><li>神经网络不可用0来初始化参数！</li> <li>随机赋值是为了打破对称性，使得不同的神经元可以有不同的功能</li> <li>推荐在初始化的时候使用He Initialization</li></ul></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/26.a658bfa8.js" defer></script>
  </body>
</html>
