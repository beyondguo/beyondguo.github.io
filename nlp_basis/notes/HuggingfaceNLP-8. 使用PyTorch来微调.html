<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Huggingface🤗NLP笔记8：使用PyTorch来微调模型 | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/65.f7077be9.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/26.a658bfa8.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/39.ac23927e.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/61.68581797.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/77.0018ed7f.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/86.4180d39a.js"><link rel="prefetch" href="/assets/js/87.04e4ab18.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link router-link-active">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link router-link-active">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Huggingface🤗NLP笔记8：使用PyTorch来微调模型</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/nlp_basis/notes/HuggingfaceNLP-8.%20%E4%BD%BF%E7%94%A8PyTorch%E6%9D%A5%E5%BE%AE%E8%B0%83.html#_1-数据集预处理" class="sidebar-link">1. 数据集预处理</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-8.%20%E4%BD%BF%E7%94%A8PyTorch%E6%9D%A5%E5%BE%AE%E8%B0%83.html#_2-模型" class="sidebar-link">2. 模型</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-8.%20%E4%BD%BF%E7%94%A8PyTorch%E6%9D%A5%E5%BE%AE%E8%B0%83.html#定义-optimizer-和-learning-rate-scheduler" class="sidebar-link">定义 optimizer 和 learning rate scheduler</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-8.%20%E4%BD%BF%E7%94%A8PyTorch%E6%9D%A5%E5%BE%AE%E8%B0%83.html#_3-training" class="sidebar-link">3. Training</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-8.%20%E4%BD%BF%E7%94%A8PyTorch%E6%9D%A5%E5%BE%AE%E8%B0%83.html#编写pytorch-training-loops" class="sidebar-link">编写pytorch training loops:</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-8.%20%E4%BD%BF%E7%94%A8PyTorch%E6%9D%A5%E5%BE%AE%E8%B0%83.html#_4-evaluation" class="sidebar-link">4. Evaluation</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><blockquote><p><strong>「Huggingface🤗NLP笔记系列-第8集」</strong>
Huggingface初级教程完结撒花！🌸🌼ヽ(°▽°)ノ🌸🌺
最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的<strong>精简+注解版</strong>。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。</p></blockquote> <ul><li>官方教程网址：https://huggingface.co/course/chapter1</li> <li>本期内容对应网址：https://huggingface.co/course/chapter3/4?fw=pt</li> <li>本系列笔记的<strong>GitHub Notebook(可下载直接运行)</strong>： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP</li></ul> <hr> <h1 id="更加透明的方式-使用pytorch来微调模型"><a href="#更加透明的方式-使用pytorch来微调模型" class="header-anchor">#</a> 更加透明的方式——使用PyTorch来微调模型</h1> <p>这里我们不使用Trainer这个高级API，而是用pytorch来实现。</p> <h2 id="_1-数据集预处理"><a href="#_1-数据集预处理" class="header-anchor">#</a> 1. 数据集预处理</h2> <p>在Huggingface官方教程里提到，在使用pytorch的dataloader之前，我们需要做一些事情：</p> <ul><li>把dataset中一些不需要的列给去掉了，比如‘sentence1’，‘sentence2’等</li> <li>把数据转换成pytorch tensors</li> <li>修改列名 label 为 labels</li></ul> <p>其他的都好说，但<strong>为啥要修改列名 label 为 labels，好奇怪哦！</strong>
这里探究一下：</p> <p>首先，Huggingface的这些transformer Model直接call的时候，接受的标签这个参数是叫&quot;labels&quot;。
所以不管你使用Trainer，还是原生pytorch去写，最终模型处理的时候，肯定是使用的名为&quot;labels&quot;的标签参数。</p> <p>但在Huggingface的datasets中，数据集的标签一般命名为&quot;label&quot;或者&quot;label_ids&quot;，那为什么在前两集中，我们没有对标签名进行处理呢？</p> <p>这一点在transformer的源码<code>trainer.py</code>里找到了端倪：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 位置在def _remove_unused_columns函数里</span>
<span class="token comment"># Labels may be named label or label_ids, the default data collator handles that.</span>
signature_columns <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;label_ids&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这里提示了， data collator 会负责处理标签问题。然后我又去查看了<code>data_collator.py</code>中发现了一下内容：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DataCollatorWithPadding</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Union<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        <span class="token keyword">if</span> <span class="token string">&quot;label&quot;</span> <span class="token keyword">in</span> batch<span class="token punctuation">:</span>
            batch<span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">]</span>
            <span class="token keyword">del</span> batch<span class="token punctuation">[</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> <span class="token string">&quot;label_ids&quot;</span> <span class="token keyword">in</span> batch<span class="token punctuation">:</span>
            batch<span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">&quot;label_ids&quot;</span><span class="token punctuation">]</span>
            <span class="token keyword">del</span> batch<span class="token punctuation">[</span><span class="token string">&quot;label_ids&quot;</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> batch
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>这就真相大白了：不管数据集中提供的标签名叫&quot;label&quot;，还是&quot;label_ids&quot;，
DataCollatorWithPadding 都会帮你转换成&quot;labels&quot;，装进batch里，再返回。</p> <p>前面使用Trainer的时候，DataCollatorWithPadding已经帮我们自动转换了，因此我们不需要操心这个问题。</p> <p>但这就是让我疑惑的地方：我们使用pytorch来写，其实也不用管这个，因为在pytorch的data_loader里面，有一个<code>collate_fn</code>参数，我们可以把DataCollatorWithPadding对象传进去，也会帮我们自动把&quot;label&quot;转换成&quot;labels&quot;。因此实际上，<strong>这应该是教程中的一个小错误，我们不需要手动设计</strong>（前两天在Huggingface GitHub上提了issue，作者回复我确实不用手动设置）。</p> <hr> <p>下面开始正式使用pytorch来训练：</p> <p>首先是跟之前一样，我们需要加载数据集、tokenizer，然后把数据集通过map的方式进行预处理。我们还需要定义一个<code>data_collator</code>方便我们后面进行批量化处理模型：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding

raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>查看一下处理后的dataset：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">print</span><span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
<span class="token punctuation">[</span><span class="token string">'attention_mask'</span>, <span class="token string">'idx'</span>, <span class="token string">'input_ids'</span>, <span class="token string">'label'</span>, <span class="token string">'sentence1'</span>, <span class="token string">'sentence2'</span>, <span class="token string">'token_type_ids'</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>huggingface datasets贴心地准备了三个方法：<code>remove_columns</code>, <code>rename_column</code>, <code>set_format</code></p> <p>来方便我们为pytorch的Dataloader做准备：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>remove_columns<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">,</span> <span class="token string">'sentence2'</span><span class="token punctuation">,</span><span class="token string">'idx'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># tokenized_datasets = tokenized_datasets.rename_column('label','labels')  # 实践证明，这一行是不需要的</span>
tokenized_datasets<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">'torch'</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
<span class="token punctuation">[</span><span class="token string">'attention_mask'</span>, <span class="token string">'input_ids'</span>, <span class="token string">'label'</span>, <span class="token string">'token_type_ids'</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>查看一下：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span>  <span class="token comment"># 经过上面的处理，它就可以直接丢进pytorch的Dataloader中了，跟pytorch中的Dataset格式已经一样了</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
    features: <span class="token punctuation">[</span><span class="token string">'attention_mask'</span>, <span class="token string">'input_ids'</span>, <span class="token string">'label'</span>, <span class="token string">'token_type_ids'</span><span class="token punctuation">]</span>,
    num_rows: <span class="token number">3668</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>定义我们的<strong>pytorch dataloaders</strong>：</p> <p>在pytorch的<code>DataLoader</code>里，有一个<code>collate_fn</code>参数，其定义是：&quot;merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset.&quot; 我们可以直接把Huggingface的<code>DataCollatorWithPadding</code>对象传进去，用于对数据进行padding等一系列处理：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset
train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>data_collator<span class="token punctuation">)</span>  <span class="token comment"># 通过这里的dataloader，每个batch的seq_len可能不同</span>
eval_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>data_collator<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 查看一下train_dataloader的元素长啥样</span>
<span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
    <span class="token keyword">break</span>
<span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>shape <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
<span class="token comment"># 可见都是长度为72，size=8的batch</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
<span class="token punctuation">{</span><span class="token string">'attention_mask'</span><span class="token builtin class-name">:</span> torch.Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span>, <span class="token number">72</span><span class="token punctuation">]</span><span class="token punctuation">)</span>,
 <span class="token string">'input_ids'</span><span class="token builtin class-name">:</span> torch.Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span>, <span class="token number">72</span><span class="token punctuation">]</span><span class="token punctuation">)</span>,
 <span class="token string">'token_type_ids'</span><span class="token builtin class-name">:</span> torch.Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span>, <span class="token number">72</span><span class="token punctuation">]</span><span class="token punctuation">)</span>,
 <span class="token string">'labels'</span><span class="token builtin class-name">:</span> torch.Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>观察一下经过DataLoader处理后的数据，我们发现，标签那一列的列名，已经从<code>&quot;label&quot;</code>变为<code>&quot;labels&quot;</code>了！</p> <h2 id="_2-模型"><a href="#_2-模型" class="header-anchor">#</a> 2. 模型</h2> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>前面dataloader出来的batch可以直接丢进模型处理：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span> 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
SequenceClassifierOutput<span class="token punctuation">(</span>loss<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token number">0.7563</span>, <span class="token assign-left variable">grad_fn</span><span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>, <span class="token assign-left variable">logits</span><span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>-0.2171, -0.4416<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2248, -0.4694<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2440, -0.4664<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2421, -0.4510<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2273, -0.4545<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2339, -0.4515<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2334, -0.4387<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>-0.2362, -0.4601<span class="token punctuation">]</span><span class="token punctuation">]</span>, <span class="token assign-left variable">grad_fn</span><span class="token operator">=</span><span class="token operator">&lt;</span>AddmmBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>, <span class="token assign-left variable">hidden_states</span><span class="token operator">=</span>None, <span class="token assign-left variable">attentions</span><span class="token operator">=</span>None<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h2 id="定义-optimizer-和-learning-rate-scheduler"><a href="#定义-optimizer-和-learning-rate-scheduler" class="header-anchor">#</a> 定义 optimizer 和 learning rate scheduler</h2> <p>按道理说，Huggingface这边提供Transformer模型就已经够了，具体的训练、优化，应该交给pytorch了吧。但鉴于Transformer训练时，最常用的优化器就是AdamW，这里Huggingface也直接在<code>transformers</code>库中加入了<code>AdamW</code>这个优化器，还贴心地配备了lr_scheduler，方便我们直接使用。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> get_scheduler

optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">5e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span>

num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>  <span class="token comment"># num of batches * num of epochs</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">'linear'</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>  <span class="token comment"># scheduler是针对optimizer的lr的</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><div class="language- extra-class"><pre><code>1377
</code></pre></div><h2 id="_3-training"><a href="#_3-training" class="header-anchor">#</a> 3. Training</h2> <p>首先，我们设置cuda device，然后把模型给移动到cuda上：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h2 id="编写pytorch-training-loops"><a href="#编写pytorch-training-loops" class="header-anchor">#</a> 编写pytorch training loops:</h2> <p>这里也很简单，思路就是这样：</p> <ol><li>for每一个epoch</li> <li>从dataloader里取出一个个batch</li> <li>把batch喂给model（先把batch都移动到对应的device上）</li> <li>拿出loss，进行反向传播backward</li> <li>分别把optimizer和scheduler都更新一个step</li></ol> <p>最后别忘了每次更新都要清空grad，即对optimizer进行zero_grad()操作。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 要在GPU上训练，需要把数据集都移动到GPU上：</span>
        batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span>v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
        loss <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span><span class="token punctuation">.</span>loss
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token number">100</span>%<span class="token operator">|</span>██████████<span class="token operator">|</span> <span class="token number">459</span>/459 <span class="token punctuation">[</span>01:5<span class="token operator"><span class="token file-descriptor important">4</span>&lt;</span>00:00,  <span class="token number">4</span>.01it/s<span class="token punctuation">]</span>
<span class="token number">100</span>%<span class="token operator">|</span>██████████<span class="token operator">|</span> <span class="token number">459</span>/459 <span class="token punctuation">[</span>01:5<span class="token operator"><span class="token file-descriptor important">5</span>&lt;</span>00:00,  <span class="token number">3</span>.98it/s<span class="token punctuation">]</span>
<span class="token number">100</span>%<span class="token operator">|</span>██████████<span class="token operator">|</span> <span class="token number">459</span>/459 <span class="token punctuation">[</span>01:5<span class="token operator"><span class="token file-descriptor important">5</span>&lt;</span>00:00,  <span class="token number">3</span>.96it/s<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h2 id="_4-evaluation"><a href="#_4-evaluation" class="header-anchor">#</a> 4. Evaluation</h2> <p>这里跟train loop还是挺类似的，一些细节见注释即可：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_metric

metric<span class="token operator">=</span> load_metric<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> batch <span class="token keyword">in</span> eval_dataloader<span class="token punctuation">:</span>
    batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># evaluation的时候不需要算梯度</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
    
    logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
    predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># 由于dataloader是每次输出一个batch，因此我们要等着把所有batch都添加进来，再进行计算</span>
    metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
<span class="token punctuation">{</span><span class="token string">'accuracy'</span><span class="token builtin class-name">:</span> <span class="token number">0.8651960784313726</span>, <span class="token string">'f1'</span><span class="token builtin class-name">:</span> <span class="token number">0.9050086355785838</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr> <p>至此，Huggingface Transformer初级教程就完结撒花了！</p> <center>🌸🌼ヽ(°▽°)ノ🌸🌺</center> <p>更高级的教程，Huggingface也还没出😂，所以咱们敬请期待吧！不过，学完了这个初级教程，我们基本是也可以快乐地操作各种各样Transformer-based模型自由玩耍啦！</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/65.f7077be9.js" defer></script>
  </body>
</html>
