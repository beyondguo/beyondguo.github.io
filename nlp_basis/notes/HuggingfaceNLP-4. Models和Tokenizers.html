<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Huggingface🤗NLP笔记4：Models，Tokenizers，以及如何做Subword tokenization | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/61.68581797.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/26.a658bfa8.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/39.ac23927e.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/65.f7077be9.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/77.0018ed7f.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/86.4180d39a.js"><link rel="prefetch" href="/assets/js/87.04e4ab18.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link router-link-active">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link router-link-active">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Huggingface🤗NLP笔记4：Models，Tokenizers，以及如何做Subword tokenization</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#models" class="sidebar-link">Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#随机初始化一个transformer模型-通过config来加载" class="sidebar-link">随机初始化一个Transformer模型：通过config来加载</a></li><li class="sidebar-sub-header"><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#初始化一个预训练的transformer模型-通过from-pretrained来加载" class="sidebar-link">初始化一个预训练的Transformer模型：通过from_pretrained来加载</a></li></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#tokenizer" class="sidebar-link">Tokenizer</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#subword-tokenization-☆☆☆" class="sidebar-link">Subword tokenization (☆☆☆)</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#bpe-byte-pair-encoding" class="sidebar-link">BPE————Byte-Pair Encoding：</a></li><li class="sidebar-sub-header"><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#了解一下内部的具体步骤" class="sidebar-link">了解一下内部的具体步骤：</a></li><li class="sidebar-sub-header"><a href="/nlp_basis/notes/HuggingfaceNLP-4.%20Models%E5%92%8CTokenizers.html#special-tokens" class="sidebar-link">Special Tokens</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><blockquote><p><strong>「Huggingface🤗NLP笔记系列-第4集」</strong>
最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的精简+注解版。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。</p></blockquote> <ul><li>官方教程网址：https://huggingface.co/course/chapter1</li> <li>本期内容对应网址：https://huggingface.co/course/chapter2/3?fw=pt</li> <li>本系列笔记的<strong>GitHub</strong>： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP</li></ul> <hr> <h1 id="models-tokenizers-以及如何做subword-tokenization"><a href="#models-tokenizers-以及如何做subword-tokenization" class="header-anchor">#</a> Models，Tokenizers，以及如何做Subword tokenization</h1> <h2 id="models"><a href="#models" class="header-anchor">#</a> Models</h2> <p>前面都是使用的<code>AutoModel</code>，这是一个智能的wrapper，可以根据你给定的checkpoint名字，自动去寻找对应的网络结构，故名Auto。</p> <p>如果明确知道我们需要的是什么网络架构，就可以直接使用具体的<code>*Model</code>，比如<code>BertModel</code>，就是使用Bert结构。</p> <h3 id="随机初始化一个transformer模型-通过config来加载"><a href="#随机初始化一个transformer模型-通过config来加载" class="header-anchor">#</a> 随机初始化一个Transformer模型：通过<code>config</code>来加载</h3> <p><code>*Config</code>这个类，用于给出某个模型的网络结构，通过config来加载模型，得到的就是一个模型的架子，没有预训练的权重。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel<span class="token punctuation">,</span> BertConfig

config <span class="token operator">=</span> BertConfig<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>  <span class="token comment"># 模型是根据config来构建的，这时构建的模型是参数随机初始化的</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>看看config打印出来是啥：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">print</span><span class="token punctuation">(</span>config<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code>BertConfig <span class="token punctuation">{</span>
  <span class="token string">&quot;attention_probs_dropout_prob&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0.1</span>,
  <span class="token string">&quot;gradient_checkpointing&quot;</span><span class="token builtin class-name">:</span> false,
  <span class="token string">&quot;hidden_act&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;gelu&quot;</span>,
  <span class="token string">&quot;hidden_dropout_prob&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0.1</span>,
  <span class="token string">&quot;hidden_size&quot;</span><span class="token builtin class-name">:</span> <span class="token number">768</span>,
  <span class="token string">&quot;initializer_range&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0.02</span>,
  <span class="token string">&quot;intermediate_size&quot;</span><span class="token builtin class-name">:</span> <span class="token number">3072</span>,
  <span class="token string">&quot;layer_norm_eps&quot;</span><span class="token builtin class-name">:</span> 1e-12,
  <span class="token string">&quot;max_position_embeddings&quot;</span><span class="token builtin class-name">:</span> <span class="token number">512</span>,
  <span class="token string">&quot;model_type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;bert&quot;</span>,
  <span class="token string">&quot;num_attention_heads&quot;</span><span class="token builtin class-name">:</span> <span class="token number">12</span>,
  <span class="token string">&quot;num_hidden_layers&quot;</span><span class="token builtin class-name">:</span> <span class="token number">12</span>,
  <span class="token string">&quot;pad_token_id&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
  <span class="token string">&quot;position_embedding_type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;absolute&quot;</span>,
  <span class="token string">&quot;transformers_version&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;4.3.3&quot;</span>,
  <span class="token string">&quot;type_vocab_size&quot;</span><span class="token builtin class-name">:</span> <span class="token number">2</span>,
  <span class="token string">&quot;use_cache&quot;</span><span class="token builtin class-name">:</span> true,
  <span class="token string">&quot;vocab_size&quot;</span><span class="token builtin class-name">:</span> <span class="token number">30522</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>更常用的做法则是直接加载预训练模型，然后微调。</p> <h3 id="初始化一个预训练的transformer模型-通过from-pretrained来加载"><a href="#初始化一个预训练的transformer模型-通过from-pretrained来加载" class="header-anchor">#</a> 初始化一个预训练的Transformer模型：通过<code>from_pretrained</code>来加载</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel

model <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-cased'</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>模型的保存：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">&quot;directory_on_my_computer&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># 会生成两个文件： config.json pytorch_model.bin</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h2 id="tokenizer"><a href="#tokenizer" class="header-anchor">#</a> Tokenizer</h2> <p>transformer模型使用的分词方法，往往不是直接的word-level分词或者char-level分词。</p> <p>前者会让词表过大，后者则表示能力很低。</p> <p>因此主流的方式是进行 <strong>subword-level</strong> 的分词。例如对 &quot;tokenization&quot; 这个词，可能会被分成 &quot;token&quot; 和 &quot;ization&quot; 两部分。</p> <p>常见的subword tokenization方法有：</p> <ul><li>BPE</li> <li>WordPiece</li> <li>Unigram</li> <li>SentencePiece</li> <li>...</li></ul> <p>这里对BPE做一个简单的介绍，让我们对 sub-word tokenization 的原理有一个基本了解：</p> <h2 id="subword-tokenization-☆☆☆"><a href="#subword-tokenization-☆☆☆" class="header-anchor">#</a> Subword tokenization (☆☆☆)</h2> <p>Subword tokenization的核心思想是：“<strong>频繁出现了词不应该被切分成更小的单位，但不常出现的词应该被切分成更小的单位</strong>”。</p> <p>比方&quot;annoyingly&quot;这种词，就不是很常见，但是&quot;annoying&quot;和&quot;ly&quot;都很常见，因此细分成这两个sub-word就更合理。中文也是类似的，比如“仓库管理系统”作为一个单位就明显在语料中不会很多，因此分成“仓库”和“管理系统”就会好很多。</p> <p>这样分词的好处在于，大大节省了词表空间，还能够解决OOV问题。因为我们很多使用的词语，都是由更简单的词语或者词缀构成的，我们不用去保存那些“小词”各种排列组合形成的千变万化的“大词”，而用较少的词汇，去覆盖各种各样的词语表示。同时，相比与直接使用最基础的“字”作为词表，sub-word的语义表示能力也更强。</p> <p>那么，用什么样的标准得到sub-word呢？一个著名的算法就是 <strong>Byte-Pair Encoding (BPE)</strong> ：</p> <p>（下面的内容，主要翻译自Huggingface Docs中讲解tokenizer的部分，十分推荐大家直接阅读： https://huggingface.co/transformers/master/tokenizer_summary.html ）</p> <h3 id="bpe-byte-pair-encoding"><a href="#bpe-byte-pair-encoding" class="header-anchor">#</a> BPE————Byte-Pair Encoding：</h3> <h4 id="step1-首先-我们需要对语料进行一个预分词-pre-tokenization"><a href="#step1-首先-我们需要对语料进行一个预分词-pre-tokenization" class="header-anchor">#</a> <strong>Step1</strong>：首先，我们需要对<strong>语料</strong>进行一个<strong>预分词（pre-tokenization）</strong>：</h4> <p>比方对于英文，我可以直接简单地使用空格加一些标点符号来分词；中文可以使用jieba或者直接字来进行分词。</p> <p>分词之后，我们就得到了一个<strong>原始词集合</strong>，同时，还会记录每个词在训练语料中出现的<strong>频率</strong>。</p> <p>假设我们的词集合以及词频是：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token punctuation">(</span><span class="token string">&quot;hug&quot;</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;pug&quot;</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;pun&quot;</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;bun&quot;</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;hugs&quot;</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h4 id="step2-构建基础词表-base-vocab-并开始学习-结合规则-merge-rules"><a href="#step2-构建基础词表-base-vocab-并开始学习-结合规则-merge-rules" class="header-anchor">#</a> <strong>Step2</strong>：构建<strong>基础词表（base vocab）</strong> 并开始学习 <strong>结合规则（merge rules）</strong>：</h4> <p>对于英语来说，我们选择字母来构成<strong>基础词表</strong>：</p> <p><code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;]</code></p> <p>注：这个基础词表，就是我们最终词表的初始状态，我们会不断构建新词，加进去，直到达到我们理想的词表规模。</p> <p>根据这个基础词表，我们可以对原始的词集合进行细粒度分词，并看到基础词的词频：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token punctuation">(</span><span class="token string">&quot;h&quot;</span> <span class="token string">&quot;u&quot;</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;p&quot;</span> <span class="token string">&quot;u&quot;</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;p&quot;</span> <span class="token string">&quot;u&quot;</span> <span class="token string">&quot;n&quot;</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;b&quot;</span> <span class="token string">&quot;u&quot;</span> <span class="token string">&quot;n&quot;</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;h&quot;</span> <span class="token string">&quot;u&quot;</span> <span class="token string">&quot;g&quot;</span> <span class="token string">&quot;s&quot;</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>接下来就是BPE的Byte-Pair核心部分————找symbol pair（符号对）并学习结合规则，即，我们从上面这个统计结果中，找出出现次数最多的那个符号对：</p> <p>统计一下：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>h<span class="token operator">+</span>u   出现了 <span class="token number">10</span><span class="token operator">+</span><span class="token number">5</span><span class="token operator">=</span><span class="token number">15</span> 次
u<span class="token operator">+</span>g   出现了 <span class="token number">10</span><span class="token operator">+</span><span class="token number">5</span><span class="token operator">+</span><span class="token number">5</span> <span class="token operator">=</span> <span class="token number">20</span> 次
p<span class="token operator">+</span>u   出现了 <span class="token number">12</span> 次
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>统计完毕，我们发现<code>u+g</code>出现了最多次，因此，第一个结合规则就是：<strong>把<code>u</code>跟<code>g</code>拼起来，得到<code>ug</code>这个新词！</strong></p> <p>那么，我们就把<code>ug</code>加入到我们的基础词表：</p> <p><code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;]</code></p> <p>同时，词频统计表也变成了：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>(&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h4 id="step3-反复地执行上一步-直到达到预设的词表规模。"><a href="#step3-反复地执行上一步-直到达到预设的词表规模。" class="header-anchor">#</a> <strong>Step3</strong>：反复地执行上一步，直到达到预设的词表规模。</h4> <p>我们接着统计，发现下一个频率最高的symbol pair是<code>u+n</code>，出现了12+4=16次，因此词表中增加<code>un</code>这个词；再下一个则是<code>h+ug</code>，出现了10+5=15次，因此添加<code>hug</code>这个词......</p> <p>如此进行下去，当达到了预设的<code>vocab_size</code>的数目时，就停止，咱们的词表就得到啦！</p> <h4 id="step4-如何分词"><a href="#step4-如何分词" class="header-anchor">#</a> <strong>Step4</strong>：如何分词：</h4> <p>得到了最终词表，在碰到一个词汇表中没有的词的时候，比如<code>bug</code>就会把它分成<code>b</code>和<code>ug</code>。也可以理解成，我首先把<code>bug</code>分解成最基本的字母，然后根据前面的结合规律，把<code>u</code>跟<code>g</code>结合起来，而<code>b</code>单独一个。具体在分词时候是如何做的，有时间去读读源码。</p> <hr> <p>除了BPE，还有一些其他的sub-word分词法，可以参考 https://huggingface.co/transformers/master/tokenizer_summary.html 。</p> <p>下面，我们就直接使用Tokenizer来进行分词：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer  <span class="token comment"># 或者 AutoTokenizer</span>

tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;bert-base-cased&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><div class="language-python line-numbers-mode"><pre class="language-python"><code>s <span class="token operator">=</span> <span class="token string">'today is a good day to learn transformers'</span>
tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>得到：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token punctuation">{</span><span class="token string">'input_ids'</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token number">101</span>, <span class="token number">2052</span>, <span class="token number">1110</span>, <span class="token number">170</span>, <span class="token number">1363</span>, <span class="token number">1285</span>, <span class="token number">1106</span>, <span class="token number">3858</span>, <span class="token number">11303</span>, <span class="token number">1468</span>, <span class="token number">102</span><span class="token punctuation">]</span>, <span class="token string">'token_type_ids'</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span><span class="token punctuation">]</span>, <span class="token string">'attention_mask'</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="了解一下内部的具体步骤"><a href="#了解一下内部的具体步骤" class="header-anchor">#</a> 了解一下内部的具体步骤：</h3> <ol><li><code>tokenize()</code></li></ol> <div class="language-python line-numbers-mode"><pre class="language-python"><code>s <span class="token operator">=</span> <span class="token string">'today is a good day to learn transformers'</span>
tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
tokens
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>输出：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token punctuation">[</span><span class="token string">'today'</span>, <span class="token string">'is'</span>, <span class="token string">'a'</span>, <span class="token string">'good'</span>, <span class="token string">'day'</span>, <span class="token string">'to'</span>, <span class="token string">'learn'</span>, <span class="token string">'transform'</span>, <span class="token string">'##ers'</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>注意这里的分词结果，<code>transformers</code>被分成了<code>transform</code>和<code>##ers</code>。这里的##代表这个词应该紧跟在前面的那个词，组成一个完整的词。</p> <p>这样设计，主要是为了方面我们在还原句子的时候，可以正确得把sub-word组成成原来的词。</p> <ol start="2"><li><code>convert_tokens_to_ids()</code></li></ol> <div class="language-python line-numbers-mode"><pre class="language-python"><code>ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
ids
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>输出：</p> <div class="language- extra-class"><pre><code>[2052, 1110, 170, 1363, 1285, 1106, 3858, 11303, 1468]
</code></pre></div><ol start="3"><li><code>decode</code></li></ol> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1468</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>ids<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 注意这里会把subword自动拼起来</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>输出：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token comment">##ers</span>
today is a good day to learn transformers
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h3 id="special-tokens"><a href="#special-tokens" class="header-anchor">#</a> Special Tokens</h3> <p>观察一下上面的结果，直接call tokenizer得到的ids是：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token punctuation">[</span><span class="token number">101</span>, <span class="token number">2052</span>, <span class="token number">1110</span>, <span class="token number">170</span>, <span class="token number">1363</span>, <span class="token number">1285</span>, <span class="token number">1106</span>, <span class="token number">3858</span>, <span class="token number">11303</span>, <span class="token number">1468</span>, <span class="token number">102</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>而通过<code>convert_tokens_to_ids</code>得到的ids是：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token punctuation">[</span><span class="token number">2052</span>, <span class="token number">1110</span>, <span class="token number">170</span>, <span class="token number">1363</span>, <span class="token number">1285</span>, <span class="token number">1106</span>, <span class="token number">3858</span>, <span class="token number">11303</span>, <span class="token number">1468</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以发现，前者在头和尾多了俩token，id分别是 101 和 102。</p> <p>decode出来瞅瞅：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2052</span><span class="token punctuation">,</span> <span class="token number">1110</span><span class="token punctuation">,</span> <span class="token number">170</span><span class="token punctuation">,</span> <span class="token number">1363</span><span class="token punctuation">,</span> <span class="token number">1285</span><span class="token punctuation">,</span> <span class="token number">1106</span><span class="token punctuation">,</span> <span class="token number">3858</span><span class="token punctuation">,</span> <span class="token number">11303</span><span class="token punctuation">,</span> <span class="token number">1468</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>输出：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token string">'[CLS] today is a good day to learn transformers [SEP]'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>它们分别是 <code>[CLS]</code> 和 <code>[SEP]</code>。这两个token的出现，是因为我们调用的模型，在pre-train阶段使用了它们，所以tokenizer也会使用。</p> <p>不同的模型使用的special tokens不一定相同，所以一定要让tokenizer跟model保持一致！</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/61.68581797.js" defer></script>
  </body>
</html>
