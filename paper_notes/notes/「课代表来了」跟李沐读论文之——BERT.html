<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>「课代表来了」跟李沐读论文之——BERT | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/86.4180d39a.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/26.a658bfa8.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/39.ac23927e.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/61.68581797.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/65.f7077be9.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/77.0018ed7f.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/87.04e4ab18.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link router-link-active">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link router-link-active">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>「课代表来了」跟李沐读论文之——BERT</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#标题-作者" class="sidebar-link">标题/作者</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#摘要" class="sidebar-link">摘要</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#intro" class="sidebar-link">Intro</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#结论" class="sidebar-link">结论</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#相关工作" class="sidebar-link">相关工作</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#bert模型设计" class="sidebar-link">BERT模型设计</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#两个步骤-pre-training-和-fine-tuning" class="sidebar-link">两个步骤：pre-training 和 fine-tuning</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#模型结构和参数" class="sidebar-link">模型结构和参数</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#输入的表示" class="sidebar-link">输入的表示</a></li></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#bert的预训练" class="sidebar-link">BERT的预训练</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#masked-lm" class="sidebar-link">Masked LM</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#next-sentence-prediction" class="sidebar-link">Next Sentence Prediction</a></li></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#bert的微调" class="sidebar-link">BERT的微调</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#具体如何针对下游任务进行微调" class="sidebar-link">具体如何针对下游任务进行微调</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#glue" class="sidebar-link">GLUE</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#squad-问答" class="sidebar-link">SQuAD （问答）</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#swag" class="sidebar-link">SWAG</a></li></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#消融实验" class="sidebar-link">消融实验</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#预训练模型的大小" class="sidebar-link">预训练模型的大小</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#只使用bert作为特征抽取器的效果" class="sidebar-link">只使用BERT作为特征抽取器的效果</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94BERT.html#总结" class="sidebar-link">总结</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="「课代表来了」跟李沐读论文之-bert"><a href="#「课代表来了」跟李沐读论文之-bert" class="header-anchor">#</a> 「课代表来了」跟李沐读论文之——BERT</h1> <p>B站视频地址：https://b23.tv/XbDJENb</p> <h2 id="标题-作者"><a href="#标题-作者" class="header-anchor">#</a> 标题/作者</h2> <p>BERT：Pre-trainingof Deep Bidirectional Transformers for Language Understanding</p> <blockquote><p>Comments by Li Mu: 作者是Google AI Language团队的几位小哥，据说是一作当时突然有了一个idea，然后花了几星期去写代码、跑实验，发现效果特别好，最后就成了这篇文章。可能从idea到成型就几个月。</p></blockquote> <h2 id="摘要"><a href="#摘要" class="header-anchor">#</a> 摘要</h2> <p>BERT这个名字是从 Bidirectional Encoder Representations from Transformers得来的，猜测是为了凑出Bert这个词，因为前面的著名工作ELMo就是美国家喻户晓的动画片芝麻街中的主角之一。在BERT出来之后，后面的研究者就开始想方设法地把芝麻街中的重要人物都用了个遍。</p> <p>主要对比对象是ELMo和GPT。最大的作用就是我们可以只是使用预训练好的BERT模型，添加一个任务相关的输出层，就可以在下游任务上达到SOTA水平，极大地降低了NLP任务的门槛。而前面的ELMo则需要对模型进行修改。</p> <p>最后讲了BERT的效果非常好，即列出了在benchmark上的绝对精度，还列出了相对精度，在11个NLP任务上都达到了SOTA。</p> <blockquote><p>Comments by Li Mu: 在摘要中直接进行跟前人工作的对比，这种写法是很有意思的（在你的模型很大程度上基于或者对比前人工作的话，是可以且应该直接在最开始进行介绍的）。 在说明模型效果的时候，绝对精度和相对精度都是需要的，前者让我们知道在公共数据集上的绝对实力（尤其对于小同行），后者则给读者（尤其是更广泛的读者甚至外行）一个关于模型效果的直观的感受。</p></blockquote> <h2 id="intro"><a href="#intro" class="header-anchor">#</a> Intro</h2> <blockquote><p>BERT不是第一个做NLP预训练的，而是第一次让这个方法出圈了。</p></blockquote> <p>从intro部分我们可以知道，language model pre-training其实之前多年前就有了。</p> <p>使用预训练模型来帮助下游任务的时候，现有的做法有两种：</p> <ul><li>feature-based方式，例如ELMo，就是把预训练的表示<strong>作为额外的特征</strong>，加入到特定任务的模型中；</li> <li>fine-tuning方式，例如GPT，尽可能少的引入任务相关的参数，而主要是在预训练好的参数上面进行微调；</li></ul> <p>前面的ELMo和GPT的方法，都是使用<strong>单向的语言模型</strong>来学习通用的语言表示。 例如在GPT中，作者设计了一种从左到右的架构，在Transformer的self-attention中每个token只能attend到前面的token。在更早的ELMo中，由于使用的是RNN的架构，更加是单向的语言模型。这一点严重限制了作为预训练使用的语言表示能力。比如在做NER的时候，我们都是可以看到上下文的。</p> <p>BERT主要就是为了解决这种单向的限制，设计了一种&quot;mask language modeling&quot;(MLM)的方式，来进行双向的语言模型预训练。这一点是借鉴了完形填空（cloze）任务。另外，作者还设计了一个叫&quot;next sentence prediction&quot;(NSP)的任务来预训练，即判断两个句子是否是相邻的，还是随机的，这样可以学习句子层面的信息。</p> <p>下图展示了BERT跟前面工作的结构上的对比（在最新版的论文中，这个图是在附录部分，在最初的版本中这则是文章第一个图）：
<img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211216115137.png" alt="BERT vs. GPT vs. ELMo"></p> <p>贡献：</p> <ul><li>展现了双向语言模型的作用；</li> <li>展示了预训练表示对于降低下游任务工作量的巨大作用，并且是首个在一大把NLP任务上都取得SOTA的预训练-微调模式的表示模型；</li> <li>代码和预训练模型都公开了。</li></ul> <h2 id="结论"><a href="#结论" class="header-anchor">#</a> 结论</h2> <p>使用非监督的预训练是非常好的，对于低资源场景的任务尤其有益。主要贡献来自于使用了双向的语言模型。</p> <h2 id="相关工作"><a href="#相关工作" class="header-anchor">#</a> 相关工作</h2> <ol><li>无监督的feature-based pre-training，代表作ELMo</li> <li>无监督的fine-tuning pre-training，代表作GPT</li> <li>有监督的transfer learning，代表作就是CV中那些进行Imagenet进行transfer learning，这在NLP中却用的不是很多。主要是由于高质量的通用的有标签文本数据相对较少。</li></ol> <h2 id="bert模型设计"><a href="#bert模型设计" class="header-anchor">#</a> BERT模型设计</h2> <h3 id="两个步骤-pre-training-和-fine-tuning"><a href="#两个步骤-pre-training-和-fine-tuning" class="header-anchor">#</a> 两个步骤：pre-training 和 fine-tuning</h3> <p>在pre-training阶段使用无标签的数据，在fine-tuning阶段，BERT模型使用前面预训练的权重来初始化，然后使用下游任务有标签的数据进行微调。</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211216115356.png" alt="两阶段"></p> <h3 id="模型结构和参数"><a href="#模型结构和参数" class="header-anchor">#</a> 模型结构和参数</h3> <p>模型结构是直接使用原始的Transformer。使用了两种不同架构：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>E</mi><mi>R</mi><msub><mi>T</mi><mrow><mi>B</mi><mi>A</mi><mi>S</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">BERT_{BASE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">BER</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（L=12, H=768, A=12，总参数量110M）和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>E</mi><mi>R</mi><msub><mi>T</mi><mrow><mi>L</mi><mi>A</mi><mi>R</mi><mi>G</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">BERT_{LARGE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">BER</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">RGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（L=24, H=1024, A=16，总参数量340M），其中L是Transformer的层数/block数，H是hidden size，A是头数。</p> <p>后面沐神也讲解了参数量是咋算的（这部分真是太棒了）：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211216121215.png" alt="参数量的计算"></p> <p>参数的来源主要是Transformer中的embedding层、multi-head attention的投影矩阵、MLP层：</p> <ul><li>embedding层：词汇量为V，词向量维度为H，所以这部分参数里为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>×</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">V \times H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span>；</li> <li>multi-head：分别是使用了A个小投影矩阵来讲原本的H维向量给降维成多个低维向量，但向量维度之和还是H，所以多个小投影矩阵合并起来就是一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">H \times H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span>矩阵，然后因为self-attention会分成QKV，所以这里有3个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">H^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>；除此之外，在经过multi-head分开后又会合并成一个H的向量，会再经过一个投影矩阵，也是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">H^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>，所以这部分总共有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">4 H^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>；</li> <li>MLP层：Transformer中使用的是一个由两个全连接层构成的FNN，第一个全连接层会将维度放大4倍，第二个则降维到原始的H，因此，这里的参数量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mn>4</mn><mi>H</mi><mo>+</mo><mn>4</mn><mi>H</mi><mo>×</mo><mi>H</mi><mo>=</mo><mn>8</mn><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">H \times 4H + 4H\times H=8H^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">8</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.</li> <li>上面的multi-head和MLP，都属于一个Transformer block，而我们会使用L个blocks。</li></ul> <p>因此，总体参数量=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>H</mi><mo>+</mo><mn>12</mn><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">VH + 12LH^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">12</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.</p> <p>这么算下来，差不多<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>E</mi><mi>R</mi><msub><mi>T</mi><mrow><mi>B</mi><mi>A</mi><mi>S</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">BERT_{BASE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">BER</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>参数量是108M，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>E</mi><mi>R</mi><msub><mi>T</mi><mrow><mi>L</mi><mi>A</mi><mi>R</mi><mi>G</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">BERT_{LARGE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">BER</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">RGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是330M。（跟原文说的接近的，但相差的部分在哪儿呢？）</p> <h3 id="输入的表示"><a href="#输入的表示" class="header-anchor">#</a> 输入的表示</h3> <p>为了适应不同的下游任务，BERT的输入既可以是<strong>单个句子</strong>，也可以是一个句子对（例如&lt;Question, Answer&gt;）。</p> <p>在输入token方面，使用WordPiece的embedding方式，也是sub-word tokenization的方式的一种，我们看到的那些前前面带有&quot;##&quot;的词就代表这是被wordpiese给切开的子词。这样可以减少词汇量，最终词汇量是30000。</p> <p>每个序列的开头的token，都是一个特殊的分类token——[CLS]，这个token对应的最后一次的hidden state会被用来作为分类任务中的整个序列的表示。对于非分类任务，这个向量是被忽略的。</p> <p>处理句子对时，对模型来说还是一个序列，只不过两个句子中间用一个特殊的[SEP] token进行了连接。两个句子分别还配有可学习的segment embedding；而对于仅有一个句子的输入，我们就只使用一个segment embedding.</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211216152005.png" alt="输入的embedding"></p> <h2 id="bert的预训练"><a href="#bert的预训练" class="header-anchor">#</a> BERT的预训练</h2> <h3 id="masked-lm"><a href="#masked-lm" class="header-anchor">#</a> Masked LM</h3> <p>随机地把原文中的15%的token给遮盖住，即用一个 [MASK] token来替换原来的词。然后把mask之后的文本输入到模型中，让模型去预测这些被mask掉的词。这样就实现了双向的语言模型。</p> <p>但这样做会导致预训练和微调阶段的不一致性：预训练的时候输入都是带有 [MASK] token的，而这个token在微调阶段是看不到的，这样自然会影响微调时的效果。为了缓解这个问题，作者使用了如下的操作：</p> <ul><li>当挑到某个词去mask的时候，80%的概率会真的被替换成[MASK]，10%的概率会被替换成一个随机的真实token，还有10%的概率不进行任何操作。</li></ul> <p>这种做法，说实话还是挺费解的，让人感觉也不一定有多大效果，但作者说这样可以缓解一点就缓解一点吧。（实际上现在也有很多研究在解决这个问题，这部分后面补充...）</p> <p>另外一个问题在于MLM在这里只使用了15%的mask比例，这会让模型需要训练更久才能收敛。但好在最终的效果非常好，所以也值了。（不知道如果使用更大的比例会怎么样？）</p> <h3 id="next-sentence-prediction"><a href="#next-sentence-prediction" class="header-anchor">#</a> Next Sentence Prediction</h3> <p>很多的下游任务，比如QA（问答）和NLI（自然语言推理）任务，都需要模型能够理解句子之间的关系，而这种关系难以被MLM所学习到。因此作者设计了一个输入句子对的二分类的NSP任务：</p> <ul><li>50%的样本中，句子A和句子B是在真实文本中连续的句子，标签是 IsNext；</li> <li>50%的样本中，B跟A不是连续的，而是随机挑选的句子，标签是 NotNext.</li></ul> <p>虽然这个任务看起来非常简单，而且作者说在预训练时这个任务可以达到97%以上的准确率，但后面的实验证明确实对QA和NLI任务有很大的帮助。</p> <p>注意到pre-training的那个图，<strong>在NSP任务中，我们使用的是[CLS] token对应的hidden state来训练</strong>的，即我们使用这个[CLS]来代表我整个句子对的表示，用它来进行二分类任务。</p> <h2 id="bert的微调"><a href="#bert的微调" class="header-anchor">#</a> BERT的微调</h2> <p>对于sequence-level的任务，我们可以直接使用CLS的向量作为sequence的表示，然后后面加一个简单的softmax层来进行训练；对于span-level或者token-level的任务，也只用稍微修改一下跟任务相关的输出层即可。</p> <p>另外，微调跟预训练时的差别还在BERT模型训练的一些超参数上，比如learning rate，batch size等等。例如在pre-training阶段batch size=256，而在fine-tuning阶段作者推荐使用16或者32.</p> <h2 id="具体如何针对下游任务进行微调"><a href="#具体如何针对下游任务进行微调" class="header-anchor">#</a> 具体如何针对下游任务进行微调</h2> <h3 id="glue"><a href="#glue" class="header-anchor">#</a> GLUE</h3> <p>GLUE数据集一般都是sequence-level的任务，主要都是分类，既有单句子的，也有句子对的任务。这种就是直接用CLS配合一个softmax来跑即可。</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211216162841.png" alt="GLUE"></p> <h3 id="squad-问答"><a href="#squad-问答" class="header-anchor">#</a> SQuAD （问答）</h3> <p>给定一个问题，从一段话中找出答案所在的片段。所以问题转化为对每个token判断是否是答案的开头或结尾。 具体细节由于我不做问答，所以详情见论文吧。</p> <h3 id="swag"><a href="#swag" class="header-anchor">#</a> SWAG</h3> <p>这是另外一个句子对推理任务，其实跟NSP任务比较像，所以这里也不多介绍了。</p> <p>以上的任务，作者也花了几个示意图告诉我们如何做任务相关的模型调整：</p> <p>对于分类任务：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211217143020.png" alt="sequence classification"></p> <p>对于token标注：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211217143102.png" alt="token classification"></p> <p>所以总体上看，我们只需要做微小的调整，就可以应对各种下游任务。</p> <h2 id="消融实验"><a href="#消融实验" class="header-anchor">#</a> 消融实验</h2> <p>作者继续做了一些消融实验，来看看NSP、双向语言模型等的作用。</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211217143346.png" alt="ablation study"></p> <p>从上图可看到，对于各种任务来说，NSP预训练还是有帮助的，把NSP去掉的话，在很多任务上效果都会降低（但好像也没有那么明显哈）；然后如果只使用Left-to-right（LTR）的语言模型的话，效果会进一步降低，这个降低就比较明显了。</p> <p>总之，这个实验论证了BERT的几个关键点都是重要的。</p> <h2 id="预训练模型的大小"><a href="#预训练模型的大小" class="header-anchor">#</a> 预训练模型的大小</h2> <p>BERT这个论文，证明了使用一个很大的预训练模型，可以极大地提高下游任务的表现。</p> <p>从现在的眼光看，BERT也不算大了，例如GPT3的大小就是BERT的1000倍（千亿），现在甚至万亿级别的模型都在不断出现。</p> <h2 id="只使用bert作为特征抽取器的效果"><a href="#只使用bert作为特征抽取器的效果" class="header-anchor">#</a> 只使用BERT作为特征抽取器的效果</h2> <p>作者还探究了一下用feature-based的方式来利用BERT预训练的表示的效果，下表是在一个NER任务上的结果：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211217144355.png" alt="使用bert作为静态特征提取器"></p> <p>总体结论是，如果使用BERT的话，还是尽量用fine-tuning的方式效果会更好。但是从图中看，将最后几层的hidden states拼接起来作为特征，效果在NER上也不错。</p> <h2 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h2> <p>作者对这个工作最大的贡献总结为BERT的双向性，然而双向语言模型和单向模型，其实只是不同的策略，使用双向的方式进行预训练，那自然在某些任务上会有些损失，比如在机器翻译、摘要生成等任务，可能BERT就没有GPT那么优秀。这其实就是有得必有失。</p> <blockquote><p>Comments by Li Mu: 对于写文章，我们最好是重点突出一个卖点，不要太多了。例如这篇文章就是突出”双向语言模型“。</p></blockquote> <p>最后，沐神提出了一个灵魂拷问：其实BERT从整个流程上，跟GPT的工作是很类似，都是先预训练，在进行下游任务微调。为什么BERT更晚，却更出圈，现在名气和影响力是远远大于GPT的？</p> <p>这个问题，在B站评论区大家也在讨论，大家的一个观点是：因为BERT做了更好的开源，把代码、预训练模型都直接公开出来了。这让广大的研究者可以直接拿来使用，提现预训练的威力，因此很快就可以传播开。</p> <p>这一点告诉我们，<strong>开源、可复现、方便后续研究者使用，对一个研究工作有很大的推动作用</strong>。现在很多的论文，发表在顶会顶刊上，却不公开代码，或者代码公开了却写的稀烂，没有任何的文档来帮助人们复现，这必然极大影响论文最终的影响力，甚至影响作者的声誉。</p> <p>“做真正有影响力、有价值的研究，而不是为了水水文章、增加自己的publications。”这句最简单、最朴素的科研工作者都应该有的价值观，在当下的环境下，尤其是国内这种长期以来的追求论文数量的价值观、高校不合理的考核机制、各大技术厂商的极端内卷等影响下，显得无比珍贵。</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/86.4180d39a.js" defer></script>
  </body>
</html>
