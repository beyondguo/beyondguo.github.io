<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>「课代表来了」跟李沐读论文之Transformer | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/87.04e4ab18.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/26.a658bfa8.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/39.ac23927e.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/61.68581797.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/65.f7077be9.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/77.0018ed7f.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/86.4180d39a.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link router-link-active">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link router-link-active">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>「课代表来了」跟李沐读论文之Transformer</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#标题-作者" class="sidebar-link">标题+作者</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#摘要" class="sidebar-link">摘要</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#结论" class="sidebar-link">结论</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#intro-导言" class="sidebar-link">Intro（导言）</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#background-相关工作" class="sidebar-link">Background（相关工作）</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#模型" class="sidebar-link">模型</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#encoder" class="sidebar-link">Encoder</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#decoder" class="sidebar-link">Decoder</a></li></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#逐个看看每个sub-layer" class="sidebar-link">逐个看看每个sub-layer：</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#scaled-dot-product-attention" class="sidebar-link">Scaled Dot-product Attention</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#multi-head-attention" class="sidebar-link">Multi-Head Attention</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#point-wise-feed-forward-networks" class="sidebar-link">Point-wise Feed-forward Networks</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#embedding和softmax层" class="sidebar-link">Embedding和softmax层</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#position-encoding" class="sidebar-link">Position Encoding</a></li></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#为啥使用self-attention" class="sidebar-link">为啥使用Self-Attention</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#训练细节-超参数" class="sidebar-link">训练细节&amp;超参数</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#沐神点评" class="sidebar-link">沐神点评</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#写作上" class="sidebar-link">写作上</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%E3%80%8C%E8%AF%BE%E4%BB%A3%E8%A1%A8%E6%9D%A5%E4%BA%86%E3%80%8D%E8%B7%9F%E6%9D%8E%E6%B2%90%E8%AF%BB%E8%AE%BA%E6%96%87%E4%B9%8B%E2%80%94%E2%80%94Transformer.html#transformer" class="sidebar-link">Transformer</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="「课代表来了」跟李沐读论文之transformer"><a href="#「课代表来了」跟李沐读论文之transformer" class="header-anchor">#</a> 「课代表来了」跟李沐读论文之Transformer</h1> <blockquote><p>墙裂推荐沐神在B上开的一系列公开课，尤其是最近出的“论文精读”系列，真的讲得太好了。可能很多经典论文咱们之前也读过，但是听李沐老师再讲一遍，我们又可以收获很多新知识，尤其是李沐老师结合自己丰富的经验做出的很多精彩评论。</p> <p>我也慕名去听了课，第一堂课我选择听NLP经典论文&quot;Attention Is All You Need&quot;，边听边整理了如下笔记 📒 ，方便自己日后查阅，也分享给大家。</p></blockquote> <blockquote><p>李沐老师读论文的顺序一般是这样的：标题+作者 &gt; 摘要 &gt; 导言 &gt; 结论 &gt; 相关工作 &gt; 模型设计 &gt; 实验，最后做一个总评。我这里也是按照这个顺序进行记录的。</p></blockquote> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211219131751.png" alt=""></p> <h2 id="标题-作者"><a href="#标题-作者" class="header-anchor">#</a> 标题+作者</h2> <p>标题：Attention Is All You Need
发表：NIPS2017
机构：Google</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211219121954.png" alt="image-20211219121954021"></p> <blockquote><p>Comments by Li Mu: 8个作者都是共同贡献，这在计算机论文中很少见。但是为了解释为什么有这么多共同一作，论文在脚注中清楚了写明了每个人在这个工作中的贡献，确实大家都担得上“共同贡献”这一说。</p></blockquote> <h2 id="摘要"><a href="#摘要" class="header-anchor">#</a> 摘要</h2> <p>背景是机器翻译、encoder-decoder框架，而且一般都会在中间使用Attention机制。</p> <p>本文提出了一个新的简单的网络架构——Transformer，完全基于attention机制。</p> <blockquote><p>Comments by Li Mu: 这是个好名字，跟“变形金刚”表达相同</p></blockquote> <p>贡献：网络简单，且跟之前的网络结构都不一样，不使用rnn或cnn的单元。并行度更好，训练快很多。在机器翻译上取得了更好的效果。</p> <blockquote><p>Comments by Li Mu: 最开始Transformer是在机器翻译的背景上提出的，这是个小领域，因此可能最初并没有收到广泛关注。后面Transformer开始在各种其他领域大显神通，到最近在vision上面也表现优异，才真正火出圈了</p></blockquote> <h2 id="结论"><a href="#结论" class="header-anchor">#</a> 结论</h2> <p>本文提出的Transformer是第一个纯基于attention的序列转录模型，使用multi-head self-attention替代了之前的rnn结构。</p> <p>在机器翻译上，比RNN和CNN都要快，还取得了新的SOTA。</p> <p>作者们对这种纯attention的模型能取得这么好的效果感到非常激动。因此作者们期待Transformer能在其他领域（CV，audio等）大放异彩。</p> <blockquote><p>Comments by Li Mu: 作者在2017年的时候就一定程度上预测了未来。如今Transformer不光横扫了NLP的各种任务，还在CV上也取得了惊人的效果。</p></blockquote> <p>最后代码开源在tensor2tensor库里面。</p> <blockquote><p>Comments by Li Mu: 其实代码最好是放在摘要的最后一句话，让读者可以第一时间看到代码。</p></blockquote> <h2 id="intro-导言"><a href="#intro-导言" class="header-anchor">#</a> Intro（导言）</h2> <p>介绍了传统的RNN，CNN以及encoder-decoder架构。分析了RNN的缺点：1. 难以并行 2. 容易遗忘。再介绍了attention机制。最后提出了一个全新的架构Transformer。</p> <blockquote><p>Comments by Li Mu:  这个intro很短，主要是内容比较多，而NIPS的篇幅较短。</p></blockquote> <h2 id="background-相关工作"><a href="#background-相关工作" class="header-anchor">#</a> Background（相关工作）</h2> <p>为了提高对序列数据的计算效率，很多工作都使用卷积神经网络作为基础的building block来进行模型构建，从而实现并行的计算。然而，CNN是通过滑动窗口来提取特征，所以对于长距离的关系较难捕捉。但CNN还有一个优点——<strong>多通道机制</strong>，使得模型可以从多个角度去提取数据的特征。</p> <p>所以<strong>Transformer借用了多通道的思想，设计了多头的注意力机制</strong>。</p> <p>另外，self-attention不是本工作提出了，而是在曾经的很多工作中都被成功应用了。</p> <h2 id="模型"><a href="#模型" class="header-anchor">#</a> 模型</h2> <p>序列模型中较好的是encoder-decoder架构。</p> <p>要点：</p> <ol><li>encoder把输入序列处理得到中间表示，然后decoder读入这个中间表示，处理后得到输出序列；</li> <li>输入序列和输出序列不一定一样长；</li> <li>decoder是一种auto-regressive的方式来输出的，即每一步都会读入上一步的输出。</li></ol> <p>Transformer依然是一个encoder-decoder的架构，但它主要组成是<strong>self-attention</strong>和<strong>point-wise fully connected layer</strong>，结构如下：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129211929.png" alt="image-20211129211929634"></p> <blockquote><p>Comments by Li Mu:  这个图画的很好，在神经网络的时代，画图是一个重要的技能。然而这个图属于那种“不明觉厉”的图，很好看，但是不容易看懂。</p></blockquote> <h3 id="encoder"><a href="#encoder" class="header-anchor">#</a> Encoder</h3> <ul><li>Encoder由N=6个一模一样的层组成；</li> <li>每个层，包含2个子层：①multi-head self-attention layer，②position-wise fully connected feed-forward network （就是个MLP）；</li> <li>每个子层，都会使用residual connection和layer norm来处理，子层的输出都可以表示为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>S</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">LayerNorm(x+Sublayer(X))</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">yer</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">u</span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">yer</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">))</span></span></span></span>；</li> <li>为了方便残差连接，上面所有的层、包括embedding层，都使用d=512作为输出维度。</li></ul> <blockquote><p>Comments by Li Mu:  总之，Encoder就俩超参数：N和d。这种设计直接影响了后面各种基于Transformer的模型设计，比如BERT，GPT等等，都主要调节这两个参数。</p></blockquote> <h4 id="「插播」沐神小课堂-什么是layer-norm"><a href="#「插播」沐神小课堂-什么是layer-norm" class="header-anchor">#</a> 「插播」沐神小课堂——什么是Layer Norm：</h4> <ul><li>Batch Norm就是把一个batch的tensor，按照feature的每个维度（即按照列）去进行规范化（均值0方差1）</li> <li>Layer Norm则是在batch内逐个样本去做规范化</li></ul> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129220224.png" alt="image-20211129220224178"></p> <p>我们在序列化数据中更常使用的是Layer Norm，因为序列的长度会变化，如果使用batch norm的话，可能导致均值方差波动很大，从而影响效果，而layer norm则是逐个样本去进行的，就不会受影响。</p> <h3 id="decoder"><a href="#decoder" class="header-anchor">#</a> Decoder</h3> <ul><li>跟Encoder一样由N=6个一模一样的层构成；</li> <li>每个层，包含3个子层，相比于Encoder中的设计，多了一个multi-head attention layer；</li> <li>为了防止Decoder在处理时看到未来的信息，这里对self-attention做了进一步的处理，即使用了一个<strong>mask</strong>机制，在t时刻时把后面的单元都mask掉，从而不会attend到未来的信息。</li></ul> <h2 id="逐个看看每个sub-layer"><a href="#逐个看看每个sub-layer" class="header-anchor">#</a> 逐个看看每个sub-layer：</h2> <h3 id="scaled-dot-product-attention"><a href="#scaled-dot-product-attention" class="header-anchor">#</a> Scaled Dot-product Attention</h3> <p>在Transformer中我们使用的attention机制是Scaled Dot-product Attention，下图中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>代表的Q，K，V的维度：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129221807.png" alt="image-20211129221807714"></p> <p>这里的attention机制，相比于经典的Dot-product Attention其实就是多了一个scale项。这里的作用是啥呢？当d比较小的时候，要不要scale都无所谓，但是当d比较大时，内积的值的范围就会变得很大，不同的内积的差距也会拉大，这样的话，再经过softmax进一步的扩大差距，就会使得得到的attention分布很接近one-hot，这样会导致梯度下降困难，模型难以训练。在Transformer中，d=512，算比较大了，因此需要进行scaling。</p> <p>下图很清晰地展示了scaled Dot-product Attention是如何进行的：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129224528.png" alt="image-20211129224528855"></p> <h3 id="multi-head-attention"><a href="#multi-head-attention" class="header-anchor">#</a> Multi-Head Attention</h3> <p>原本的SDP Attention，没什么可学习的参数，作者发现，我们可以先把原本的向量，通过线性层隐射到多个低维的空间，然后再并行地进行SDP Attention操作，在concat起来，可以取得更好的效果。这类似于CNN中的多通道机制。一个向量先隐射成多个更低维的向量，相当于分成了多个视角，然后每个视角都去进行Attention，这样模型的学习能力和潜力就会大大提升，另外由于这里的降维都是参数化的，所以让模型可以根据数据来学习最有用的视角。</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129225233.png" alt="image-20211129225233453"></p> <h3 id="point-wise-feed-forward-networks"><a href="#point-wise-feed-forward-networks" class="header-anchor">#</a> Point-wise Feed-forward Networks</h3> <p>这里其实就是名字比较花哨，实际上就是简单的MLP。唯一需要注意的是这个MLP的修饰词——Point-wise，它的意思是它是对每个position（词）都分开、独立地处理。我之前画过一个图：</p> <p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-8/1623129452764-image.png" alt="point-wise feed-forward示意图"></p> <p>即MLP只是作用于最后一个维度，具体公式是：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">FFN(x) = relu(xW_1 + b_1)W_2 +b_2
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FFN</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p> <p>沐神后面也画图总结了一下：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211215101654.png" alt="image-20211215101654477"></p> <p>这个图左边是Transformer示意图，右边是RNN示意图（其中对RNN单元简化成一个MLP，本质上类似）。</p> <ul><li>Transformer是通过attention来全局地聚合序列的信息，然后通过MLP进行语义空间的转换；</li> <li>RNN则是通过把上一时刻的信息传入下一时刻的单元，来使用序列信息，但也是通过MLP进行语义空间转换。所以二者本质区别在于如何使用序列的信息。</li></ul> <h3 id="embedding和softmax层"><a href="#embedding和softmax层" class="header-anchor">#</a> Embedding和softmax层</h3> <p>Transformer中使用了三处embedding：对input和output的token进行embedding，以及在softmax前面的Linear transformation中也使用跟embedding相同的权重（这样是为了能够把decoder的output通过相同的embedding给转换回token的概率，因为embedding的作用就是做token跟vector之间的转换）。三处的embedding都是同样的权重。</p> <p>另外值得注意的点就是，作者把embedding都乘上了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_{model}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span></span>。这是为了在后面跟position embedding相乘的时候能够保持差不多的scale。</p> <h3 id="position-encoding"><a href="#position-encoding" class="header-anchor">#</a> Position Encoding</h3> <p>由于self-attention实际上是不考虑序列顺序的，只是单纯把各个position的信息进行聚合，无论顺序怎么改变，对于self-attention来说都是一样的。因此，这里特意设计了position encoding这种东西，来添加位置信息。</p> <p>具体的，文章使用的是周期不同的sin和cos函数来计算得到每个position的Embedding：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos, 2i) = sin(pos/10000^{2i/d_{model}}) \\
PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">in</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">cos</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p> <p>其中pos是具体位置的index，i则是具体的dimension。总之，这里给每个位置，都构造了一个长尾<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d_{model}=512</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">512</span></span></span></span>的向量，来作为该position也就是某个具体token的位置表示。</p> <p>最后，这个position encoding是直接跟embedding相加，输入到模型中。</p> <h2 id="为啥使用self-attention"><a href="#为啥使用self-attention" class="header-anchor">#</a> 为啥使用Self-Attention</h2> <blockquote><p>Comments by Li Mu: 整个文章实际上对模型的解释是比较欠缺的。</p></blockquote> <p>作者主要通过下面这个表来对比self-attention和其他结构的差别（restricted self-attention不用管，不怎么用）：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211215104354.png" alt="image-20211215104354876"></p> <p>上图中，sequential operations衡量的是在处理序列的时候的并行复杂度，越小说明并行度越高；max path length则表示序列中任意两个点传递信息的最大距离。</p> <p>Self-attention的主要优势在于并行度高（相比RNN）、信息距离短（相比RNN和CNN）。而在复杂度方面，其实没有明显优势：</p> <ul><li><p>self-attention每层的复杂度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2 \cdot d)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span>，是因为有n个position，每个position都要计算n次attention，在计算attention的时候，是进行维度为d的内积，所以复杂度是n * n * d；</p></li> <li><p>RNN层的复杂度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo>⋅</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \cdot d^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，因为有n个position，每个position会经过一个线性变化（矩阵乘法），变换的复杂度是d * d，因此最终复杂度是n * d * d；</p></li> <li><p>CNN实际上跟RNN类似，但因为往往涉及到多个kernel，所以多乘了一个k。但一般在NLP中k也不大，所以没有大差别。</p></li></ul> <p>在NLP中，往往n和d都会比较大，所以这三者的计算复杂度没有质的差别。</p> <blockquote><p>Comments by Li Mu: 看起来应该Transformer在计算效率上会比CNN、RNN更快，但我们现在体会到的Transformer的模型却不是这样的，为啥呢？实际上是因为self-attention对整个模型的假设更少，所以我们往往需要更大量的数量、更大的模型才能够训练出跟CNN、RNN同样的效果来。这就导致现在基于Transformer的模型都特别大特别贵。</p></blockquote> <p>这里就涉及到inductive bias这个概念了，在「什么是inductive bias」文章中，我们知道合理的inductive bias可以让模型的训练加速，这里由于self-attention的inductive bias相比于CNN、RNN更少，所以训练起来也更费劲。但我猜想，对于大规模预训练来说，少一点inductive bias是不是更好？</p> <h2 id="训练细节-超参数"><a href="#训练细节-超参数" class="header-anchor">#</a> 训练细节&amp;超参数</h2> <p>英语翻译德语，使用BPE分词法构造英语德语共用的词典，使用8个P100 GPU，每个batch大概0.4s，总共训练了12小时，其实时间成本相对来说还是可承受的。</p> <p>学习率使用了warmup，先增强后减。</p> <p>使用了两种正则化：</p> <ol><li>Residual dropout，对每个sub-layer的输出都使用了dropout，还对embedding层也使用dropout，dropout rate=0.1</li> <li>Label Smoothing，使用了程度为0.1的smoothing，这会损害一点的perplexity，但是nuisance提高accuracy和BLEU得分</li></ol> <p>下表则是展示了不同的模型结构（超参数）的性能差别：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211215121451.png" alt="image-20211215121451571"></p> <p>实际上可修改的参数不多，主要就是层数（N）、向量维度（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）、头数（h）。像<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">d_k, d_v</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>都是根据<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi></mrow></msub><mi>l</mi></mrow><annotation encoding="application/x-tex">d_{mode}l</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span>和h算出来的。</p> <h2 id="沐神点评"><a href="#沐神点评" class="header-anchor">#</a> 沐神点评</h2> <h3 id="写作上"><a href="#写作上" class="header-anchor">#</a> 写作上</h3> <p>这篇文章写的非常简洁，没有太多的介绍和解释，属于大佬的写作风格。不过对于我们大多数研究者来说，还是需要尽可能把背景解释清楚，在正文花足够的篇幅在把故事讲清楚，从而让读者更好理解、认识的更深入。</p> <h3 id="transformer"><a href="#transformer" class="header-anchor">#</a> Transformer</h3> <p>这个模型的最大意义在于给NLP届乃至CV、audio等其他模态的数据提供了统一的模型架构，有点类似于CNN刚提出时对CV领域的巨大变革。有了Transformer，以及现在各种基于Transformer的预训练模型、其他模态的成功运用，Transformer对于多模态学习的进步有着深远的意义。</p> <p>然后，纵使Transformer已经提出了多年，我们对Transformer的真正原理依然缺乏理解。例如，文章的标题Attention is all you need实际上也是不对的，后续的研究者已经证明Transformer的成功对于MLP、residual connection等其他组件也是缺一不可的，attention的作用就是对序列的信息做了聚合，并不是attention一个人起了全部作用。</p> <p>另外，self-attention相比如RNN、CNN等的优势，可能在于它所作的归纳偏置（inductive bias）更加一般化，所以经过大规模的训练，可以学习到更丰富、更一般化的知识。但代价就是它对数据的关键信息的抓取能力就下降了，我们需要更多的数据更大的模型才能训练出理想的效果。</p> <p>但Transformer的出现，给了整个AI届新的活力，让我们发现在CNN、RNN统治的时代，我们依然可以设计出新的网络结构发光发热，因此也带动了一系列新的网络架构设计，比如纯MLP的模型等等。</p> <hr></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/87.04e4ab18.js" defer></script>
  </body>
</html>
