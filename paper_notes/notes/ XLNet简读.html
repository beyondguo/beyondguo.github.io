<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>XLNet简读 | 郭必扬的写字楼</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-alpha/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/logo_no_words.png">
    <script>
    var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5aca48f844181444aea941eb9d707584";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
    </script>
    <meta name="description" content="欢迎光临写字楼，大楼尚在施工中🚧，对您造成的不便请您谅解">
    
    <link rel="preload" href="/assets/css/0.styles.29dce6f0.css" as="style"><link rel="preload" href="/assets/js/app.41fc4255.js" as="script"><link rel="preload" href="/assets/js/2.a266bdc2.js" as="script"><link rel="preload" href="/assets/js/77.0018ed7f.js" as="script"><link rel="prefetch" href="/assets/js/10.4ee4705c.js"><link rel="prefetch" href="/assets/js/100.29306973.js"><link rel="prefetch" href="/assets/js/101.b7851f01.js"><link rel="prefetch" href="/assets/js/102.969e84bd.js"><link rel="prefetch" href="/assets/js/103.a356b143.js"><link rel="prefetch" href="/assets/js/104.51024b34.js"><link rel="prefetch" href="/assets/js/105.43f26204.js"><link rel="prefetch" href="/assets/js/106.c52df18d.js"><link rel="prefetch" href="/assets/js/107.856c79a2.js"><link rel="prefetch" href="/assets/js/108.bfb45b71.js"><link rel="prefetch" href="/assets/js/11.e2907ec5.js"><link rel="prefetch" href="/assets/js/12.65ea25ed.js"><link rel="prefetch" href="/assets/js/13.cbd5d07c.js"><link rel="prefetch" href="/assets/js/14.b84744eb.js"><link rel="prefetch" href="/assets/js/15.aadcc6a8.js"><link rel="prefetch" href="/assets/js/16.db45b326.js"><link rel="prefetch" href="/assets/js/17.7b58bd1b.js"><link rel="prefetch" href="/assets/js/18.38d69ee8.js"><link rel="prefetch" href="/assets/js/19.4e5ae4a8.js"><link rel="prefetch" href="/assets/js/20.7095c920.js"><link rel="prefetch" href="/assets/js/21.242fa291.js"><link rel="prefetch" href="/assets/js/22.5d8fafb1.js"><link rel="prefetch" href="/assets/js/23.7ce40eb4.js"><link rel="prefetch" href="/assets/js/24.18bba9ed.js"><link rel="prefetch" href="/assets/js/25.5d7dc814.js"><link rel="prefetch" href="/assets/js/26.a658bfa8.js"><link rel="prefetch" href="/assets/js/27.67a75243.js"><link rel="prefetch" href="/assets/js/28.f618d912.js"><link rel="prefetch" href="/assets/js/29.cebd6470.js"><link rel="prefetch" href="/assets/js/3.56cdaf41.js"><link rel="prefetch" href="/assets/js/30.8b95a1f1.js"><link rel="prefetch" href="/assets/js/31.c91ff346.js"><link rel="prefetch" href="/assets/js/32.b98ca641.js"><link rel="prefetch" href="/assets/js/33.d0d05ad0.js"><link rel="prefetch" href="/assets/js/34.4c521488.js"><link rel="prefetch" href="/assets/js/35.b804bcd7.js"><link rel="prefetch" href="/assets/js/36.5c07cfef.js"><link rel="prefetch" href="/assets/js/37.a0565011.js"><link rel="prefetch" href="/assets/js/38.b8ae16d0.js"><link rel="prefetch" href="/assets/js/39.ac23927e.js"><link rel="prefetch" href="/assets/js/4.b9abaf2a.js"><link rel="prefetch" href="/assets/js/40.51f0a3f5.js"><link rel="prefetch" href="/assets/js/41.c1a8242a.js"><link rel="prefetch" href="/assets/js/42.749efe23.js"><link rel="prefetch" href="/assets/js/43.624833c8.js"><link rel="prefetch" href="/assets/js/44.bbddd63e.js"><link rel="prefetch" href="/assets/js/45.d8d15651.js"><link rel="prefetch" href="/assets/js/46.1e480f1b.js"><link rel="prefetch" href="/assets/js/47.4f1075d4.js"><link rel="prefetch" href="/assets/js/48.a8f6c2ba.js"><link rel="prefetch" href="/assets/js/49.106d8698.js"><link rel="prefetch" href="/assets/js/5.1c0b46f4.js"><link rel="prefetch" href="/assets/js/50.a0819973.js"><link rel="prefetch" href="/assets/js/51.cb93eaba.js"><link rel="prefetch" href="/assets/js/52.69fe7f19.js"><link rel="prefetch" href="/assets/js/53.ad8f6842.js"><link rel="prefetch" href="/assets/js/54.c25909c5.js"><link rel="prefetch" href="/assets/js/55.833154c7.js"><link rel="prefetch" href="/assets/js/56.75e06106.js"><link rel="prefetch" href="/assets/js/57.f5b5d514.js"><link rel="prefetch" href="/assets/js/58.574fd406.js"><link rel="prefetch" href="/assets/js/59.d07a17b6.js"><link rel="prefetch" href="/assets/js/6.4b4de84d.js"><link rel="prefetch" href="/assets/js/60.ee946b2f.js"><link rel="prefetch" href="/assets/js/61.68581797.js"><link rel="prefetch" href="/assets/js/62.99d168fd.js"><link rel="prefetch" href="/assets/js/63.14c4cfa2.js"><link rel="prefetch" href="/assets/js/64.61b2d608.js"><link rel="prefetch" href="/assets/js/65.f7077be9.js"><link rel="prefetch" href="/assets/js/66.0ce2a0ec.js"><link rel="prefetch" href="/assets/js/67.ee0094b6.js"><link rel="prefetch" href="/assets/js/68.c61d0f9f.js"><link rel="prefetch" href="/assets/js/69.c59f3168.js"><link rel="prefetch" href="/assets/js/7.ea2aa07c.js"><link rel="prefetch" href="/assets/js/70.21ea68bf.js"><link rel="prefetch" href="/assets/js/71.11355b09.js"><link rel="prefetch" href="/assets/js/72.cdc4bc45.js"><link rel="prefetch" href="/assets/js/73.d8946aed.js"><link rel="prefetch" href="/assets/js/74.75a2fd5c.js"><link rel="prefetch" href="/assets/js/75.14d3ef7d.js"><link rel="prefetch" href="/assets/js/76.3f73a91e.js"><link rel="prefetch" href="/assets/js/78.c471326e.js"><link rel="prefetch" href="/assets/js/79.f9a1e6f0.js"><link rel="prefetch" href="/assets/js/8.f25633c9.js"><link rel="prefetch" href="/assets/js/80.a8c296e0.js"><link rel="prefetch" href="/assets/js/81.f6451a8b.js"><link rel="prefetch" href="/assets/js/82.f7393e31.js"><link rel="prefetch" href="/assets/js/83.a0642cbd.js"><link rel="prefetch" href="/assets/js/84.f2c12c65.js"><link rel="prefetch" href="/assets/js/85.f0077176.js"><link rel="prefetch" href="/assets/js/86.4180d39a.js"><link rel="prefetch" href="/assets/js/87.04e4ab18.js"><link rel="prefetch" href="/assets/js/88.93ea241c.js"><link rel="prefetch" href="/assets/js/89.555a3ba8.js"><link rel="prefetch" href="/assets/js/9.9aae1341.js"><link rel="prefetch" href="/assets/js/90.0701a493.js"><link rel="prefetch" href="/assets/js/91.8347eb90.js"><link rel="prefetch" href="/assets/js/92.9320e5ec.js"><link rel="prefetch" href="/assets/js/93.87dec14c.js"><link rel="prefetch" href="/assets/js/94.7a4ce357.js"><link rel="prefetch" href="/assets/js/95.060612ac.js"><link rel="prefetch" href="/assets/js/96.0b640400.js"><link rel="prefetch" href="/assets/js/97.5572d4ae.js"><link rel="prefetch" href="/assets/js/98.e013c8af.js"><link rel="prefetch" href="/assets/js/99.867ed389.js">
    <link rel="stylesheet" href="/assets/css/0.styles.29dce6f0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo_no_words.png" alt="郭必扬的写字楼" class="logo"> <span class="site-name can-hide">郭必扬的写字楼</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link router-link-active">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/dl_basis/" class="nav-link">
  深度学习笔记
</a></div><div class="nav-item"><a href="/nlp_basis/" class="nav-link">
  NLP笔记
</a></div><div class="nav-item"><a href="/paper_notes/" class="nav-link router-link-active">
  吃点儿论文
</a></div><div class="nav-item"><a href="/engineering/" class="nav-link">
  工程
</a></div><div class="nav-item"><a href="/opinions/" class="nav-link">
  随笔
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  Me
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="在别处~" class="dropdown-title"><span class="title">在别处~</span> <span class="arrow down"></span></button> <button type="button" aria-label="在别处~" class="mobile-dropdown-title"><span class="title">在别处~</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/beyondguo" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://mp.weixin.qq.com/s/v35g-p7wK2MkuM-SqjkF3g" target="_blank" rel="noopener noreferrer" class="nav-link external">
  微信公众号「SimpleAI」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/guo-bi-yang-78" target="_blank" rel="noopener noreferrer" class="nav-link external">
  知乎「蝈蝈」
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.jianshu.com/u/f4fe92da869c" target="_blank" rel="noopener noreferrer" class="nav-link external">
  简书
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>XLNet简读</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#要解决的痛点" class="sidebar-link">要解决的痛点</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#bert的优缺点" class="sidebar-link">BERT的优缺点</a></li><li class="sidebar-sub-header"><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#gpt模型的优缺点" class="sidebar-link">GPT模型的优缺点</a></li></ul></li><li><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#permutation-language-modeling-plm" class="sidebar-link">Permutation Language Modeling（PLM）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#目标函数" class="sidebar-link">目标函数</a></li></ul></li><li><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#使用plm要解决的问题" class="sidebar-link">使用PLM要解决的问题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#two-stream-self-attention" class="sidebar-link">Two-Stream Self-Attention</a></li></ul></li><li><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#借鉴transformer-xl" class="sidebar-link">借鉴Transformer-XL</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/paper_notes/notes/%20XLNet%E7%AE%80%E8%AF%BB.html#xlnet的效果" class="sidebar-link">XLNet的效果</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="xlnet-generalized-autoregressive-pretraining-for-language-understanding"><a href="#xlnet-generalized-autoregressive-pretraining-for-language-understanding" class="header-anchor">#</a> <strong>XLNet: Generalized Autoregressive Pretraining for Language Understanding</strong></h1> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20220322180723.png" alt="image-20220322180714470"></p> <p>XLNet发表在NeurIPS-19上，是BERT之后相当有名的一个工作，这里简单总结一下它的要点。</p> <h2 id="要解决的痛点"><a href="#要解决的痛点" class="header-anchor">#</a> 要解决的痛点</h2> <p>XLNet，如果做一个总结的话，可以理解为是结合了BERT、GPT、Transformer-XL这些代表性工作各自优点的一个综合体。那么自然，GPT-2、BERT都是各自有个字的缺点。</p> <h3 id="bert的优缺点"><a href="#bert的优缺点" class="header-anchor">#</a> BERT的优缺点</h3> <p>BERT在pre-training中主要的技术就是Masked-language-modeling（MLM），MLM为了实现双向语言模型，在训练过程中，会随机挑选15%的token用<code>[MASK]</code>来替换掉，然后用带<code>[MASK]</code>的上下文来预测这个token。这就会造成预训练和微调时的不匹配（pretrain-finetune discrepancy）问题。虽然BERT采用了一些trick来缓解这个问题（15%的选中词里面，有80%的概率会被MASK，剩下的要么替换成随机词，要么不变），但这是杯水车薪的。</p> <p>这就是BERT存在的一个问题，但是MLM毕竟实现了双向的语言模型，所以比传统的单向语言模型还是更好的。</p> <h3 id="gpt模型的优缺点"><a href="#gpt模型的优缺点" class="header-anchor">#</a> GPT模型的优缺点</h3> <p>GPT模型，跟BERT是不一样的路子，是单向语言模型，采用auto-regressive的方式，用前面的词去预测下一个词，这么做自然就不需要对token进行MASK，所以不存在pretrain-finetune discrepancy问题。但是这种单向语言模型，在语义表示的方面，自然没有双向的好。</p> <p>XLNet，就是为了解决BERT和GPT各自的问题，想把他们各自的优点结合在一起的一个工作。</p> <h2 id="permutation-language-modeling-plm"><a href="#permutation-language-modeling-plm" class="header-anchor">#</a> Permutation Language Modeling（PLM）</h2> <p>这个PLM，就是XLNet最主要的贡献和亮点。</p> <p>对于BERT的使用MASK token导致的pretrain-finetune discrepancy问题，如果还是用BERT采用的denoising auto-encoding的方式的话，那是没办法的。想不用MASK，就只能用auto-regressive方式，那如何在auto-regressive方式下还能进行双向建模呢？作者们就提出了这个PLM的想法。</p> <p>一图胜千言，原文理论部分不是很容易看明白，但他们在附录里花了一张图，就很容易懂：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20220322183115.png" alt="image-20220322183115674"></p> <p>如上图所示，一个序列正常的顺序假设是1-2-3-4，然后我们想对<strong>位置3</strong>的token进行双向语言模型预测。</p> <p>PLM的做法就是，先把1-2-3-4的顺序打乱（假设序列长度为<code>L</code>，那么就有 <code>L!</code>种token的不同排列组合）：</p> <ul><li>① 3-2-4-1</li> <li>② 2-4-3-1</li> <li>③ 1-4-2-3</li> <li>④ 4-3-1-2</li> <li>...</li></ul> <p>然后，对于每一种排列，我们使用经典的auto-regressive的方式对3进行预测，那么：</p> <ul><li>对于①，3的上文是空的，所以在通过self-attention的时候，没有一个被attend，相当于凭空预测3；</li> <li>对于②，3的上文是2、4，所以2、4的位置被attend，相当于用2、4来预测3；</li> <li>对于③，3的上文是1、4、2，所以1、2、4的位置被attend，相当于用1、2、4来预测3；</li> <li>对于④，3的上文是4，所以只有4的位置被attend，相当于用4来预测3。</li></ul> <p>这就是PLM的思想，通过这种方式，某个token的上下文，实际上都有可能参与预测该token，也就实现了双向的语言模型，这种设计还是挺精妙的。</p> <p>**Notice！**虽然PLM把顺序都打乱了，但实际上输入模型的，都是原始的顺序，只是在进行language model预测的时候，对所谓的“上文”进行了各种采样，因此实现了实际上的上下文建模。所以我们不用担心这里的permutation对语义的影响，模型学习的还是正常的句子。</p> <p>其实BERT还存在一个问题，那就是BERT预训练中随机MASK掉的那些词，在训练的时候没有考虑到彼此之间的关系，比方作者举的这个例子：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20220322190718.png" alt="image-20220322190717952"></p> <p>[New, York, is, a, city]这个句子，如果把New, York都给mask掉了，那么BERT在预测New的时候，使用的上下文就只有[is, a, city]，预测York的时候使用的上下文也只有[is, a, city]，而New和York之间的依赖关系就被忽略了。</p> <p>而XLNet不同，假设对句子随机采样的一个排序是[is, a, city, New, York], 那么预测New的时候，使用的上文是[is, a, city]，预测York的时候使用的上文则是[New，is, a, city]，比BERT对了一个对New的考虑。</p> <p>其实你细想一下为啥XLNet可以做到？因为<strong>XLNet不会同时去预测New和York，一次只预测一个token</strong>，而BERT则是同时预测所有被mask掉的词，那自然就没法考虑彼此之间的关系了。</p> <h3 id="目标函数"><a href="#目标函数" class="header-anchor">#</a> 目标函数</h3> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231508363.png" alt="image-20220322191723617"></p> <p>目标函数，就是岁所有可能permutation序列的联合概率分布的期望，比方sequence length是3，那么就有6种排列，那目标函数怎么计算呢？对于每种排列，依次计算每个位置token的概率，然后连乘再取log，最后把6种排列的结果进行平均。</p> <p>实际上，这么算的话计算开销是很大的，所以作者实际使用的方式，是名为<strong>Partial Prediction</strong>的方式，即对于一个排列，我们只对最后的几个position进行预测，前面的不管了。比方1-2-3-4的一个排列是1-3-2-4，我们设置一个截断长度=2，那么我们只做1,3-&gt;2和1,2,3-&gt;4这两个LM预测。</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231113738.png" alt="image-20220323110245318"></p> <h2 id="使用plm要解决的问题"><a href="#使用plm要解决的问题" class="header-anchor">#</a> 使用PLM要解决的问题</h2> <p>上面描述PLM感觉很美好，但直接使用会存在问题。</p> <p>首先我们看看auto-regressive是如何预测下一个词的概率的：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231135234.png" alt="image-20220323113528191"></p> <p>假设一个sequence的排列是a-b-c-d，那我们预测c的时候使用的上文就是a-b，但是套用上面的公式的话，</p> <p>原始的sequence可能是c-a-b-d，也可能是a-b-c-d，还可能是a-c-d-b，所有的可能，只要在a-b-c-d的排列下，计算出来的概率就是一模一样的。也就是说，还用传统的计算方法的话，<strong>会忽略要预测词的位置信息</strong>，这就肯定很影响学习的效果。</p> <p>所以，我们要做的改进，就是把要预测词的位置信息加进去：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231154483.png" alt="image-20220323115436438"></p> <p>但是，我们加入的也只能是位置信息，不能把内容信息给加进去了，不然预测就没有意义了。</p> <p>上面这个公式实际上是Transformer的最后一层的处理，如何把要预测的词的位置，通过层层的self-attention，把位置信息给传上来的同时不传递内容信息呢？另外，我一个sequence输进来，也不能每次只预测一个token吧，那样效率就太低了，那对于要预测的token1如果只有位置信息，而要预测的token2如果要使用token1的信息的话，那岂不是矛盾了。</p> <p>说得再具体一点，对于一个序列x1-x2-x3-x4，我们在预测x3的时候，不希望使用x3的内容，只使用它的位置；然后我们还想预测x4，这个时候，我们有需要同时获取x3的内容和位置。要想做到这一点，传统的Transformer结构是无法做到的。那么，我们能怎么办呢：</p> <ul><li>首先得把token representation分成embedding和position两部分，不能混在一起</li> <li>得有两套self-attention机制来帮忙传递信息</li></ul> <p>这就是作者提出的<strong>Two-Stream Self-Attention</strong>方法：</p> <h3 id="two-stream-self-attention"><a href="#two-stream-self-attention" class="header-anchor">#</a> Two-Stream Self-Attention</h3> <p>content stream attention和query stream attention：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231252681.png" alt="image-20220323125241646"></p> <p>上图中，我们可以理解蓝色的部分就是content，绿色的部分就是position。</p> <ul><li>(a)代表Content stream attention，它就是传统Transformer中一模一样的self-attention。它的作用是保证每个token的内容信息的传递。</li> <li>(b)代表Query stream attention，它对于当前的token，只访问position，对于其他的token，则访问content。当前token的position作为query，所有其他token的content作为K和V，然后使用position跟K计算attention权重，再对V进行加权求和，就得到下一层的position。总之，这个stream的作用就是保证每个token只有位置传递上去。</li> <li>另外注意，content stream只使用了content，但是query stream同事使用了position和content，这种交互使得在参数不断更新之后，content里面也会包含position的信息，而position中本来也包含了content的信息，只是不包含当前位置的content。</li></ul> <p>综合起来，我们的decoder就是这样的：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231259573.png" alt="image-20220323125928535"></p> <p>图中最上面红色虚线，代表预测，比如要预测x3的话，我们使用的context hidden representation就是图中的g3，g3只包含x3的位置信息，以及x3的上文的所有信息；而要预测x4的时候，如果x3在x4的上文中，那么x3的所有信息都可以被g4获取。这样，上面提到的矛盾就可以解决了。</p> <h2 id="借鉴transformer-xl"><a href="#借鉴transformer-xl" class="header-anchor">#</a> 借鉴Transformer-XL</h2> <p>实际上XLNet的名字就是沿用了Transformer-XL来取的，因为这里在backbone上主要就采用了Transformer-XL的设计，即采用相对位置编码（relative positional encoding）和片段重用机制（ segment recurrence mechanism），这样可以让模型接收更长的序列，从而对长文本的表示更好。具体这里不展开了。</p> <h2 id="xlnet的效果"><a href="#xlnet的效果" class="header-anchor">#</a> XLNet的效果</h2> <p>XLNet的其他细节，就暂不列出了，下面贴一下跟bert的公平对比，反正棒就完事儿了：</p> <p><img src="https://gitee.com/beyond_guo/typora_pics/raw/master/typora/202203231444362.png" alt="image-20220323144437307"></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.41fc4255.js" defer></script><script src="/assets/js/2.a266bdc2.js" defer></script><script src="/assets/js/77.0018ed7f.js" defer></script>
  </body>
</html>
