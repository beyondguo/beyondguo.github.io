(window.webpackJsonp=window.webpackJsonp||[]).push([[36],{403:function(t,s,a){"use strict";a.r(s);var n=a(44),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"【dl笔记9】迁移学习-光明正大借用前人劳动成果"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#【dl笔记9】迁移学习-光明正大借用前人劳动成果"}},[t._v("#")]),t._v(" 【DL笔记9】迁移学习——光明正大借用前人劳动成果")]),t._v(" "),a("blockquote",[a("p",[t._v("好一阵没有继续更新深度学习系列的笔记了，其实文章的技术方面的内容在暑假就已经准备好了，只是开学一个月事情比较多，一直没时间整理下来。今天咱们再续前缘，继续完善深度学习的基础知识框架。今天的主角是——"),a("strong",[t._v("迁移学习(Transfer learning)")]),t._v("。而且，这次不让大家空手离开，特地准备了一个实践案例——"),a("strong",[t._v("手势识别，让你的小电脑看得懂手语")]),t._v("！")])]),t._v(" "),a("h3",{attrs:{id:"一、什么是迁移学习-有何来历"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、什么是迁移学习-有何来历"}},[t._v("#")]),t._v(" 一、什么是迁移学习，有何来历？")]),t._v(" "),a("p",[t._v("在前面学习了CNN的基本原理，并且了解了一些著名的框架之后，我们似乎都有一种冲动————训练一个自己的模型，做出一些有趣的应用。毕竟，学过前面的知识之后，我们发现很多网络结构虽然层数多，但是其实很简单，比如VGG，十分富有规律性。")]),t._v(" "),a("p",[t._v("但是，自己从0到1训练一个模型十分困难，主要在下面两个方面：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("没有足够多的数据：")]),t._v("\n深度学习模型，最需要的就是大数据。没有大量的数据进行支撑，我们很难训练出一个理想的模型。")]),t._v(" "),a("li",[a("strong",[t._v("没有足够多的计算、时间资源")]),t._v("\n即使我们有了足够多的数据，计算量也是一个大问题。那些著名的模型，基本都是用最好的GPU，动辄训练一个星期得到的。对于我们初学者来说，这个条件很难满足。而且，实际操作中，有很多的trick，这需要大量的试验。")])]),t._v(" "),a("p",[t._v("因此，从头开始训练一个模型很多时候是不现实的。这个时候，迁移学习就有了它的用武之地。让我们在 "),a("strong",[t._v("数据量不大、资源也不够的情况下，也可以训练很好的模型")]),t._v("。")]),t._v(" "),a("p",[a("big",[a("strong",[t._v("迁移学习")])]),t._v("，顾名思义，就是把别人的模型，迁移过来，来学习自己的任务。")],1),t._v(" "),a("p",[t._v("对于迁移学习，我可以打一个不是那么恰当的"),a("strong",[t._v("比方")]),t._v("：别人修建了一间房子，住了一段时候不住了，转手给你住，请问你需要把房子推倒了重建吗？当然不需要，我们顶多把里面的重新装修一下，就行了。")]),t._v(" "),a("p",[t._v("现成的模型就是别人造好的房子，你拿过来，结构、参数都可以不变，因为很多东西是通用的。我们只用把我们属于自己个性化任务的那部分，按照自己喜好改造一下即可。")]),t._v(" "),a("p",[t._v("比方说，我们可以把在ImageNet上用千万张图片训练好的VGG拿过来（可以识别1000个种类），把最后两个FC层（全连接层）给拿掉，换成我们自己定义的FC层和输出层，其他的层则保持结构不变，参数也采用之前的参数。然后拿我们自己的金毛、二哈的照片（也许只有不到一千张）去训练，得到一个分辨金毛二哈的分类器。")]),t._v(" "),a("h3",{attrs:{id:"二、为什么可以这样做"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、为什么可以这样做"}},[t._v("#")]),t._v(" 二、为什么可以这样做？")]),t._v(" "),a("p",[t._v("如果你还记得我之前的文章【DL笔记8】，你应该已经熟悉了卷积神经网络(CNN)各层学习到的特征有什么特点。这里简单回顾一下：")]),t._v(" "),a("ol",[a("li",[t._v("越浅的层，学习的特征越简单基础；越深的层，学习的特征越复杂而具体。所以，我们可以发现，从浅层到深层，识别的特征从边缘、线条、颜色，过渡到纹理、图案，再过渡到复杂图形，甚至到具体的物品。")]),t._v(" "),a("li",[a("strong",[t._v("越前面的层，特征越具有一般性，越深的层，特征越具有场景的特殊性。")])])]),t._v(" "),a("p",[t._v("尤其是最后一条，我们知道，"),a("strong",[t._v("一个训练好的CNN模型，其实已经具备了很多通用的特征")]),t._v("。比如用ImageNet训练出来的VGG，它的各层的特征已经包含了识别各种物体的基本属性，比如棱角、颜色、基本的形状等等，而我们想训练一个金毛二哈分类器，其实也要从这些底层的基本特征来学起，因此，完全可以直接把训练好的VGG和配套的特征(特征即filters的参数)拿过来，我们只用训练一些高级特征即可。")]),t._v(" "),a("h4",{attrs:{id:"运用transfer-learning一般有两种方式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#运用transfer-learning一般有两种方式"}},[t._v("#")]),t._v(" ——>运用transfer learning一般有两种方式：")]),t._v(" "),a("ol",[a("li",[t._v("用训练好的模型作为"),a("strong",[t._v("特征抽取器")]),t._v("(Pre-trained model as a "),a("strong",[t._v("feature extractor")]),t._v(")")])]),t._v(" "),a("p",[t._v("这个是我们最常见的形式。前面说了，其实训练好的很多CNN模型，已经可以从我们的图片中提取出很多有用的特征了，那么这个时候我们可以直接拿过来，把这些CNN当做我们特征提取的工具。特征提取好了之后，我们再在后面接上一个简单的分类器，即可实现我们的学习任务。\n这种方法，往往用在我们没有大量的训练样本的时候，也可以取得出色的效果。(今天我们就会用这种方法，来实现一个“手势”的分类器！)")]),t._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[t._v("对训练好的模型进行"),a("strong",[t._v("微调")]),t._v("("),a("strong",[t._v("Fine-tuning")]),t._v(" a pre-trained model)")])]),t._v(" "),a("p",[t._v("Fine-tune这个词大家应该经常听说。它的意思是把训练好的模型，在它原来的基础上，借助我们自己的训练样本，进行微调。\n什么意思呢？在上面的直接当做feature extractor中，我们是直接固定好之前训练好的参数，只是把网络最后几层去掉，换成一个小型的分类器进行训练，训练的实际上是我们的小分类器。但是fine-tune不光是把原网络的末端换成我们自己的分类器，还会把整个网络的参数都继续训练，只不过我们不用从头训练，而是在原来的参数的基础上接着训练即可。\n这种方法，一般用于我们自己也有大量的数据，这个时候，借用他人成功的网络结构和预训练参数，再充分利用自己的数据，就可以得到十分好的效果。\n如果我们的样本量不大，那就没必要去fine-tune了，因为对效果的提升不会有很大的帮助，直接拿来用即可。")]),t._v(" "),a("h3",{attrs:{id:"三、亲手试一试吧"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三、亲手试一试吧"}},[t._v("#")]),t._v(" 三、亲手试一试吧！")]),t._v(" "),a("p",[t._v("看了半天猪在跑，不如亲自尝一尝猪肉！\n这里，我拿来了吴恩达的一个数据集，是他们团队自己采集的————0~5的手势。长的如下这样：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624598534927-image.png",alt:""}})]),t._v(" "),a("p",[t._v("这个任务是不是看起来比之前的MNIST手写数字识别要难多了？手写数字毕竟单纯，而且那个数据集十分地规范，而且图片还都是单通道的！。但是吴恩达的这个数据集就复杂多了，是他们团队亲自拍摄的，每一个数字的手势会有很大的差别，形态各异，还是正经的RGB图片，具体大家可以下载数据集之后自己去查看。这个任务就比较贴近我们的实际生活了。")]),t._v(" "),a("p",[t._v("我们的目标是：达到"),a("strong",[t._v("90%以上的测试集准确率")]),t._v("！")]),t._v(" "),a("p",[t._v("由于这个任务稍微有点复杂，所以我们需要借助一个更复杂的网络结构来实现。于是自然而然地想到用Transfer learning。这里，我选择的是著名的"),a("strong",[t._v("VGG-19")]),t._v("，是一个有19层的卷积神经网络。")]),t._v(" "),a("p",[t._v("我先画一张图来表示一下我们要做的事儿：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624598549463-image.png",alt:""}})]),t._v(" "),a("p",[t._v("话不多说，上代码吧：")]),t._v(" "),a("h4",{attrs:{id:"_1-引入相关的包"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-引入相关的包"}},[t._v("#")]),t._v(" 1. 引入相关的包：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" h5py\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" keras\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vgg19 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VGG19\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Model\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("GlobalMaxPooling2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("matplotlib inline\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("p",[t._v("相关说明：\n"),a("strong",[t._v("h5py")]),t._v("是一个python读取h5数据文件的工具，本实验中，我们的数据是存在h5中，这种存储方式十分经济。\n"),a("strong",[t._v("在keras中，已经内置了一些著名的模型的结构")]),t._v("，比如VGG19，因此我们可以直接通过keras.applications调用。")]),t._v(" "),a("h4",{attrs:{id:"_2-加载数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-加载数据"}},[t._v("#")]),t._v(" 2. 加载数据：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 导入数据：")]),t._v("\ntrain_dataset "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" h5py"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("File"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'datasets/train_signs.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"r"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrain_set_x_orig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train_set_x"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \ntrain_set_y_orig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train_set_y"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \ntest_dataset "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" h5py"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("File"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'datasets/test_signs.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"r"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_set_x_orig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test_set_x"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \ntest_set_y_orig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test_set_y"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\nclasses "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"list_classes"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the list of classes")]),t._v("\n\ntrain_set_y_orig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_set_y_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_set_y_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_set_y_orig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_set_y_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_set_y_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"-----Reshaped data:"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nX_train "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_set_x_orig"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),t._v("\nX_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_set_x_orig"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),t._v("\nY_train "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_categorical"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_set_y_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nY_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_categorical"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_set_y_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Y_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br")])]),a("p",[t._v("这部分代码没什么好看的，唯一需要留意的就是后面的“reshape”的过程，首先我们要对训练集进行"),a("strong",[t._v("归一化处理")]),t._v("，然后要把标签值通过"),a("strong",[t._v("keras.utils.to_categorical")]),t._v("方法转换成"),a("strong",[t._v("one-hot")]),t._v("的形式，方便训练。")]),t._v(" "),a("p",[a("strong",[t._v("为了防止有的读者对one-hot形式不熟悉，我这里解释一下：")]),t._v("\n本实验中，我们的手势的标签分别是0,1,2,3,4,5这6个标签。\n在原数据中，y就是一维的，就是0~5这些数字，\none-hot表示法就是这样变：\n0 --\x3e (1,0,0,0,0,0)\n1 --\x3e (0,1,0,0,0,0)\n2 --\x3e (0,0,1,0,0,0)\n3 --\x3e (0,0,0,1,0,0)\n4 --\x3e (0,0,0,0,1,0)\n5 --\x3e (0,0,0,0,0,1)")]),t._v(" "),a("p",[t._v("为啥要这么处理呢？\n因为我们知道，我们的输出层，无论是sigmoid还是Softmax，都是这样的one-hot形式，所以我们需要把我们的标签改成一致的。那么这个keras.utils.to_categorical方法，大家可得记住了，日后会经常用到。")]),t._v(" "),a("p",[t._v("加载完之后，可以随便挑一个出来看看，我们的训练集长啥样：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("plt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("imshow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_set_x_orig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("得到")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624598565090-image.png",alt:""}})]),t._v(" "),a("h4",{attrs:{id:"_3-构造模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-构造模型"}},[t._v("#")]),t._v(" 3.构造模型")]),t._v(" "),a("p",[t._v("我们首先引入训练好的VGG模型，同时把原模型的FC层去掉：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("base_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VGG19"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'imagenet'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("include_top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("我们看看参数：\n"),a("code",[t._v("weights = 'imagenet'")]),t._v("说明，我们这个VGG19是通过ImageNet的图片训练的，ImageNet上有成千上万张图片，因此已经学得了大量的各种图片的特征，我们完全可以拿过来用。\n"),a("code",[t._v("include_top = False")]),t._v("表示去掉模型的最后的全连接层（一般是两层，其中一个是Softmax输出层）。")]),t._v(" "),a("p",[t._v("然后，我们重新构造最后两层，我们先把上面的去掉尾巴的模型做一个pooling，简化计算量，然后添加一个128单元的FC层，用relu激活，最后输出层使用Softmax（就是一个6单元的FC层使用了Softmax激活）：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" base_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output\nx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GlobalMaxPooling2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npredictions "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("base_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("outputs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" layer "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" base_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainable "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n    \nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])]),a("p",[t._v("注意，由于我们的样本数量比较少，所以不用去fine-tune原模型的参数，因此我们直接"),a("strong",[t._v("把原模型的参数固定")]),t._v("下来，所以上面我们需要设置"),a("code",[t._v("layer.trainable = False")]),t._v(".")]),t._v(" "),a("p",[t._v("模型的编译就很简单了，直接选择优化器、损失函数即可。")]),t._v(" "),a("h4",{attrs:{id:"_4-模型的训练和评估"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-模型的训练和评估"}},[t._v("#")]),t._v(" 4.模型的训练和评估")]),t._v(" "),a("p",[t._v("这里就直接上代码了：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nscore "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("evaluate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Y_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Total loss:"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Test accuracy:"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br")])]),a("p",[t._v("迭代次数大概20~30次吧，我的电脑只能使用CPU，每次迭代大概70多秒，所以总时间需要约半小时。\n最后的输出结果(只展示最后3个epoch的数据，不然太占地儿了)：")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("Epoch 1/3\n1080/1080 [==============================] - 72s 67ms/step - loss: 0.2111 - acc: 0.9630\nEpoch 2/3\n1080/1080 [==============================] - 75s 69ms/step - loss: 0.1942 - acc: 0.9676\nEpoch 3/3\n1080/1080 [==============================] - 83s 77ms/step - loss: 0.1845 - acc: 0.9685\n\n120/120 [==============================] - 8s 67ms/step\nTotal loss: 0.2701009293397268\nTest accuracy: 0.9166666626930237\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])]),a("p",[t._v("可以看到，最后的测试集准确率达到了91.67%！目标达成！")]),t._v(" "),a("p",[t._v("准确率不错，为了更直观地看看模型的效果，我们不妨做一个可交互的查看的结果的方法：输入一个图片，计算机返回预测结果：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624598578722-image.png",alt:""}})]),t._v(" "),a("hr"),t._v(" "),a("blockquote",[a("p",[t._v("至此，我们已经知道了迁移学习的原理，并亲自动手实践了一下。\n可以看出，虽然手势识别比我们之前做的数字识别要复杂的多，但是相比ImageNet的图片识别，还是太简单。因此，我们用VGG来做迁移学习，显然是“杀鸡用牛刀”，所以我们简单地训练20来次，就可以达到很高的准确率，如果使用GPU的话，那10分钟的训练，估计准确率就可以接近100%了（我的猜测）。这也说明了迁移学习的强大和方便之处。\n因此，在我们的实际任务中，其实可以多想想，是否有机会**“站在巨人的肩膀上”**，让我们事半功倍。")])])])}),[],!1,null,null,null);s.default=e.exports}}]);