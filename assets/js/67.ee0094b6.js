(window.webpackJsonp=window.webpackJsonp||[]).push([[67],{435:function(t,s,a){"use strict";a.r(s);var i=a(44),n=Object(i.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"词向量why-and-how"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#词向量why-and-how"}},[t._v("#")]),t._v(" 词向量Why and How")]),t._v(" "),a("blockquote",[a("p",[t._v("从今天起，我们一起来学习词向量word2vector（简称w2v）相关的知识。虽然，对于NLP来说，w2v技术和操作已经烂大街了，随便一个NLP任务，底层基本都要搞一个w2v，但是到底为什么需要w2v，它背后的思想是什么，是怎么训练得到的，也许很多经常使用w2v的人都不一定很清楚。不清楚，就会导致我们知道w2v很好用很神奇，但是出了问题却不知道从哪里去改善。\n所以，我会边学习斯坦福cs224n的NLP课程的词向量内容，边查阅其他的资料，并结合自己的实践经验来写下关于w2v的一系列笔记。")])]),t._v(" "),a("p",[t._v("本文介绍两个方面：")]),t._v(" "),a("ol",[a("li",[t._v("为什么我们需要w2v——WHY")]),t._v(" "),a("li",[t._v("得到w2v的大致思路是怎样的——HOW")])]),t._v(" "),a("h2",{attrs:{id:"一、why"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、why"}},[t._v("#")]),t._v(" 一、WHY")]),t._v(" "),a("p",[t._v("词，是NLP要对付的一个基本语言单位，我们在用计算机处理海量文本的时候，希望尽可能地让机器明白词蕴含的信息，这样可以大大地提高文本分类、文本聚类、文本生成、对话、翻译等等任务的准确性。所以，我们需要解决一个基本问题：")]),t._v(" "),a("blockquote",[a("p",[t._v("####如何表示一个词?")])]),t._v(" "),a("h3",{attrs:{id:"_1-wordnet"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-wordnet"}},[t._v("#")]),t._v(" 1.WordNet")]),t._v(" "),a("p",[t._v("在英文中，我们可以借助WordNet，来导出一个词的同义词、近义词等等各种信息。\nWordNet是由Princeton 大学的心理学家，语言学家和计算机工程师联合设计的一种基于认知语言学的英语词典。\n网址：https://wordnet.princeton.edu/")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624614013757-image.png",alt:"wordnet上查询一个词"}})]),t._v(" "),a("p",[t._v("我们可以下载wordnet，也可以通过NLTK等等包来获取，便可以在计算机程序里查询我们想要的词。")]),t._v(" "),a("p",[t._v("对于"),a("strong",[t._v("中文")]),t._v("的话，也有类似的对应的wordnet。下面展示的这个"),a("strong",[t._v("Chinese wordnet")]),t._v("由台湾国立大学的学者们设计搭建。而且，他们还做了相应的"),a("strong",[t._v("知识图谱")]),t._v("，更加直观地表达词之间的关系。不过都是繁体中文。\n网址：http://lope.linguistics.ntu.edu.tw/cwn/")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624614030099-image.png",alt:"Chinese wordnet"}})]),t._v(" "),a("p",[t._v("这些资源都十分地优秀，而且都是花费研究机构多年的心血才搭建手工而成，所以质量也很高，在很多情况下可以满足我们的需求。")]),t._v(" "),a("p",[a("strong",[t._v("但是通过wordnet来进行词义表示有什么问题呢？")])]),t._v(" "),a("ol",[a("li",[t._v("难以更新,毕竟这个靠人力搭建，无法应对"),a("strong",[t._v("新词")]),t._v("或者词的"),a("strong",[t._v("新含义")])]),t._v(" "),a("li",[t._v("难以定量地计算不同词之间的"),a("strong",[t._v("相似度")])])]),t._v(" "),a("p",[t._v("所以，wordnet更多地是作为一个“词典”，提供一个词意思的参考或者补充，而往往无法应对现在NLP的许多任务。")]),t._v(" "),a("p",[t._v("所以我们需要其他的表示方法。")]),t._v(" "),a("h3",{attrs:{id:"_2-one-hot表示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-one-hot表示"}},[t._v("#")]),t._v(" 2.One-hot表示")]),t._v(" "),a("p",[t._v("假设我们要研究的一个问题，涉及到的语料共有10000个不同的词。比如把它们按照首字母排序：\n[阿，爱，······，明天，······，你，······，智能，职业]\n那么"),a("strong",[t._v("每个词可以表示成一个10000维的向量")]),t._v("。比如：\n“阿”可以表示为这样的向量：\n[1,0,0,0,......,0]\n“爱”可以表示为：\n[0,1,0,0,......,0]\n最后一个词“职业”可以表示为：\n[0,0,0,0,......,1]\n因为每个词对应的向量只有一个位置为1，故称为“独热编码（one-hot）”。\n"),a("strong",[t._v("说白了，这种向量表示的就是这个词在我们语料库中的一个index。")])]),t._v(" "),a("p",[t._v("这种方法是很流行的，在很多机器学习应用中，都是对词语进行这样的处理的。")]),t._v(" "),a("p",[t._v("但是这样做一个"),a("strong",[t._v("最大的问题")]),t._v("就在于：任何两个不同的词的向量，都是**“正交”"),a("strong",[t._v("的，内积为0.\n这样的话，即使是意思十分相似的词，也无法从one-hot词向量中看出它们的联系。因此，这种表示方法，我们")]),t._v("无法衡量两个词的相似度**。而相似度无法计算，很多NLP任务就无法进行。")]),t._v(" "),a("h3",{attrs:{id:"_3-word2vector词向量模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-word2vector词向量模型"}},[t._v("#")]),t._v(" 3.Word2Vector词向量模型")]),t._v(" "),a("p",[t._v("上面的one-hot表示，其实也是一种“词向量”，只不过这个词向量，就是由该词的index决定的，而"),a("strong",[t._v("仅由index决定，就导致了无法表示词义的问题")]),t._v("。")]),t._v(" "),a("p",[t._v("因此，我们想，为何不"),a("strong",[t._v("让词向量表示的信息更加丰富一点")]),t._v("呢？\n比如，我们可以定义一些维度，然后对一个词在每个维度上打分，这样一个词的意思不就丰富啦？\n比如，我们定义“男”，“女”，“水果”，“产品”，“地址”，这几个维度，对项目几个词来定义词向量：")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Dimension")]),t._v(" "),a("th",[t._v("国王")]),t._v(" "),a("th",[t._v("皇后")]),t._v(" "),a("th",[t._v("苹果")]),t._v(" "),a("th",[t._v("酒店")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("男")]),t._v(" "),a("td",[t._v("0.98")]),t._v(" "),a("td",[t._v("0.02")]),t._v(" "),a("td",[t._v("0.15")]),t._v(" "),a("td",[t._v("0.3")])]),t._v(" "),a("tr",[a("td",[t._v("女")]),t._v(" "),a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.99")]),t._v(" "),a("td",[t._v("0.27")]),t._v(" "),a("td",[t._v("0.2")])]),t._v(" "),a("tr",[a("td",[t._v("水果")]),t._v(" "),a("td",[t._v("0.03")]),t._v(" "),a("td",[t._v("0.07")]),t._v(" "),a("td",[t._v("0.97")]),t._v(" "),a("td",[t._v("0")])]),t._v(" "),a("tr",[a("td",[t._v("产品")]),t._v(" "),a("td",[t._v("0.11")]),t._v(" "),a("td",[t._v("0.12")]),t._v(" "),a("td",[t._v("0.89")]),t._v(" "),a("td",[t._v("0.23")])]),t._v(" "),a("tr",[a("td",[t._v("地址")]),t._v(" "),a("td",[t._v("0.07")]),t._v(" "),a("td",[t._v("0.28")]),t._v(" "),a("td",[t._v("0.04")]),t._v(" "),a("td",[t._v("0.99")])]),t._v(" "),a("tr",[a("td",[t._v("...")]),t._v(" "),a("td",[t._v("...")]),t._v(" "),a("td",[t._v("...")]),t._v(" "),a("td",[t._v("...")]),t._v(" "),a("td",[t._v("...")])])])]),t._v(" "),a("p",[t._v("上面只是我随便编的几个维度，只是举个例子，我们可以"),a("strong",[t._v("让一个词在多个维度上表示，这个维度上的数越大，就代表越具有某种属性")]),t._v("。这样的方法，就可以让意思相近的词，拥有相近的属性值，它们之间的相似度就可以很容易的表示了，比如用余弦相似度来计算向量之间的距离。")]),t._v(" "),a("p",[t._v("这是个很好的思路，这样就可以很好地表示一个词在各个维度的含义，可以尽可能地表达一个词的含义。")]),t._v(" "),a("blockquote",[a("p",[a("strong",[t._v("但是，如何定义维度呢？")])])]),t._v(" "),a("p",[t._v("实际上，我们几乎"),a("strong",[t._v("不可能定义一个完善的维度")]),t._v("。这么多的词汇，维度根本无法定义。而且，这种定义是仁者见仁智者见智，每个人对词的属性的定义都是不一样的。")]),t._v(" "),a("blockquote",[a("p",[t._v("这个时候，我们就会自然而然地想到**“深度学习”**了。")])]),t._v(" "),a("p",[t._v("因为词的"),a("strong",[t._v("属性")]),t._v("（即我说的"),a("strong",[t._v("维度")]),t._v("），相当于一个词的一个**“特征”"),a("strong",[t._v("，定义词的属性的过程，其实就是")]),t._v("“特征工程”**，当特征工程难以实施的时候，就是深度学习大展拳脚的时候了。我们用大量的文本喂给深度学习模型，然后自动训练，学得词向量。")]),t._v(" "),a("p",[t._v("下面我们来讲解，怎么利用深度学习的方法，来学习词向量。")]),t._v(" "),a("h2",{attrs:{id:"二、how"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、how"}},[t._v("#")]),t._v(" 二、HOW")]),t._v(" "),a("p",[t._v("我们怎么构造一个模型，来“学习”词向量呢？\n如果你有机器学习或者深度学习经验的话，应该，知道，最重要的是要明确"),a("strong",[t._v("目标函数")]),t._v("，也就是我们要调整模型的参数，让什么目标最大/最小。")]),t._v(" "),a("p",[t._v("针对词的表示，有语言学家提出，"),a("strong",[t._v("“一个词的意思，应该由和它一起出现的周围的词来表现”")]),t._v("。故，一个词的含义，应该由这个词的各种各样的上下文来构建。\n所以我们有两个思路来构造一个模型：")]),t._v(" "),a("ul",[a("li",[t._v("输入上下文的词，预测中心词是什么")]),t._v(" "),a("li",[t._v("输入中心词，预测上下文长什么样")])]),t._v(" "),a("p",[t._v("实际上，后面我们会讲到，这就是词向量训练的两种基本模型，前者为CBOW，后者为Skip-Gram（SG）。我们这里暂时用后者，也就是SG的方式来构建。")]),t._v(" "),a("p",[t._v("于是我们构建一个神经网络模型，可以输入一个句子中的某个词，输出其他所有词出现在这个词周围的概率。也就是说，既输出这个词附近的上下文的词，也输出跟这个词完全不相干八竿子打不着的词。\n我们希望，这个原本来语料库中这个中心词周围的词的概率的乘积越大越好，因为这个乘积就是这几个词同时出现的概率，用极大似然的思想，我们希望这个概率尽可能大。\n而这个网络的参数矩阵，实际上就是输入文本的特征，也就是我们想要得到的词向量。\n所以，我们通过梯度下降法不断更新词向量，就可以使得概率越来越大，最终就得到一组很好的词向量。")]),t._v(" "),a("p",[t._v("下面，画个图来示意、讲解：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624614053470-image.png",alt:""}})]),t._v(" "),a("p",[t._v("假设我们从我们的语料库里面随便挑出一句话。我们想"),a("strong",[t._v("预测如果中心词是natural的话，那么什么词应该出现在它的周围")]),t._v("呢？或者说，其他词出现在natural周围的概率为大多呢？概率越大，说明越可能出现在附近。\n另外，什么叫**“周围、附近”"),a("strong",[t._v("呢？我们就用")]),t._v("window来定义**。")]),t._v(" "),a("p",[t._v("所以，我们的神经网络模型是这样的：\n"),a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624614059564-image.png",alt:"神经网络模型示意图"}})]),t._v(" "),a("p",[a("strong",[t._v("注意："),a("strong",[t._v("我们的输出，是")]),t._v("输出所有的词")]),t._v("的概率，比如我们的语料库中有10000个不同的词，那么这里的输出就有10000个。但是我们"),a("strong",[t._v("更关心我们附近的词的概率")]),t._v("，也就是图中所示的**“窗口内的词”**。")]),t._v(" "),a("p",[t._v("窗口内的词，本来就是一个自然的语言组合，所以他们这些词在一起出现的概率应该高，所以他们的概率乘积就应该高。")]),t._v(" "),a("p",[t._v("这就是词向量怎么得来的基本思路。总结一下就是这张图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624614076179-image.png",alt:"好的图片，值得收藏"}})]),t._v(" "),a("p",[t._v("当然，这个示意图，只是示意了一个词。实际上，我们要把我们的语料库中的所有的词，全部遍历一次，分别计算“窗口内概率的乘积”，然后求和，最后是要使得这个所有概率之和尽可能大。")]),t._v(" "),a("p",[t._v("用cs224n课堂上的例子：\n语料库中的句子：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624614094301-image.png",alt:""}})]),t._v(" "),a("p",[t._v("我们设当前中心词的位置为t，而语料库总长度为T。设窗口大小为m，那么对于中心词 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("w")]),a("mi",[t._v("t")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("w_{t}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.58056em","vertical-align":"-0.15em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.2805559999999999em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])])]),t._v("，我们希望"),a("strong",[t._v("窗口内的词的概率之积")]),t._v("：$$\\prod_{-m\\leq j\\leq m}P(w_{t+j}|w_{t};\\theta )$$尽可能大。\n但是，我们"),a("strong",[t._v("需要遍历整个语料库")]),t._v("，也就是计算位置t从0到T的所有的词，所以最终的目标函数应该是：$$L(\\theta) = \\prod "),a("em",[t._v("{t=1}^{T}\\prod")]),t._v("{-m\\leq j\\leq m}P(w_{t+j}|w_{t};\\theta )$$\n其中，"),a("strong",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("θ")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\theta")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em","vertical-align":"0em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")])])])]),t._v("就是代表我们的词向量参数")]),t._v("，我们这个模型就是要优化这个词向量参数去让"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("L")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("θ")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("L(θ)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("L")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("最大化。\n但是上面是乘积，不打好求，一般我们取个"),a("strong",[t._v("负对数")]),t._v("，转化成最小化某个和函数，然后再取个平均，就方便求解了，所以我们转化成如下的"),a("strong",[t._v("Cost function")]),t._v("：")]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mi",[t._v("J")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("θ")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mo",[t._v("−")]),a("mfrac",[a("mn",[t._v("1")]),a("mi",[t._v("T")])],1),a("munderover",[a("mo",[t._v("∑")]),a("mrow",[a("mi",[t._v("t")]),a("mo",[t._v("=")]),a("mn",[t._v("1")])],1),a("mi",[t._v("T")])],1),a("munder",[a("mo",[t._v("∑")]),a("mrow",[a("mo",[t._v("−")]),a("mi",[t._v("m")]),a("mo",[t._v("≤")]),a("mi",[t._v("j")]),a("mo",[t._v("≤")]),a("mi",[t._v("m")])],1)],1),a("mi",[t._v("l")]),a("mi",[t._v("o")]),a("mi",[t._v("g")]),a("mi",[t._v("P")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("msub",[a("mi",[t._v("w")]),a("mrow",[a("mi",[t._v("t")]),a("mo",[t._v("+")]),a("mi",[t._v("j")])],1)],1),a("mi",{attrs:{mathvariant:"normal"}},[t._v("∣")]),a("msub",[a("mi",[t._v("w")]),a("mi",[t._v("t")])],1),a("mo",{attrs:{separator:"true"}},[t._v(";")]),a("mi",[t._v("θ")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-m\\leq j \\leq m}logP(w_{t+j}|w_{t};\\theta) \n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.09618em"}},[t._v("J")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"3.2421130000000002em","vertical-align":"-1.413777em"}}),a("span",{staticClass:"mord"},[t._v("−")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mopen nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.32144em"}},[a("span",{staticStyle:{top:"-2.314em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])]),a("span",{staticStyle:{top:"-3.23em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),a("span",{staticStyle:{top:"-3.677em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("1")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.686em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter"})]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mop op-limits"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.8283360000000002em"}},[a("span",{staticStyle:{top:"-1.882887em","margin-left":"0em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),a("span",{staticClass:"mrel mtight"},[t._v("=")]),a("span",{staticClass:"mord mtight"},[t._v("1")])])])]),a("span",{staticStyle:{top:"-3.050005em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),a("span",[a("span",{staticClass:"mop op-symbol large-op"},[t._v("∑")])])]),a("span",{staticStyle:{top:"-4.3000050000000005em","margin-left":"0em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.267113em"}},[a("span")])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mop op-limits"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.050005em"}},[a("span",{staticStyle:{top:"-1.8723309999999997em","margin-left":"0em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("−")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mrel mtight"},[t._v("≤")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")]),a("span",{staticClass:"mrel mtight"},[t._v("≤")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")])])])]),a("span",{staticStyle:{top:"-3.0500049999999996em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),a("span",[a("span",{staticClass:"mop op-symbol large-op"},[t._v("∑")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.413777em"}},[a("span")])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),a("span",{staticClass:"mord mathnormal"},[t._v("o")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("P")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.311664em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),a("span",{staticClass:"mbin mtight"},[t._v("+")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.286108em"}},[a("span")])])])])]),a("span",{staticClass:"mord"},[t._v("∣")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.2805559999999999em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mpunct"},[t._v(";")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),a("span",{staticClass:"mclose"},[t._v(")")])])])])])]),t._v(" "),a("p",[t._v("注意，上面的函数中，都要求"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("j")]),a("mo",{attrs:{mathvariant:"normal"}},[t._v("≠")]),a("mn",[t._v("0")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("j\\neq0")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[a("span",{staticClass:"mrel"},[a("span",{staticClass:"mord vbox"},[a("span",{staticClass:"thinbox"},[a("span",{staticClass:"rlap"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"inner"},[a("span",{staticClass:"mord"},[a("span",{staticClass:"mrel"},[t._v("")])])]),a("span",{staticClass:"fix"})])])])]),a("span",{staticClass:"mrel"},[t._v("=")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("0")])])])]),t._v("，也就是不计算自己跟自己的概率。")]),t._v(" "),a("h4",{attrs:{id:"总之-一句话总结一下求词向量的基本思路"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总之-一句话总结一下求词向量的基本思路"}},[t._v("#")]),t._v(" 总之，一句话总结一下求词向量的基本思路：")]),t._v(" "),a("blockquote",[a("p",[t._v("找一个巨大的文本形成语料库，选择一个窗口大小，遍历每一个词，依次把每个词输入进神经网络，词向量作为网络的参数，网络输出其他词的概率。求得所有的词的对应窗口词的条件概率的乘积，不断地调整词向量参数，使得这个概率的乘积最大化，从而得到最终的词向量。")])]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("此时，脑海里突然蹦出一句话：“够了吧！”\n所以本文就这么戛然而止了。\n下一篇文章，会详细讲解有关词向量的一些很重要的细节，和一些训练的高级方法。")]),t._v(" "),a("h4",{attrs:{id:"后记"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#后记"}},[t._v("#")]),t._v(" 后记：")]),t._v(" "),a("p",[t._v("其实虽然本文不长，但是来来回回修改了N次，酝酿了好久才写成。而且，实际上相关的内容我在暑假的时候，就已经学习过了，最近才下笔，主要是我在思考“为什么这个词向量要这么设计，为什么模型要这样设计，为什么目标函数是这个”。因为，cs224n课程上面的说法，一直不能说服我，虽然他们讲课的时候给人一种“这就是自然而然的啊”的感觉。而之前吴恩达在深度学习课程中讲的word2vector的方式跟cs224n又很不一样，这让我就更矛盾了。所以，我花了大量时间，去协调二者的方法，总结成本文的思路，这才“自我说服”了。")]),t._v(" "),a("h4",{attrs:{id:"伏笔"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#伏笔"}},[t._v("#")]),t._v(" 伏笔：")]),t._v(" "),a("p",[t._v("这里想提前埋下一个伏笔，也是我想写下有关词向量系列文章的主要原因之一：\nw2v的一个最基本用途就是寻找similar words（相似词），但是w2v的一个"),a("strong",[t._v("最明显的局限")]),t._v("就是，这个相似，"),a("strong",[t._v("是“分布相似”")]),t._v("，而"),a("strong",[t._v("不是语义相似")]),t._v("。虽然，语义相似的词，通常分布也是相似的，但是分布相似的词，语义可能很不相似！\n这个问题到底怎么解释？为什么训练出来的词向量就是反映的分布相似度？是什么原因导致的？这些问题，我们后面再详细探讨！")])])}),[],!1,null,null,null);s.default=n.exports}}]);