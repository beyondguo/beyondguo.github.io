(window.webpackJsonp=window.webpackJsonp||[]).push([[62],{431:function(s,t,a){"use strict";a.r(t);var n=a(44),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("blockquote",[a("p",[a("strong",[s._v("「Huggingface🤗NLP笔记系列-第5集」")]),s._v("\n最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的精简+注解版。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。")])]),s._v(" "),a("ul",[a("li",[s._v("官方教程网址：https://huggingface.co/course/chapter1")]),s._v(" "),a("li",[s._v("本期内容对应网址：https://huggingface.co/course/chapter2/5?fw=pt")]),s._v(" "),a("li",[s._v("本系列笔记的"),a("strong",[s._v("GitHub")]),s._v("： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP")])]),s._v(" "),a("hr"),s._v(" "),a("h1",{attrs:{id:"attention-mask在处理多个序列时的作用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#attention-mask在处理多个序列时的作用"}},[s._v("#")]),s._v(" "),a("code",[s._v("attention_mask")]),s._v("在处理多个序列时的作用")]),s._v(" "),a("p",[s._v("现在我们训练和预测基本都是批量化处理的，而前面展示的例子很多都是单条数据。单条数据跟多条数据有一些需要注意的地方。")]),s._v(" "),a("h2",{attrs:{id:"处理单个序列"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#处理单个序列"}},[s._v("#")]),s._v(" 处理单个序列")]),s._v(" "),a("p",[s._v("我们首先加载一个在情感分类上微调过的模型，来进行我们的实验（注意，这里我们就不能能使用"),a("code",[s._v("AutoModel")]),s._v("，而应该使用"),a("code",[s._v("AutoModelFor*")]),s._v("这种带Head的model）。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" pprint "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" pprint "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 这个pprint能让打印的格式更好看一点")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" AutoTokenizer\ncheckpoint "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'distilbert-base-uncased-finetuned-sst-2-english'")]),s._v("\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("对一个句子，使用tokenizer进行处理：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("s "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Today is a nice day!'")]),s._v("\ninputs "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" return_tensors"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'pt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2651")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2003")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1037")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3835")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2154")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("999")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("可以看到，这里的inputs包含了两个部分："),a("code",[s._v("input_ids")]),s._v("和"),a("code",[s._v("attention_mask")]),s._v(".")]),s._v(" "),a("p",[s._v("模型可以直接接受"),a("code",[s._v("input_ids")]),s._v("：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("input_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-4.3232,  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4.6906")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("grad_fn")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("AddmmBackward"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("也可以通过"),a("code",[s._v("**inputs")]),s._v("同时接受"),a("code",[s._v("inputs")]),s._v("所有的属性：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[s._v("tensor([[-4.3232,  4.6906]], grad_fn=<AddmmBackward>)\n")])])]),a("p",[s._v("上面两种方式的"),a("strong",[s._v("结果是一样的")]),s._v("。")]),s._v(" "),a("h2",{attrs:{id:"但是当我们需要同时处理多个序列时-情况就有变了"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#但是当我们需要同时处理多个序列时-情况就有变了"}},[s._v("#")]),s._v(" 但是当我们需要同时处理"),a("strong",[s._v("多个序列")]),s._v("时，情况就有变了！")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("ss "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Today is a nice day!'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n      "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'But what about tomorrow? Im not sure.'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\ninputs "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" return_tensors"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'pt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2651")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2003")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1037")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3835")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2154")]),s._v(",   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("999")]),s._v(",   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),s._v(",     "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(",     "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(",\n             "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2021")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2054")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2055")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4826")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1029")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10047")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2025")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2469")]),s._v(",  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1012")]),s._v(",\n           "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("然后，我们试着直接把这里的"),a("code",[s._v("input_ids")]),s._v("喂给模型")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("input_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 第一个句子原本的logits是 [-4.3232,  4.6906]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-4.1957,  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4.5675")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3.9803")]),s._v(", -3.2120"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("grad_fn")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("AddmmBackward"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("发现，"),a("strong",[s._v("第一个句子的"),a("code",[s._v("logits")]),s._v("变了")]),s._v("！")]),s._v(" "),a("p",[s._v("这是"),a("strong",[s._v("因为在padding之后，第一个句子的encoding变了，多了很多0， 而self-attention会attend到所有的index的值，因此结果就变了")]),s._v("。")]),s._v(" "),a("p",[s._v("这时，就需要我们不仅仅是传入"),a("code",[s._v("input_ids")]),s._v("，还需要给出"),a("code",[s._v("attention_mask")]),s._v("，这样模型就会在attention的时候，不去attend被mask掉的部分。")]),s._v(" "),a("p",[s._v("因此，"),a("strong",[s._v("在处理多个序列的时候，正确的做法是直接把tokenizer处理好的结果，整个输入到模型中")]),s._v("，即直接"),a("code",[s._v("**inputs")]),s._v("。\n通过"),a("code",[s._v("**inputs")]),s._v("，我们实际上就把"),a("code",[s._v("attention_mask")]),s._v("也传进去了:")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-4.3232,  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4.6906")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3.9803")]),s._v(", -3.2120"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("grad_fn")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("AddmmBackward"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("现在第一个句子的结果，就跟前面单条处理时的一样了。")])])}),[],!1,null,null,null);t.default=e.exports}}]);