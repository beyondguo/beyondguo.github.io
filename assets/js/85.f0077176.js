(window.webpackJsonp=window.webpackJsonp||[]).push([[85],{460:function(t,a,e){"use strict";e.r(a);var s=e(44),o=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"google的-pathways-理想-与-palm-现实"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#google的-pathways-理想-与-palm-现实"}},[t._v("#")]),t._v(" Google的 Pathways（理想）与 PaLM（现实）")]),t._v(" "),e("h2",{attrs:{id:"pathways构想"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pathways构想"}},[t._v("#")]),t._v(" Pathways构想")]),t._v(" "),e("p",[t._v("Google 在2021年提出了Pathways的构想：")]),t._v(" "),e("p",[t._v("当前模型的主要问题：")]),t._v(" "),e("ul",[e("li",[t._v("基本都是一个模型做一个任务；")]),t._v(" "),e("li",[t._v("在一个通用的模型上继续fine-tune，会遗忘很多其他知识；")]),t._v(" "),e("li",[t._v("基本都是单模态；")]),t._v(" "),e("li",[t._v("基本都是 "),e("strong",[t._v("dense")]),t._v(" 模型，在完成一个任务时（不管难易程度），网络的所有参数都被激活和使用；")])]),t._v(" "),e("p",[e("strong",[t._v("Pathways")]),t._v(" 的愿景 —— 一个跟接近人脑的框架：")]),t._v(" "),e("ul",[e("li",[t._v("一个模型，可以做多任务，多模态")]),t._v(" "),e("li",[t._v("sparse model，在做任务时，只是 "),e("strong",[t._v("sparsely activated")]),t._v("，只使用一部分的参数")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/pathways_3.gif",alt:"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/pathways_3.gif"}})]),t._v(" "),e("h2",{attrs:{id:"pathways-系统"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pathways-系统"}},[t._v("#")]),t._v(" Pathways 系统")]),t._v(" "),e("p",[t._v("2022年3月，Google发布了Pathways系统，用于更高效地训练大型模型：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132135364.png",alt:"image-20220713213527311"}}),t._v("\n这个太工程的东西我也看不懂，所以就不评论了。")]),t._v(" "),e("h2",{attrs:{id:"palm-lanugage-modeling-with-pathways"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#palm-lanugage-modeling-with-pathways"}},[t._v("#")]),t._v(" PaLM: Lanugage Modeling with Pathways")]),t._v(" "),e("p",[t._v("2022年4月，Google发布了一个鸿篇巨制——PaLM：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132136826.png",alt:"image-20220713213609786"}})]),t._v(" "),e("p",[t._v("说实话，在看完Jeff Dean介绍Pathways愿景博客之后，再看这篇文章前本来是充满期待的，比较牛皮已经吹了一年了，论文一开打一屏幕的作者，文83页（比GPT-3的paper都长），结果浏览一遍，满脑子都是”就这？？？“")]),t._v(" "),e("p",[t._v("Anyway，还是介绍一下：")]),t._v(" "),e("p",[e("strong",[t._v("一句话介绍：")])]),t._v(" "),e("blockquote",[e("p",[e("strong",[t._v("PaLM")]),t._v(" 是第一款基于 Google "),e("strong",[t._v("Pathways")]),t._v(" 系统训练的超大规模的语言模型（但依然是经典结构：a dense, decoder-only, full-attention Transformer model），再次展现了“大力出奇迹”还有很大空间。但是离Google的Pathways愿景还有很大距离。")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132136489.png",alt:"image-20220713213633466"}})]),t._v(" "),e("p",[e("strong",[t._v("Key Points：")])]),t._v(" "),e("ul",[e("li",[t._v("Efficient scaling：在Pathways系统的加持下，PaLM的训练效率比之前的方法有了显著提高；")]),t._v(" "),e("li",[t._v("Few-shot SOTA：在众多任务上取得了 few-shot 的 SOTA；")]),t._v(" "),e("li",[t._v("Breakthrough capabilities：在很多推理（reasoning）任务上，PaLM在few-shot的情况就可以超越很多之前需要fine-tune的方法；")]),t._v(" "),e("li",[t._v("Discontinuous improvements：随着模型规模的提高，边际效益可能会有质的提升（在25%的任务上，观察到了“量变产生质变”的现象）；")]),t._v(" "),e("li",[t._v("Multilingual understanding：多语言能力大幅提高。")])]),t._v(" "),e("p",[e("strong",[t._v("Model：")])]),t._v(" "),e("ul",[e("li",[t._v("A "),e("strong",[t._v("dense")]),t._v(", "),e("strong",[t._v("decoder")]),t._v("-only, full-attention Transformer model")]),t._v(" "),e("li",[t._v("使用 SwiGLU Activation，Parallel Layers，Multi-Query Attention 等提升计算效率的机制")]),t._v(" "),e("li",[t._v("完全无损、可逆的vocabulary：空格保留、OOV切分成UTF8 bytes、数值切分成单个token")]),t._v(" "),e("li",[t._v("只训练一个epoch——防止overfitting，减轻memorization现象")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132136000.png",alt:"image-20220713213654964"}})]),t._v(" "),e("p",[e("strong",[t._v("Training Dataset：")])]),t._v(" "),e("p",[t._v("filtered webpages, books, Wikipedia, news articles, source code, and social media conversations.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132137642.png",alt:"image-20220713213713606"}})]),t._v(" "),e("p",[e("strong",[t._v("Results：")])]),t._v(" "),e("p",[t._v("Few-shot 实验：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132137104.png",alt:"image-20220713213748078"}})]),t._v(" "),e("p",[t._v("Finetune实验：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132138313.png",alt:"image-20220713213818286"}})]),t._v(" "),e("p",[t._v("比最好的encoder-decoder模型效果要差一点，但是显著高于之前的decoder-only的模型。")]),t._v(" "),e("p",[t._v("Big-Bench：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132138425.png",alt:"image-20220713213838402"}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207132138202.png",alt:"image-20220713213852176"}})]),t._v(" "),e("p",[e("strong",[t._v("PaLM自己的总结：")])]),t._v(" "),e("p",[t._v("虽然文章没有给人惊喜，但是论文自己的总结也还是挺中肯的：")]),t._v(" "),e("p",[t._v("PaLM只是构建Pathways这个愿景迈出的第一步，PaLM的意义在于进一步扩展了大模型的能力边界（尤其是few-shot），说明了传统的模型架构和训练方法依然有很大的提升空间。另一方面，PaLM验证了Pathways训练系统的有效性，为下一代的模型架构研发做了经验积累。")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"other-related-work"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#other-related-work"}},[t._v("#")]),t._v(" Other Related Work")]),t._v(" "),e("p",[t._v("其实在这个PaLM之前，Google探索过很多基于"),e("strong",[t._v("MoE")]),t._v("（Mixture-of-experts）的大型sparse model，包括 GShard，Switch-Transformer，GLaM。这些模型，通过MoE的形式，实现了一个大模型中包含很多子网络，且针对不同的token自动选择不同的子网络（experts）进行推理的能力。其实看了PaLM之后，我觉得MoE这条线才更接近与Jeff Dean所说的Pathways的愿景，但我也猜测可能一个超大的类MoE模型正在Pathways系统上训练呢（我赌一根钟薛高hhh）......")]),t._v(" "),e("hr"),t._v(" "),e("p",[t._v("参考链接：")]),t._v(" "),e("ul",[e("li",[t._v("Jeff Dean关于Pathways愿景的介绍：https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/")]),t._v(" "),e("li",[t._v("PaLM Blog：https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html")]),t._v(" "),e("li",[t._v("PaLM paper：https://arxiv.org/pdf/2204.02311.pdf")]),t._v(" "),e("li",[t._v("Pathways ML system paper：https://arxiv.org/pdf/2203.12533.pdf")])])])}),[],!1,null,null,null);a.default=o.exports}}]);