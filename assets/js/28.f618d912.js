(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{396:function(v,_,s){"use strict";s.r(_);var t=s(44),i=Object(t.a)({},(function(){var v=this,_=v.$createElement,s=v._self._c||_;return s("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[s("h1",{attrs:{id:"【dl笔记1】logistic回归-最基础的神经网络"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#【dl笔记1】logistic回归-最基础的神经网络"}},[v._v("#")]),v._v(" 【DL笔记1】Logistic回归：最基础的神经网络")]),v._v(" "),s("blockquote",[s("p",[v._v("个人认为理解并掌握这个logistic regression是学习神经网络和深度学习最重要的部分，也是最基础的部分，学完这个再去看浅层神经网络、深层神经网络，会发现后者就是logistic重复了若干次（当然一些细节会有不同，但是原理上一模一样）。"),s("br"),v._v("\n本文是【专题“DeepLearning学习笔记”】的第【1】篇")])]),v._v(" "),s("h2",{attrs:{id:"一、什么是logictic-regression"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、什么是logictic-regression"}},[v._v("#")]),v._v(" 一、什么是logictic regression")]),v._v(" "),s("p",[v._v("下面的图是Andrew Ng提供的一个用logistic regression来识别主子的图片的算法结构示意图：")]),v._v(" "),s("p",[s("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624593601928-image.png",alt:""}})]),v._v(" "),s("p",[s("strong",[v._v("左边")]),v._v("的"),s("strong",[v._v("x"),s("sub",[v._v("0")]),v._v("到x"),s("sub",[v._v("12287")]),s("strong",[v._v("是输入（input），我们称之为")]),v._v("特征（feather）")]),v._v("，常常用"),s("strong",[v._v("列向量x"),s("sup",[v._v("(i)")]),s("strong",[v._v("来表示（这里的i代表第i个训练样本,下面在只讨论一个样本的时候，就暂时省略这个标记，免得看晕了-_-|||），在图片识别中，特征通常是图片的像素值，把所有的像素值排成一个序列就是输入特征，每一个特征都有自己的一个")]),v._v("权重（weight）")]),v._v("，就是图中连线上的"),s("strong",[v._v("w"),s("sub",[v._v("0")]),v._v("到w"),s("sub",[v._v("12287")])]),v._v("，通常我们也把左右的权重组合成一个"),s("strong",[v._v("列向量W")]),v._v("。")]),v._v(" "),s("p",[s("strong",[v._v("中间的圆圈")]),v._v("，我们可以叫它一个神经元，它接收来自左边的输入并乘以相应的权重,再加上一个偏置项b（一个实数），所以最终接收的总输入为：")]),v._v(" "),s("p",[s("strong",[v._v("x"),s("sub",[v._v("0")]),v._v("w"),s("sub",[v._v("0")]),v._v("+x"),s("sub",[v._v("1")]),v._v("w"),s("sub",[v._v("1")]),v._v("+...+x"),s("sub",[v._v("12287")]),v._v("w"),s("sub",[v._v("12287")]),v._v("+b=W"),s("sup",[v._v("T")]),v._v("x+b")]),v._v("\n(简书打公式真心累啊。。。)")]),v._v(" "),s("p",[v._v("但是这个并不是最后的输出，就跟神经元一样，会有一个"),s("strong",[v._v("激活函数（activation function）"),s("strong",[v._v("来对输入进行处理，来决定是否输出或者输出多少。Logistic Regression的激活函数是")]),v._v("sigmoid函数")]),v._v("，介于0和1之间，中间的斜率比较大，两边的斜率很小并在远处趋于零。长这样（记住函数表达式）：")]),v._v(" "),s("p",[s("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624593618380-image.png",alt:""}})]),v._v(" "),s("p",[v._v("我们用y'来表示该神经元的输出，σ()函数代表sigmoid，则可知：")]),v._v(" "),s("p",[s("strong",[v._v("y' = σ(W"),s("sup",[v._v("T")]),v._v("x+b)")]),v._v("\n（简书的markdown居然不支持居中。。。）")]),v._v(" "),s("p",[v._v("这个y'可以看做是我们这个小模型根据输入做出的一个预测，在最开始的图对应的案例中，就是根据图片的像素在预测图片是不是猫。\n与y'对应的，每一个样本x都有自己的一个真实标签y，y=1代表图片是猫，y=0代表不是猫。我们希望模型输出的y'可以尽可能的接近真实标签y，这样，这个模型就可以用来预测一个新图片是不是猫了。所以，我们的任务就是要找出一组W，b，使得我们的模型**y' = σ(W"),s("sup",[v._v("T")]),v._v("x+b)**可以根据给定的x，正确地预测y。在此处，我们可以认为，只要算出的y'大于0.5，那么y'就更接近1，于是可以预测为“是猫”，反之则“不是猫”。")]),v._v(" "),s("p",[v._v("以上就是Logistic Regression的基本结构说明。")]),v._v(" "),s("h2",{attrs:{id:"二、怎么学习w和b"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、怎么学习w和b"}},[v._v("#")]),v._v(" 二、怎么学习W和b")]),v._v(" "),s("p",[v._v("前面其实提到过了，我们"),s("strong",[v._v("需要学习到的W和b可以让模型的预测值y'与真实标签y尽可能地接近，也就是y'和y的差距尽量地缩小")]),v._v("。因此，我们可以定义一个"),s("strong",[v._v("损失函数（Loss function）")]),v._v("，来衡量y'和y的差距：\n"),s("strong",[v._v("L(y',y) = -[y·log(y')+(1-y)·log(1-y')]")]),v._v("\n可以暂时忽略后面这个看似复杂其实不复杂的表达式，只记住损失函数是L(y',y)就行了。\n如何说明这个式子适合当损失函数呢？且看：")]),v._v(" "),s("ul",[s("li",[v._v("当y=1时，L(y',y)=-log(y')，要使L最小，则y'要最大，则y'=1；")]),v._v(" "),s("li",[v._v("当y=0时，L(y',y)=-log(1-y')，要使L最小，则y'要最小，则y'=0.")])]),v._v(" "),s("p",[v._v("如此，便知L(y',y)符合我们对损失函数的期望，因此适合作为损失函数。")]),v._v(" "),s("p",[v._v("我们知道，x代表一组输入，相当于是一个样本的特征。但是我们训练一个模型会有很多很多的训练样本，也就是有很多很多的x，就是会有x"),s("sup",[v._v("(1)")]),v._v("，x"),s("sup",[v._v("(2)")]),v._v("，...，x"),s("sup",[v._v("(m)")]),v._v(" 共m个样本，它们可以写成一个大"),s("strong",[v._v("X 行向量")]),v._v("：\n"),s("strong",[v._v("X = (x"),s("sup",[v._v("(1)")]),v._v("，x"),s("sup",[v._v("(2)")]),v._v("，...，x"),s("sup",[v._v("(m)")]),v._v(" )")]),v._v("\n对应的样本的真实标签Y（也是行向量）：\n"),s("strong",[v._v("Y = (y"),s("sup",[v._v("(1)")]),v._v("，y"),s("sup",[v._v("(2)")]),v._v("，...，y"),s("sup",[v._v("(m)")]),v._v(" )")]),v._v("\n通过我们的模型计算出的y'们也可以组成一个行向量：\n"),s("strong",[v._v("Y' = (y'"),s("sup",[v._v("(1)")]),v._v("，y'"),s("sup",[v._v("(2)")]),v._v("，...，y'"),s("sup",[v._v("(m)")]),v._v(" )")])]),v._v(" "),s("p",[v._v("前面讲的损失函数L,对每个x都有，因此在学习模型的时候，我们需要看所有x的"),s("strong",[v._v("平均损失")]),v._v("，因此定义一个"),s("strong",[v._v("代价函数（Cost function）")]),v._v("：\n"),s("strong",[v._v("J(W,b) = 1/m·Σ"),s("sup",[v._v("m")]),s("sub",[v._v("i=1")]),v._v("L(y'"),s("sup",[v._v("(i)")]),v._v(",y"),s("sup",[v._v("(i)")]),v._v(")")]),v._v(" 代表所有训练样本的平均损失。")]),v._v(" "),s("p",[v._v("因此，我们的学习任务就可以用一句话来表述：")]),v._v(" "),s("blockquote",[s("p",[s("em",[v._v("Find W,b that minimize J(W,b)")])])]),v._v(" "),s("p",[v._v("Minimize。。。说起来简单做起来难，好在我们有计算机，可以帮我们进行大量重复地运算，于是在神经网络中，我们一般使用"),s("strong",[v._v("梯度下降法（Gradient Decent）")]),v._v("：")]),v._v(" "),s("p",[s("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624593635010-image.png",alt:""}})]),v._v(" "),s("p",[v._v("这个方法通俗一点就是，先随机在曲线上找一个点，然后求出该点的斜率，也称为梯度，然后顺着这个梯度的方向往下走一步，到达一个新的点之后，重复以上步骤，直到到达最低点（或达到我们满足的某个条件）。\n如，对w进行梯度下降，则就是重复一下步骤（重复一次称为一个"),s("strong",[v._v("迭代")]),v._v("）：\n"),s("strong",[v._v("w := w - α(dJ/dw)")]),v._v("\n其中:=代表“用后面的值更新”，α代表“"),s("strong",[v._v("学习率（learning rate）")]),v._v("”，dJ/dw就是J对w求偏导。")]),v._v(" "),s("p",[v._v("回到我们的Logistic Regression问题，就是要初始化（initializing）一组W和b，并给定一个学习率，指定要"),s("strong",[v._v("迭代的次数")]),v._v("（就是你想让点往下面走多少步），然后每次迭代中求出w和b的梯度，并更新w和b。最终的W和b就是我们学习到的W和b，把W和b放进我们的模型y' = σ(W"),s("sup",[v._v("T")]),v._v("x+b)中，就是我们学习到的模型，就可以用来进行预测了！")]),v._v(" "),s("h2",{attrs:{id:"总结一下"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#总结一下"}},[v._v("#")]),v._v(" 总结一下：")]),v._v(" "),s("ul",[s("li",[v._v("Logistic Regression模型：y' = σ(W"),s("sup",[v._v("T")]),v._v("x+b)，记住使用的激活函数是sigmoid函数。")]),v._v(" "),s("li",[v._v("损失函数：L(y',y) = -[y·log(y')+(1-y)·log(1-y')]衡量预测值y'与真实值y的差距，越小越好。")]),v._v(" "),s("li",[v._v("代价函数：损失均值，J(W,b) = 1/m·Σ"),s("sup",[v._v("m")]),s("sub",[v._v("i=1")]),v._v("L(y'"),s("sup",[v._v("(i)")]),v._v(",y"),s("sup",[v._v("(i)")]),v._v(")，是W和b的函数，学习的过程就是寻找W和b使得J(W,b)最小化的过程。求最小值的方法是用梯度下降法。")]),v._v(" "),s("li",[s("strong",[v._v("训练模型的步骤")]),v._v("：\n"),s("ol",[s("li",[v._v("初始化W和b")]),v._v(" "),s("li",[v._v("指定learning rate和迭代次数")]),v._v(" "),s("li",[v._v("每次迭代，根据当前W和b计算对应的梯度（J对W，b的偏导数），然后更新W和b")]),v._v(" "),s("li",[v._v("迭代结束，学得W和b，带入模型进行预测，分别测试在训练集合测试集上的准确率，从而评价模型")])])])]),v._v(" "),s("p",[v._v("就这么明明白白＜(▰˘◡˘▰)")])])}),[],!1,null,null,null);_.default=i.exports}}]);