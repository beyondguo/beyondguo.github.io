(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{418:function(t,s,a){"use strict";a.r(s);var n=a(44),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"用annoy和threadpool把相似度计算加速360倍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#用annoy和threadpool把相似度计算加速360倍"}},[t._v("#")]),t._v(" 用Annoy和ThreadPool把相似度计算加速360倍")]),t._v(" "),a("h2",{attrs:{id:"_0-背景故事"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_0-背景故事"}},[t._v("#")]),t._v(" 0. 背景故事")]),t._v(" "),a("p",[t._v("我最近的一个项目中需要大量查询一个词的相似词，而无论是英文的WordNet，还是中文的同义词词林，都覆盖面太窄，我决定借助训练好的"),a("strong",[t._v("Word2Vec")]),t._v("模型，使用"),a("code",[t._v("gensim")]),t._v("库，调用它经典的"),a("code",[t._v(".most_similar()")]),t._v("函数来进行相似词查询。而由于程序中需要大量查询相似词，所以就需要大量调用"),a("code",[t._v(".most_similar()")]),t._v("函数，而这，就成为了整个程序的瓶颈，因为：")]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v(".most_similar()")]),t._v("太慢了！")])]),t._v(" "),a("p",[t._v("为什么它这么慢呢？因为这个"),a("code",[t._v("gensim")]),t._v("中查询相似词，默认是直接"),a("strong",[t._v("brute-force search")]),t._v("，即我会把当前查询的词，跟词表里所有的词都计算一个相似度，然后给你排序返回。如果词典很大，词向量维度又很高，那这个计算代价是很大的！我还特地看了看"),a("code",[t._v("gensim")]),t._v("的源码（gensim/gensim/models/keyedvectors.py#L783）：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224110300717.png",alt:"image-20220224110300717"}})]),t._v(" "),a("p",[t._v("可看到，这个"),a("code",[t._v(".most_similar()")]),t._v("函数内部，就是通过对当前向量（代码中的"),a("code",[t._v("mean")]),t._v("）去跟所有的"),a("code",[t._v("vectors")]),t._v("计算dot product，然后再排序返回。")]),t._v(" "),a("p",[t._v("另外，虽然我们可以在每次跑程序的时候都维护一个词典，查询过的结果就直接保存，这对于当前程序是可以提升效率的，但是我之后"),a("strong",[t._v("再次运行程序，或者语料库改变了，那依然需要重新计算")]),t._v("，所以必须想办法解决一下。")]),t._v(" "),a("h2",{attrs:{id:"_1-想一劳永逸-那就把word2vec变成一个相似词词典"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-想一劳永逸-那就把word2vec变成一个相似词词典"}},[t._v("#")]),t._v(" 1. 想一劳永逸，那就把Word2Vec变成一个相似词词典")]),t._v(" "),a("p",[t._v("一个很直接的思路就是，既然我使用Word2Vec是为了查相似词，其他的功能不需要（比如我不需要获取向量），那么我可以把一个Word2Vec词向量模型，转化成一个相似词词典，这样通过一个现成的词典查询相似词，就比使用"),a("code",[t._v(".most_similar()")]),t._v("快得多了！")]),t._v(" "),a("p",[t._v("于是我开开心心得写下了如下代码（针对一个100维，40万词的中文词向量）：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" gensim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyedvectors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" KeyedVectors\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tqdm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tqdm\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# synonyms_words.vector为一个100维的中文词向量模型")]),t._v("\nw2v_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KeyedVectors"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_word2vec_format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weights/synonyms_words.vector"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" binary"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" unicode_errors"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ignore'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取该词向量的词汇表")]),t._v("\nvocab "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index_to_key\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把所有词遍历一遍，查询最相似的15个词，并保存到词典")]),t._v("\nsimilars_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" w "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" tqdm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    similar_words "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pair"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pair "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("most_similar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topn"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indexer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("indexer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    similars_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" similar_words\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("p",[t._v("运行，耗时"),a("strong",[t._v("2小时20分钟")]),t._v("。\n基本上就是出去吃个晚饭，散个步，就跑完了，so easy~")]),t._v(" "),a("p",[t._v("心想着，后面直接把项目程序中所有的"),a("code",[t._v(".most_similar(w)")]),t._v("，都替换成"),a("code",[t._v("similars_dict[w]")]),t._v("，速度直接起飞~舒服！")]),t._v(" "),a("p",[t._v("（本文结束）")]),t._v(" "),a("p",[t._v("...")]),t._v(" "),a("h2",{attrs:{id:"_2-问题来了"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-问题来了"}},[t._v("#")]),t._v(" 2. 问题来了...")]),t._v(" "),a("p",[t._v("我本来确实以为就这么结束了，直到我对一个英文Word2Vec模型重复了上面的操作：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" gensim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyedvectors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" KeyedVectors\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tqdm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tqdm\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# GoogleNews-vectors-negative300.bin为一个300维的英文词向量模型")]),t._v("\nw2v_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KeyedVectors"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_word2vec_format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weights/GoogleNews-vectors-negative300.bin"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" binary"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" unicode_errors"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ignore'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取该词向量的词汇表")]),t._v("\nvocab "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index_to_key\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把所有词遍历一遍，查询最相似的15个词，并保存到词典")]),t._v("\nsimilars_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" w "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" tqdm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    similar_words "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pair"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pair "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("most_similar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topn"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indexer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("indexer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    similars_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" similar_words\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("p",[t._v("给大家看看进度条：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224111445169.png",alt:"进度条1"}})]),t._v(" "),a("p",[t._v("预计用时"),a("strong",[t._v("150小时")]),t._v("！天哪，为什么差别这么大？")]),t._v(" "),a("p",[t._v("原来我前面那么轻松，是因为使用了一个较小的词向量模型：")]),t._v(" "),a("ul",[a("li",[t._v("100维，40万词——> 2小时 （一次健身的时间）")]),t._v(" "),a("li",[t._v("300维，300万词——> 150小时（你可以去度一个假了，回来应该可以跑完吧）")])]),t._v(" "),a("p",[t._v("我还试着用了一个线程池，发现依然需要80~100小时...")]),t._v(" "),a("p",[t._v("怎么办？")]),t._v(" "),a("p",[t._v("一看时间，已经8:00 PM了，开启多线程让这玩意儿跑着吧，明早过来看看能跑多少吧，溜了溜了~")]),t._v(" "),a("h2",{attrs:{id:"_3-approximate-nearest-neighbors-oh-yeah-annoy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-approximate-nearest-neighbors-oh-yeah-annoy"}},[t._v("#")]),t._v(" 3. Approximate Nearest Neighbors Oh Yeah ! (Annoy)")]),t._v(" "),a("p",[t._v("第二天中午来到实验室，打开电脑一看，跑了50万了，还有250万没跑完... 摸一摸主机，已经滚烫了，我的8核CPU哼哧哼哧了一晚上才跑了1/5的词，现在一定怨声载道了...")]),t._v(" "),a("p",[t._v("我果断kill掉了程序，看着任务管理器缓缓下降的CPU利用率曲线，我和CPU们都进入了贤者时间。")]),t._v(" "),a("p",[t._v("之前也了解过ANN算法，即近似最近邻算法，于是我开始在Google上搜索有关ANN和gensim的内容，终于，找到了这篇文章的主角——Annoy，而且我发现，gensim其实已经对Annoy做了封装，支持使用Annoy来进行加速。")]),t._v(" "),a("p",[t._v("Annoy算法，是一种"),a("strong",[t._v("基于二叉树的近似最近邻算法")]),t._v("，他的全称是："),a("strong",[t._v("A")]),t._v("pproximate Nearest "),a("strong",[t._v("N")]),t._v("eighbors "),a("strong",[t._v("O")]),t._v("h "),a("strong",[t._v("Y")]),t._v("eah，别的不说，这个"),a("strong",[t._v("Oh Yeah")]),t._v("直接让我对这个算法好感倍增。下面看看Annoy自己的介绍：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224121732634.png",alt:"image-20220224121732634"}})]),t._v(" "),a("p",[t._v("看最后一句话：")]),t._v(" "),a("blockquote",[a("p",[t._v("Annoy算法是Erik这个老哥，在Spotify的Hack Week期间，花了几个下午的时候开发的。")])]),t._v(" "),a("p",[t._v('你都可以想想那个场景，Erik在阳光的午后，边喝咖啡，边写代码，构思着一个巧妙的ANN算法，几天后他做到了，成功地发明了一种新的ANN算法，他高呼"Oh Yeah!"，遂取名ANNOY~（纯属个人遐想，请勿当真）')]),t._v(" "),a("h3",{attrs:{id:"annoy算法原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#annoy算法原理"}},[t._v("#")]),t._v(" Annoy算法原理")]),t._v(" "),a("p",[t._v("一个有追求的programmer，除了知道有这么个算法外，一定还想了解一下它背后的原理，所以我花了一天阅读Annoy作者的博客，找到YouTube上一些介绍的视频，配合一些代码一起理解，算是搞懂了Annoy的原理。下面我来简单讲解一下：")]),t._v(" "),a("p",[t._v("（下面的一些图，引自Erik的博客）")]),t._v(" "),a("p",[t._v("首先我们有一大堆点，每个点都是一个向量：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224150139702.png",alt:"image-20220224150139702"}})]),t._v(" "),a("p",[t._v("然后，对于一个新的点，我们希望找到它的最近邻。")]),t._v(" "),a("p",[t._v("然而，如果对全局都扫一遍，那复杂度就是"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",{attrs:{mathvariant:"script"}},[t._v("O")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("n")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\mathcal{O}(n)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathcal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("n")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，这样如果我们的搜索量很大的话就太费劲了。")]),t._v(" "),a("blockquote",[a("p",[t._v("Annoy的核心思想就是：\n"),a("strong",[t._v("把空间分割成一个个的子空间，且在子空间中的点都是彼此间比较接近的。那么对于一个新的点，我们只需要搜索它所在的子空间中的那些点，就可以找到它的近似的最近邻们。")])])]),t._v(" "),a("p",[t._v("所以，Annoy最终实现的效果是这样的：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224150631474.png",alt:"image-20220224150631474"}})]),t._v(" "),a("p",[t._v("图中红色叉叉就是新的点，整个空间已经被分成了很多个小区域，我们只需要在图中蓝色的那一小块搜索即可，这样，复杂度就大大大大降低了。")]),t._v(" "),a("blockquote",[a("p",[t._v("关键在于——如何划分空间？")]),t._v(" "),a("p",[t._v("答案是使用随机投影（random projection）来构建二叉树（binary tree）。")])]),t._v(" "),a("p",[t._v("回到最开始的散点图，我们先"),a("strong",[t._v("随机")]),t._v("挑两个点，这两个点的正中间就确定了一个分割超平面：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224152019731.png",alt:"image-20220224152019731"}})]),t._v(" "),a("p",[t._v("这样，就能将空间一分为二，所有的点，就都分配到一个子空间了。")]),t._v(" "),a("p",[t._v("这里可能有人会问，在确定超平面之后，如何把所有点进行区间划分呢？是不是还是得把所有点都计算一遍距离，再确定呢？答案是“"),a("strong",[t._v("是的，我们需要做一个linear scan来确定归属")]),t._v("”。为了确认，我查看作者Erik给出的一个示例代码（并非Annoy代码，Annoy使用C++写的，我还看不太明白，但作者为了展示Annoy算法的代码，也用python写了一个简单例子）：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224153529927.png",alt:"image-20220224153529927"}})]),t._v(" "),a("p",[t._v("上述代码我画了一个图来表示，应该就很清楚了，所以不再赘述：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224155453628.png",alt:"image-20220224155453628"}})]),t._v(" "),a("p",[t._v("好，接下来我们可以在每个子空间中，都使用类似的方法，继续划分，不断迭代这个过程（可设定一个超参数K，最多迭代K次）：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224153630328.png",alt:"image-20220224153630328"}})]),t._v(" "),a("p",[t._v("如果我们把每个超平面当做一个树的分支，最终每个小区域中的点当做树的叶子节点，那么就可以得到一下的一棵树：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224153834856.png",alt:"image-20220224153834856"}})]),t._v(" "),a("p",[t._v("有了这棵树之后，我们想搜索一个点的最近邻，就只用访问这棵树的一个分支即可，即使用上面说的那个确定一个点归属的算法，从root节点一直找到最下面的小分支，然后跟那个分支上的leaf节点逐一计算相似度，就完事儿了：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224160214015.png",alt:"image-20220224160214015"}})]),t._v(" "),a("p",[t._v("这样，我们就将相似节点查询的复杂度都"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",{attrs:{mathvariant:"script"}},[t._v("O")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("n")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\mathcal{O}(n)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathcal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("n")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("降低到了"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",{attrs:{mathvariant:"script"}},[t._v("O")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("l")]),a("mi",[t._v("o")]),a("mi",[t._v("g")]),a("mi",[t._v("n")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\mathcal{O}(logn)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathcal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),a("span",{staticClass:"mord mathnormal"},[t._v("o")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),a("span",{staticClass:"mord mathnormal"},[t._v("n")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v(".")]),t._v(" "),a("h3",{attrs:{id:"annoy的问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#annoy的问题"}},[t._v("#")]),t._v(" Annoy的问题")]),t._v(" "),a("p",[t._v("很明显，我们可以知道上述的构件树并查询相似点的方法是不精确的，因为我们发现每个超平面，都是随机挑选两个点来确定的，这就导致很有可能有些相近的点，会被分开，而一旦分开，在树的搜索中很可能就被丢弃了。")]),t._v(" "),a("p",[t._v("一个解决方法就是构建多棵树！形成一个森林！然后把所有树的结果进行平均，或者把所有树找到的最小区域进行合并：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224160810958.png",alt:"image-20220224160810958"}})]),t._v(" "),a("p",[t._v("这样，就可以大大提升准确率。当然，还有一些其他技巧，比如使用priority queue等等，这里也不赘述了。")]),t._v(" "),a("p",[t._v("研究了一天，终于把Annoy这优美的算法搞明白了，很是兴奋。晚上，老婆一直睡不着觉，想聊天，于是我绘声绘色地跟她讲解Annoy算法的原理，算法名称的来历，怎么诞生的...... 当我激动地完成了演讲，转头一看，老婆已经呼呼大睡~~")]),t._v(" "),a("p",[t._v("第二天早上，我要求她复述这个算法的基本原理，她说：“Oh Yeah？”")]),t._v(" "),a("h3",{attrs:{id:"在gensim中使用annoy-加速75倍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#在gensim中使用annoy-加速75倍"}},[t._v("#")]),t._v(" 在Gensim中使用Annoy，加速75倍")]),t._v(" "),a("p",[t._v("第三天，在搞懂了原理之后，终于开始动手了。幸运的是，Gensim早就为我们封装好了Annoy工具，所以我们可以直接使用：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" gensim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("similarities"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("annoy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AnnoyIndexer\nindexer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AnnoyIndexer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsimilars_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" w "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" tqdm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    similar_words "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pair"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pair "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("most_similar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topn"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indexer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("indexer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    similars_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" similar_words\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br")])]),a("p",[t._v("您猜怎么着？")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224162122864.png",alt:"image-20220224162122864"}})]),t._v(" "),a("p",[t._v("300维向量，300万的词汇量，300万次查询，只要 "),a("strong",[t._v("2小时")]),t._v("13分钟！记得在不使用Annoy indexer的情况下，上面代码需要跑150小时！（我们不用去度假啦，健个身代码就跑完啦~）所以这个Annoy足足把速度加速了75倍！")]),t._v(" "),a("p",[t._v("一些细节需要说明：")]),t._v(" "),a("ul",[a("li",[t._v("首先需要构造Indexer，这时我们要指定构建多少棵树。上面例子中我构建了200棵，建树时间大概20分钟。树越多，结果越精确，但建树和查询的速度会变慢；")]),t._v(" "),a("li",[t._v("我也测试了100或者500棵树，前者的相似度精度不够，后者则太慢（大概30~50小时？）")])]),t._v(" "),a("h2",{attrs:{id:"_4-使用多线程-把cpu榨的一滴不剩"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-使用多线程-把cpu榨的一滴不剩"}},[t._v("#")]),t._v(" 4. 使用多线程，把CPU榨的一滴不剩")]),t._v(" "),a("p",[t._v("通过上面的方法，我们已经把耗时从150小时缩短到2小时了。")]),t._v(" "),a("p",[t._v("然而，我的CPU们跃跃欲试，说“我们还可以为你做更多”。")]),t._v(" "),a("p",[t._v("注意到，上面的代码中，我是通过for循环来遍历这个长度为300万的vocab词典，而这正好可以通过多线程来进行并发，因此我写下了如下代码：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" multiprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dummy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pool "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ThreadPool\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" logger "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" logger\npool "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ThreadPool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsimilars_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("process")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    similar_words "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pair"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pair "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" w2v_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("most_similar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topn"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indexer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("indexer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    similars_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" similar_words\n    c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("similars_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        logger"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'already processed '")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' items.'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nlogger"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'start'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pool.map函数，可以把一个list中的所有item，分配到不同线程并行执行   ")]),t._v("\npool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("process"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br")])]),a("p",[t._v("这里主要使用到了"),a("code",[t._v("pool.map(process_for_item, your_list)")]),t._v("函数，这个函数可以使用你自定义的"),a("code",[t._v("process_for_item")]),t._v("函数，在多个线程中并行地对"),a("code",[t._v("your_list")]),t._v("中所有item进行处理，非常方便。")]),t._v(" "),a("p",[t._v("查看输出：")]),t._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),t._v("/Feb/2022 "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(":11:46"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" INFO - start\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),t._v("/Feb/2022 "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(":11:51"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" INFO - already processed "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),t._v(" items.\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("..")]),t._v(".\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),t._v("/Feb/2022 "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(":35:29"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" INFO - already processed "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2990000")]),t._v(" items.\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),t._v("/Feb/2022 "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(":35:46"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" INFO - already processed "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3000000")]),t._v(" items.\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("p",[t._v("总共耗时"),a("strong",[t._v("25分钟")]),t._v("！！这是值得铭记的历史的一刻！")]),t._v(" "),a("p",[t._v("至此，我们经历了将"),a("strong",[t._v("150小时")]),t._v("，缩短到100小时（多线程），再缩短到"),a("strong",[t._v("2小时")]),t._v("（Annoy近似搜索），最终缩短到25分钟（Annoy+多线程），将任务在我的单机上提速了"),a("strong",[t._v("360")]),t._v("倍。")]),t._v(" "),a("h2",{attrs:{id:"其他尝试"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其他尝试"}},[t._v("#")]),t._v(" 其他尝试")]),t._v(" "),a("p",[t._v("其实我还尝试过Faiss框架，使用"),a("code",[t._v("IndexFlatL2")]),t._v("作为quantizer，使用"),a("code",[t._v("IndexIVFFlat")]),t._v("作为indexer，使用nlist = 1000，nprobe = 10，结果对300万个query查询完毕，需要8小时。而且目测的效果，并没有比我前面使用Annoy的结果好，再加上这玩意儿调参困难，所以后面就没有继续尝试Faiss。")]),t._v(" "),a("p",[t._v("根据ANN-benchmark：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224165732371.png",alt:"image-20220224165732371"}})]),t._v(" "),a("p",[t._v("Annoy算法算是一个中规中矩的，还算可以的算法。而Gensim还提供了NMSLIB算法支持，所以有兴趣的同学，可以把Annoy换成NMSLIB看看效果。")]),t._v(" "),a("h2",{attrs:{id:"后记"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#后记"}},[t._v("#")]),t._v(" 后记")]),t._v(" "),a("p",[t._v("—— “如果当初不做改进，让它占着电脑慢慢跑，你现在应该度假还没结束吧~”\n—— “也许吧，可能我的假期还有100小时呢，哈哈”\n—— “你高兴啥，度假不是更快乐吗？”\n—— “那不是真正的快乐！” 我扶着发际线，骄傲的说\n—— “... 那你的快乐是什么？”\n—— “是我只用25分钟，把"),a("code",[t._v(".most_similar()")]),t._v("给加速了"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("5")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("10^5")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8141079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8141079999999999em"}},[a("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("5")])])])])])])])])])])]),t._v("倍~”")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/image-20220224170852415.png",alt:"真正的快乐"}})]),t._v(" "),a("p",[t._v("最后，拜谢以下资料，陪我走过这几天：")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("Annoy作者博客：https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html")])]),t._v(" "),a("li",[a("p",[t._v("Annoy官方GitHub：https://github.com/spotify/annoy")])]),t._v(" "),a("li",[a("p",[t._v("gensim上的Annoy支持：https://radimrehurek.com/gensim/similarities/annoy.html")])]),t._v(" "),a("li",[a("p",[t._v("Ball-tree & KD-tree：https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940 另外Wikipedia上的KD-tree也讲的非常好：https://en.wikipedia.org/wiki/K-d_tree")])]),t._v(" "),a("li",[a("p",[t._v("Faiss Wiki：https://github.com/facebookresearch/faiss/wiki/Getting-started")])]),t._v(" "),a("li",[a("p",[t._v("ANN算法benchmark：http://ann-benchmarks.com/index.html#algorithms")])]),t._v(" "),a("li",[a("p",[t._v("python多线程并行：https://chriskiehl.com/article/parallelism-in-one-line")])]),t._v(" "),a("li",[a("p",[t._v("Random Projection：https://medium.com/data-science-in-your-pocket/random-projection-for-dimension-reduction-27d2ec7d40cd")])]),t._v(" "),a("li",[a("p",[t._v("CVPR20上一个关于ANN的分享：https://speakerdeck.com/matsui_528/cvpr20-tutorial-billion-scale-approximate-nearest-neighbor-search?slide=115")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);