(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{430:function(s,t,a){"use strict";a.r(t);var n=a(44),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("blockquote",[a("p",[a("strong",[s._v("「Huggingface🤗NLP笔记系列-第4集」")]),s._v("\n最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的精简+注解版。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。")])]),s._v(" "),a("ul",[a("li",[s._v("官方教程网址：https://huggingface.co/course/chapter1")]),s._v(" "),a("li",[s._v("本期内容对应网址：https://huggingface.co/course/chapter2/3?fw=pt")]),s._v(" "),a("li",[s._v("本系列笔记的"),a("strong",[s._v("GitHub")]),s._v("： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP")])]),s._v(" "),a("hr"),s._v(" "),a("h1",{attrs:{id:"models-tokenizers-以及如何做subword-tokenization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#models-tokenizers-以及如何做subword-tokenization"}},[s._v("#")]),s._v(" Models，Tokenizers，以及如何做Subword tokenization")]),s._v(" "),a("h2",{attrs:{id:"models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#models"}},[s._v("#")]),s._v(" Models")]),s._v(" "),a("p",[s._v("前面都是使用的"),a("code",[s._v("AutoModel")]),s._v("，这是一个智能的wrapper，可以根据你给定的checkpoint名字，自动去寻找对应的网络结构，故名Auto。")]),s._v(" "),a("p",[s._v("如果明确知道我们需要的是什么网络架构，就可以直接使用具体的"),a("code",[s._v("*Model")]),s._v("，比如"),a("code",[s._v("BertModel")]),s._v("，就是使用Bert结构。")]),s._v(" "),a("h3",{attrs:{id:"随机初始化一个transformer模型-通过config来加载"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机初始化一个transformer模型-通过config来加载"}},[s._v("#")]),s._v(" 随机初始化一个Transformer模型：通过"),a("code",[s._v("config")]),s._v("来加载")]),s._v(" "),a("p",[a("code",[s._v("*Config")]),s._v("这个类，用于给出某个模型的网络结构，通过config来加载模型，得到的就是一个模型的架子，没有预训练的权重。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BertModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" BertConfig\n\nconfig "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 模型是根据config来构建的，这时构建的模型是参数随机初始化的")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("看看config打印出来是啥：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("BertConfig "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"attention_probs_dropout_prob"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"gradient_checkpointing"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" false,\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hidden_act"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"gelu"')]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hidden_dropout_prob"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hidden_size"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("768")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"initializer_range"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.02")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"intermediate_size"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3072")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"layer_norm_eps"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" 1e-12,\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"max_position_embeddings"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("512")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"model_type"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert"')]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"num_attention_heads"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("12")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"num_hidden_layers"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("12")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pad_token_id"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"position_embedding_type"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"absolute"')]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"transformers_version"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"4.3.3"')]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"type_vocab_size"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(",\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"use_cache"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" true,\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"vocab_size"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30522")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br")])]),a("p",[s._v("更常用的做法则是直接加载预训练模型，然后微调。")]),s._v(" "),a("h3",{attrs:{id:"初始化一个预训练的transformer模型-通过from-pretrained来加载"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#初始化一个预训练的transformer模型-通过from-pretrained来加载"}},[s._v("#")]),s._v(" 初始化一个预训练的Transformer模型：通过"),a("code",[s._v("from_pretrained")]),s._v("来加载")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BertModel\n\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'bert-base-cased'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("模型的保存：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"directory_on_my_computer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 会生成两个文件： config.json pytorch_model.bin")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h2",{attrs:{id:"tokenizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[s._v("#")]),s._v(" Tokenizer")]),s._v(" "),a("p",[s._v("transformer模型使用的分词方法，往往不是直接的word-level分词或者char-level分词。")]),s._v(" "),a("p",[s._v("前者会让词表过大，后者则表示能力很低。")]),s._v(" "),a("p",[s._v("因此主流的方式是进行 "),a("strong",[s._v("subword-level")]),s._v(' 的分词。例如对 "tokenization" 这个词，可能会被分成 "token" 和 "ization" 两部分。')]),s._v(" "),a("p",[s._v("常见的subword tokenization方法有：")]),s._v(" "),a("ul",[a("li",[s._v("BPE")]),s._v(" "),a("li",[s._v("WordPiece")]),s._v(" "),a("li",[s._v("Unigram")]),s._v(" "),a("li",[s._v("SentencePiece")]),s._v(" "),a("li",[s._v("...")])]),s._v(" "),a("p",[s._v("这里对BPE做一个简单的介绍，让我们对 sub-word tokenization 的原理有一个基本了解：")]),s._v(" "),a("h2",{attrs:{id:"subword-tokenization-☆☆☆"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#subword-tokenization-☆☆☆"}},[s._v("#")]),s._v(" Subword tokenization (☆☆☆)")]),s._v(" "),a("p",[s._v("Subword tokenization的核心思想是：“"),a("strong",[s._v("频繁出现了词不应该被切分成更小的单位，但不常出现的词应该被切分成更小的单位")]),s._v("”。")]),s._v(" "),a("p",[s._v('比方"annoyingly"这种词，就不是很常见，但是"annoying"和"ly"都很常见，因此细分成这两个sub-word就更合理。中文也是类似的，比如“仓库管理系统”作为一个单位就明显在语料中不会很多，因此分成“仓库”和“管理系统”就会好很多。')]),s._v(" "),a("p",[s._v("这样分词的好处在于，大大节省了词表空间，还能够解决OOV问题。因为我们很多使用的词语，都是由更简单的词语或者词缀构成的，我们不用去保存那些“小词”各种排列组合形成的千变万化的“大词”，而用较少的词汇，去覆盖各种各样的词语表示。同时，相比与直接使用最基础的“字”作为词表，sub-word的语义表示能力也更强。")]),s._v(" "),a("p",[s._v("那么，用什么样的标准得到sub-word呢？一个著名的算法就是 "),a("strong",[s._v("Byte-Pair Encoding (BPE)")]),s._v(" ：")]),s._v(" "),a("p",[s._v("（下面的内容，主要翻译自Huggingface Docs中讲解tokenizer的部分，十分推荐大家直接阅读： https://huggingface.co/transformers/master/tokenizer_summary.html ）")]),s._v(" "),a("h3",{attrs:{id:"bpe-byte-pair-encoding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bpe-byte-pair-encoding"}},[s._v("#")]),s._v(" BPE————Byte-Pair Encoding：")]),s._v(" "),a("h4",{attrs:{id:"step1-首先-我们需要对语料进行一个预分词-pre-tokenization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#step1-首先-我们需要对语料进行一个预分词-pre-tokenization"}},[s._v("#")]),s._v(" "),a("strong",[s._v("Step1")]),s._v("：首先，我们需要对"),a("strong",[s._v("语料")]),s._v("进行一个"),a("strong",[s._v("预分词（pre-tokenization）")]),s._v("：")]),s._v(" "),a("p",[s._v("比方对于英文，我可以直接简单地使用空格加一些标点符号来分词；中文可以使用jieba或者直接字来进行分词。")]),s._v(" "),a("p",[s._v("分词之后，我们就得到了一个"),a("strong",[s._v("原始词集合")]),s._v("，同时，还会记录每个词在训练语料中出现的"),a("strong",[s._v("频率")]),s._v("。")]),s._v(" "),a("p",[s._v("假设我们的词集合以及词频是：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hug"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pug"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"pun"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bun"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hugs"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h4",{attrs:{id:"step2-构建基础词表-base-vocab-并开始学习-结合规则-merge-rules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#step2-构建基础词表-base-vocab-并开始学习-结合规则-merge-rules"}},[s._v("#")]),s._v(" "),a("strong",[s._v("Step2")]),s._v("：构建"),a("strong",[s._v("基础词表（base vocab）")]),s._v(" 并开始学习 "),a("strong",[s._v("结合规则（merge rules）")]),s._v("：")]),s._v(" "),a("p",[s._v("对于英语来说，我们选择字母来构成"),a("strong",[s._v("基础词表")]),s._v("：")]),s._v(" "),a("p",[a("code",[s._v('["b", "g", "h", "n", "p", "s", "u"]')])]),s._v(" "),a("p",[s._v("注：这个基础词表，就是我们最终词表的初始状态，我们会不断构建新词，加进去，直到达到我们理想的词表规模。")]),s._v(" "),a("p",[s._v("根据这个基础词表，我们可以对原始的词集合进行细粒度分词，并看到基础词的词频：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"h"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"u"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"g"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"p"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"u"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"g"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"p"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"u"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"b"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"u"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"h"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"u"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"g"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"s"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("接下来就是BPE的Byte-Pair核心部分————找symbol pair（符号对）并学习结合规则，即，我们从上面这个统计结果中，找出出现次数最多的那个符号对：")]),s._v(" "),a("p",[s._v("统计一下：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("h"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v("u   出现了 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(" 次\nu"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v("g   出现了 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),s._v(" 次\np"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v("u   出现了 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("12")]),s._v(" 次\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("统计完毕，我们发现"),a("code",[s._v("u+g")]),s._v("出现了最多次，因此，第一个结合规则就是："),a("strong",[s._v("把"),a("code",[s._v("u")]),s._v("跟"),a("code",[s._v("g")]),s._v("拼起来，得到"),a("code",[s._v("ug")]),s._v("这个新词！")])]),s._v(" "),a("p",[s._v("那么，我们就把"),a("code",[s._v("ug")]),s._v("加入到我们的基础词表：")]),s._v(" "),a("p",[a("code",[s._v('["b", "g", "h", "n", "p", "s", "u", "ug"]')])]),s._v(" "),a("p",[s._v("同时，词频统计表也变成了：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h4",{attrs:{id:"step3-反复地执行上一步-直到达到预设的词表规模。"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#step3-反复地执行上一步-直到达到预设的词表规模。"}},[s._v("#")]),s._v(" "),a("strong",[s._v("Step3")]),s._v("：反复地执行上一步，直到达到预设的词表规模。")]),s._v(" "),a("p",[s._v("我们接着统计，发现下一个频率最高的symbol pair是"),a("code",[s._v("u+n")]),s._v("，出现了12+4=16次，因此词表中增加"),a("code",[s._v("un")]),s._v("这个词；再下一个则是"),a("code",[s._v("h+ug")]),s._v("，出现了10+5=15次，因此添加"),a("code",[s._v("hug")]),s._v("这个词......")]),s._v(" "),a("p",[s._v("如此进行下去，当达到了预设的"),a("code",[s._v("vocab_size")]),s._v("的数目时，就停止，咱们的词表就得到啦！")]),s._v(" "),a("h4",{attrs:{id:"step4-如何分词"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#step4-如何分词"}},[s._v("#")]),s._v(" "),a("strong",[s._v("Step4")]),s._v("：如何分词：")]),s._v(" "),a("p",[s._v("得到了最终词表，在碰到一个词汇表中没有的词的时候，比如"),a("code",[s._v("bug")]),s._v("就会把它分成"),a("code",[s._v("b")]),s._v("和"),a("code",[s._v("ug")]),s._v("。也可以理解成，我首先把"),a("code",[s._v("bug")]),s._v("分解成最基本的字母，然后根据前面的结合规律，把"),a("code",[s._v("u")]),s._v("跟"),a("code",[s._v("g")]),s._v("结合起来，而"),a("code",[s._v("b")]),s._v("单独一个。具体在分词时候是如何做的，有时间去读读源码。")]),s._v(" "),a("hr"),s._v(" "),a("p",[s._v("除了BPE，还有一些其他的sub-word分词法，可以参考 https://huggingface.co/transformers/master/tokenizer_summary.html 。")]),s._v(" "),a("p",[s._v("下面，我们就直接使用Tokenizer来进行分词：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BertTokenizer  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 或者 AutoTokenizer")]),s._v("\n\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BertTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-cased"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("s "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'today is a good day to learn transformers'")]),s._v("\ntokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("得到：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2052")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1110")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("170")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1363")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1285")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1106")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3858")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("11303")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1468")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'token_type_ids'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h3",{attrs:{id:"了解一下内部的具体步骤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#了解一下内部的具体步骤"}},[s._v("#")]),s._v(" 了解一下内部的具体步骤：")]),s._v(" "),a("ol",[a("li",[a("code",[s._v("tokenize()")])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("s "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'today is a good day to learn transformers'")]),s._v("\ntokens "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tokenize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntokens\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'today'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'is'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'a'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'good'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'day'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'to'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'learn'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'transform'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'##ers'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("注意这里的分词结果，"),a("code",[s._v("transformers")]),s._v("被分成了"),a("code",[s._v("transform")]),s._v("和"),a("code",[s._v("##ers")]),s._v("。这里的##代表这个词应该紧跟在前面的那个词，组成一个完整的词。")]),s._v(" "),a("p",[s._v("这样设计，主要是为了方面我们在还原句子的时候，可以正确得把sub-word组成成原来的词。")]),s._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[a("code",[s._v("convert_tokens_to_ids()")])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("ids "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("convert_tokens_to_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokens"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nids\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[s._v("[2052, 1110, 170, 1363, 1285, 1106, 3858, 11303, 1468]\n")])])]),a("ol",{attrs:{start:"3"}},[a("li",[a("code",[s._v("decode")])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("decode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1468")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("decode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 注意这里会把subword自动拼起来")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("##ers")]),s._v("\ntoday is a good day to learn transformers\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h3",{attrs:{id:"special-tokens"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#special-tokens"}},[s._v("#")]),s._v(" Special Tokens")]),s._v(" "),a("p",[s._v("观察一下上面的结果，直接call tokenizer得到的ids是：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2052")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1110")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("170")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1363")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1285")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1106")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3858")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("11303")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1468")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("而通过"),a("code",[s._v("convert_tokens_to_ids")]),s._v("得到的ids是：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2052")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1110")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("170")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1363")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1285")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1106")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3858")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("11303")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1468")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("可以发现，前者在头和尾多了俩token，id分别是 101 和 102。")]),s._v(" "),a("p",[s._v("decode出来瞅瞅：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("decode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("101")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2052")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1110")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("170")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1363")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1285")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1106")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3858")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("11303")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1468")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'[CLS] today is a good day to learn transformers [SEP]'")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("它们分别是 "),a("code",[s._v("[CLS]")]),s._v(" 和 "),a("code",[s._v("[SEP]")]),s._v("。这两个token的出现，是因为我们调用的模型，在pre-train阶段使用了它们，所以tokenizer也会使用。")]),s._v(" "),a("p",[s._v("不同的模型使用的special tokens不一定相同，所以一定要让tokenizer跟model保持一致！")])])}),[],!1,null,null,null);t.default=e.exports}}]);