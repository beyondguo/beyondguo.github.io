(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{427:function(r,e,t){"use strict";t.r(e);var _=t(44),v=Object(_.a)({},(function(){var r=this,e=r.$createElement,t=r._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[t("blockquote",[t("p",[t("strong",[r._v("「Huggingface🤗NLP笔记系列-第2集」")]),r._v("\n最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的精简版。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。")])]),r._v(" "),t("ul",[t("li",[r._v("官方教程网址：https://huggingface.co/course/chapter1")]),r._v(" "),t("li",[r._v("本期内容对应网址：https://huggingface.co/course/chapter1/4?fw=pt")]),r._v(" "),t("li",[r._v("本系列笔记的"),t("strong",[r._v("GitHub")]),r._v("： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP")])]),r._v(" "),t("hr"),r._v(" "),t("h1",{attrs:{id:"一文看清transformer大家族的三股势力"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#一文看清transformer大家族的三股势力"}},[r._v("#")]),r._v(" 一文看清Transformer大家族的三股势力")]),r._v(" "),t("h2",{attrs:{id:"_1-transformer结构"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-transformer结构"}},[r._v("#")]),r._v(" 1. Transformer结构")]),r._v(" "),t("p",[r._v("Transformer结构最初就是在大2017年名鼎鼎的《Attention Is All You Need》论文中提出的，最开始是用于机器翻译任务。")]),r._v(" "),t("p",[r._v("这里先简单回顾一下Transformer的基本结构：")]),r._v(" "),t("img",{attrs:{src:"https://huggingface.co/course/static/chapter1/transformers_blocks.png",width:"200",align:"center"}}),r._v(" "),t("ul",[t("li",[r._v("左边是encoder，用于对输入的sequence进行表示，得到一个很好特征向量。")]),r._v(" "),t("li",[r._v("右边是decoder，利用encoder得到的特征，以及原始的输入，进行新的sequence的生成。")])]),r._v(" "),t("p",[r._v("encoder、decoder既可以单独使用，又可以再一起使用，因此，基于Transformer的模型可以分为三大类：")]),r._v(" "),t("ul",[t("li",[r._v("Encoder-only")]),r._v(" "),t("li",[r._v("Decoder-only")]),r._v(" "),t("li",[r._v("Encoder-Decoder")])]),r._v(" "),t("h2",{attrs:{id:"_2-transformer家族及三股势力"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-transformer家族及三股势力"}},[r._v("#")]),r._v(" 2. Transformer家族及三股势力")]),r._v(" "),t("p",[r._v("随后各种基于Transformer结构的模型就如雨后春笋般涌现出来，教程中有一张图展示了一些主要模型的时间轴：")]),r._v(" "),t("img",{attrs:{src:"https://huggingface.co/course/static/chapter1/transformers_chrono.png",width:"1000"}}),r._v(" "),t("p",[r._v("虽然模型多到四只jio都数不过来，但总体上可以分为三个阵营，分别有三个组长：")]),r._v(" "),t("ul",[t("li",[r._v("组长1："),t("strong",[r._v("BERT")]),r._v("。组员都是BERT类似的结构，是一类"),t("strong",[r._v("自编码模型")]),r._v("。")]),r._v(" "),t("li",[r._v("组长2："),t("strong",[r._v("GPT")]),r._v("。组员都是类似GPT的结构，是一类"),t("strong",[r._v("自回归模型")]),r._v("。")]),r._v(" "),t("li",[r._v("组长3："),t("strong",[r._v("BART/T5")]),r._v("。组员结构都差不多是"),t("strong",[r._v("encoder-decoder")]),r._v("模型。")])]),r._v(" "),t("h3",{attrs:{id:"不同的架构-不同的预训练方式-不同的特长"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#不同的架构-不同的预训练方式-不同的特长"}},[r._v("#")]),r._v(" 不同的架构，不同的预训练方式，不同的特长")]),r._v(" "),t("p",[r._v("对于"),t("strong",[r._v("Encoder-only")]),r._v("的模型，预训练任务通常是“破坏一个句子，然后让模型去预测或填补”。例如BERT中使用的就是两个预训练任务就是"),t("strong",[r._v("Masked language modeling")]),r._v("和"),t("strong",[r._v("Next sentence prediction")]),r._v("。\n因此，这类模型擅长进行文本表示，适用于做"),t("strong",[r._v("文本的分类、实体识别、关键信息抽取")]),r._v("等任务。")]),r._v(" "),t("p",[r._v("对于"),t("strong",[r._v("Decoder-only")]),r._v("的模型，预训练任务通常是"),t("strong",[r._v("Next word prediction")]),r._v("，这种方式又被称为"),t("strong",[r._v("Causal language modeling")]),r._v("。这个Causal就是“因果”的意思，对于decoder，它在训练时是无法看到全文的，只能看到前面的信息。\n因此这类模型适合做"),t("strong",[r._v("文本生成")]),r._v("任务。")]),r._v(" "),t("p",[r._v("而"),t("strong",[r._v("Seq2seq")]),r._v("架构，由于包含了encoder和decoder，所以预训练的目标通常是融合了各自的目标，但通常还会设计一些更加复杂的目标，比如对于T5模型，会把一句话中一片区域的词都mask掉，然后让模型去预测。seq2seq架构的模型，就适合做"),t("strong",[r._v("翻译、对话")]),r._v("等需要根据给定输入来生成输出的任务，这跟decoder-only的模型还是有很大差别的。")]),r._v(" "),t("h3",{attrs:{id:"总结表如下"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结表如下"}},[r._v("#")]),r._v(" 总结表如下：")]),r._v(" "),t("table",[t("thead",[t("tr",[t("th",[r._v("类型")]),r._v(" "),t("th",[r._v("架构")]),r._v(" "),t("th",[r._v("Transformer组件")]),r._v(" "),t("th",[r._v("Examples")]),r._v(" "),t("th",[r._v("Tasks")])])]),r._v(" "),t("tbody",[t("tr",[t("td",[t("strong",[r._v("BERT")]),r._v("-like")]),r._v(" "),t("td",[r._v("auto-encoding models")]),r._v(" "),t("td",[r._v("Encoder")]),r._v(" "),t("td",[r._v("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa")]),r._v(" "),t("td",[r._v("Sentence classification, named entity recognition, extractive question answering")])]),r._v(" "),t("tr",[t("td",[t("strong",[r._v("GPT")]),r._v("-like")]),r._v(" "),t("td",[r._v("auto-regressive models")]),r._v(" "),t("td",[r._v("Decoder")]),r._v(" "),t("td",[r._v("CTRL, GPT, GPT-2, Transformer XL")]),r._v(" "),t("td",[r._v("Text generation")])]),r._v(" "),t("tr",[t("td",[t("strong",[r._v("BART/T5")]),r._v("-like")]),r._v(" "),t("td",[r._v("sequence-to-sequence models")]),r._v(" "),t("td",[r._v("Encoder-decoder")]),r._v(" "),t("td",[r._v("BART, T5, Marian, mBART")]),r._v(" "),t("td",[r._v("Summarization, translation, generative question answering")])])])]),r._v(" "),t("hr"),r._v(" "),t("p",[r._v("了解了Transformer一系列模型的来龙去脉，我们就可以更好地玩耍Transformer啦！下一集，我们会慢慢深入查看Huggingface "),t("code",[r._v("transformers")]),r._v("库背后的细节，从而更灵活地使用。")])])}),[],!1,null,null,null);e.default=v.exports}}]);