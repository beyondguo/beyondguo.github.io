(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{458:function(t,s,a){"use strict";a.r(s);var i=a(44),e=Object(i.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"「课代表来了」跟李沐读论文之transformer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#「课代表来了」跟李沐读论文之transformer"}},[t._v("#")]),t._v(" 「课代表来了」跟李沐读论文之Transformer")]),t._v(" "),a("blockquote",[a("p",[t._v("墙裂推荐沐神在B上开的一系列公开课，尤其是最近出的“论文精读”系列，真的讲得太好了。可能很多经典论文咱们之前也读过，但是听李沐老师再讲一遍，我们又可以收获很多新知识，尤其是李沐老师结合自己丰富的经验做出的很多精彩评论。")]),t._v(" "),a("p",[t._v('我也慕名去听了课，第一堂课我选择听NLP经典论文"Attention Is All You Need"，边听边整理了如下笔记 📒 ，方便自己日后查阅，也分享给大家。')])]),t._v(" "),a("blockquote",[a("p",[t._v("李沐老师读论文的顺序一般是这样的：标题+作者 > 摘要 > 导言 > 结论 > 相关工作 > 模型设计 > 实验，最后做一个总评。我这里也是按照这个顺序进行记录的。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211219131751.png",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"标题-作者"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#标题-作者"}},[t._v("#")]),t._v(" 标题+作者")]),t._v(" "),a("p",[t._v("标题：Attention Is All You Need\n发表：NIPS2017\n机构：Google")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211219121954.png",alt:"image-20211219121954021"}})]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 8个作者都是共同贡献，这在计算机论文中很少见。但是为了解释为什么有这么多共同一作，论文在脚注中清楚了写明了每个人在这个工作中的贡献，确实大家都担得上“共同贡献”这一说。")])]),t._v(" "),a("h2",{attrs:{id:"摘要"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[t._v("#")]),t._v(" 摘要")]),t._v(" "),a("p",[t._v("背景是机器翻译、encoder-decoder框架，而且一般都会在中间使用Attention机制。")]),t._v(" "),a("p",[t._v("本文提出了一个新的简单的网络架构——Transformer，完全基于attention机制。")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 这是个好名字，跟“变形金刚”表达相同")])]),t._v(" "),a("p",[t._v("贡献：网络简单，且跟之前的网络结构都不一样，不使用rnn或cnn的单元。并行度更好，训练快很多。在机器翻译上取得了更好的效果。")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 最开始Transformer是在机器翻译的背景上提出的，这是个小领域，因此可能最初并没有收到广泛关注。后面Transformer开始在各种其他领域大显神通，到最近在vision上面也表现优异，才真正火出圈了")])]),t._v(" "),a("h2",{attrs:{id:"结论"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#结论"}},[t._v("#")]),t._v(" 结论")]),t._v(" "),a("p",[t._v("本文提出的Transformer是第一个纯基于attention的序列转录模型，使用multi-head self-attention替代了之前的rnn结构。")]),t._v(" "),a("p",[t._v("在机器翻译上，比RNN和CNN都要快，还取得了新的SOTA。")]),t._v(" "),a("p",[t._v("作者们对这种纯attention的模型能取得这么好的效果感到非常激动。因此作者们期待Transformer能在其他领域（CV，audio等）大放异彩。")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 作者在2017年的时候就一定程度上预测了未来。如今Transformer不光横扫了NLP的各种任务，还在CV上也取得了惊人的效果。")])]),t._v(" "),a("p",[t._v("最后代码开源在tensor2tensor库里面。")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 其实代码最好是放在摘要的最后一句话，让读者可以第一时间看到代码。")])]),t._v(" "),a("h2",{attrs:{id:"intro-导言"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-导言"}},[t._v("#")]),t._v(" Intro（导言）")]),t._v(" "),a("p",[t._v("介绍了传统的RNN，CNN以及encoder-decoder架构。分析了RNN的缺点：1. 难以并行 2. 容易遗忘。再介绍了attention机制。最后提出了一个全新的架构Transformer。")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu:  这个intro很短，主要是内容比较多，而NIPS的篇幅较短。")])]),t._v(" "),a("h2",{attrs:{id:"background-相关工作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#background-相关工作"}},[t._v("#")]),t._v(" Background（相关工作）")]),t._v(" "),a("p",[t._v("为了提高对序列数据的计算效率，很多工作都使用卷积神经网络作为基础的building block来进行模型构建，从而实现并行的计算。然而，CNN是通过滑动窗口来提取特征，所以对于长距离的关系较难捕捉。但CNN还有一个优点——"),a("strong",[t._v("多通道机制")]),t._v("，使得模型可以从多个角度去提取数据的特征。")]),t._v(" "),a("p",[t._v("所以"),a("strong",[t._v("Transformer借用了多通道的思想，设计了多头的注意力机制")]),t._v("。")]),t._v(" "),a("p",[t._v("另外，self-attention不是本工作提出了，而是在曾经的很多工作中都被成功应用了。")]),t._v(" "),a("h2",{attrs:{id:"模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型"}},[t._v("#")]),t._v(" 模型")]),t._v(" "),a("p",[t._v("序列模型中较好的是encoder-decoder架构。")]),t._v(" "),a("p",[t._v("要点：")]),t._v(" "),a("ol",[a("li",[t._v("encoder把输入序列处理得到中间表示，然后decoder读入这个中间表示，处理后得到输出序列；")]),t._v(" "),a("li",[t._v("输入序列和输出序列不一定一样长；")]),t._v(" "),a("li",[t._v("decoder是一种auto-regressive的方式来输出的，即每一步都会读入上一步的输出。")])]),t._v(" "),a("p",[t._v("Transformer依然是一个encoder-decoder的架构，但它主要组成是"),a("strong",[t._v("self-attention")]),t._v("和"),a("strong",[t._v("point-wise fully connected layer")]),t._v("，结构如下：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129211929.png",alt:"image-20211129211929634"}})]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu:  这个图画的很好，在神经网络的时代，画图是一个重要的技能。然而这个图属于那种“不明觉厉”的图，很好看，但是不容易看懂。")])]),t._v(" "),a("h3",{attrs:{id:"encoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[t._v("#")]),t._v(" Encoder")]),t._v(" "),a("ul",[a("li",[t._v("Encoder由N=6个一模一样的层组成；")]),t._v(" "),a("li",[t._v("每个层，包含2个子层：①multi-head self-attention layer，②position-wise fully connected feed-forward network （就是个MLP）；")]),t._v(" "),a("li",[t._v("每个子层，都会使用residual connection和layer norm来处理，子层的输出都可以表示为："),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("L")]),a("mi",[t._v("a")]),a("mi",[t._v("y")]),a("mi",[t._v("e")]),a("mi",[t._v("r")]),a("mi",[t._v("N")]),a("mi",[t._v("o")]),a("mi",[t._v("r")]),a("mi",[t._v("m")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("x")]),a("mo",[t._v("+")]),a("mi",[t._v("S")]),a("mi",[t._v("u")]),a("mi",[t._v("b")]),a("mi",[t._v("l")]),a("mi",[t._v("a")]),a("mi",[t._v("y")]),a("mi",[t._v("e")]),a("mi",[t._v("r")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("X")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("LayerNorm(x+Sublayer(X))")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("L")]),a("span",{staticClass:"mord mathnormal"},[t._v("a")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("yer")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.10903em"}},[t._v("N")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("or")]),a("span",{staticClass:"mord mathnormal"},[t._v("m")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("x")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("S")]),a("span",{staticClass:"mord mathnormal"},[t._v("u")]),a("span",{staticClass:"mord mathnormal"},[t._v("b")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),a("span",{staticClass:"mord mathnormal"},[t._v("a")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("yer")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),a("span",{staticClass:"mclose"},[t._v("))")])])])]),t._v("；")]),t._v(" "),a("li",[t._v("为了方便残差连接，上面所有的层、包括embedding层，都使用d=512作为输出维度。")])]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu:  总之，Encoder就俩超参数：N和d。这种设计直接影响了后面各种基于Transformer的模型设计，比如BERT，GPT等等，都主要调节这两个参数。")])]),t._v(" "),a("h4",{attrs:{id:"「插播」沐神小课堂-什么是layer-norm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#「插播」沐神小课堂-什么是layer-norm"}},[t._v("#")]),t._v(" 「插播」沐神小课堂——什么是Layer Norm：")]),t._v(" "),a("ul",[a("li",[t._v("Batch Norm就是把一个batch的tensor，按照feature的每个维度（即按照列）去进行规范化（均值0方差1）")]),t._v(" "),a("li",[t._v("Layer Norm则是在batch内逐个样本去做规范化")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129220224.png",alt:"image-20211129220224178"}})]),t._v(" "),a("p",[t._v("我们在序列化数据中更常使用的是Layer Norm，因为序列的长度会变化，如果使用batch norm的话，可能导致均值方差波动很大，从而影响效果，而layer norm则是逐个样本去进行的，就不会受影响。")]),t._v(" "),a("h3",{attrs:{id:"decoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#decoder"}},[t._v("#")]),t._v(" Decoder")]),t._v(" "),a("ul",[a("li",[t._v("跟Encoder一样由N=6个一模一样的层构成；")]),t._v(" "),a("li",[t._v("每个层，包含3个子层，相比于Encoder中的设计，多了一个multi-head attention layer；")]),t._v(" "),a("li",[t._v("为了防止Decoder在处理时看到未来的信息，这里对self-attention做了进一步的处理，即使用了一个"),a("strong",[t._v("mask")]),t._v("机制，在t时刻时把后面的单元都mask掉，从而不会attend到未来的信息。")])]),t._v(" "),a("h2",{attrs:{id:"逐个看看每个sub-layer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#逐个看看每个sub-layer"}},[t._v("#")]),t._v(" 逐个看看每个sub-layer：")]),t._v(" "),a("h3",{attrs:{id:"scaled-dot-product-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scaled-dot-product-attention"}},[t._v("#")]),t._v(" Scaled Dot-product Attention")]),t._v(" "),a("p",[t._v("在Transformer中我们使用的attention机制是Scaled Dot-product Attention，下图中的"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("d")]),a("mi",[t._v("k")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_k")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])])]),t._v("代表的Q，K，V的维度：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129221807.png",alt:"image-20211129221807714"}})]),t._v(" "),a("p",[t._v("这里的attention机制，相比于经典的Dot-product Attention其实就是多了一个scale项。这里的作用是啥呢？当d比较小的时候，要不要scale都无所谓，但是当d比较大时，内积的值的范围就会变得很大，不同的内积的差距也会拉大，这样的话，再经过softmax进一步的扩大差距，就会使得得到的attention分布很接近one-hot，这样会导致梯度下降困难，模型难以训练。在Transformer中，d=512，算比较大了，因此需要进行scaling。")]),t._v(" "),a("p",[t._v("下图很清晰地展示了scaled Dot-product Attention是如何进行的：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129224528.png",alt:"image-20211129224528855"}})]),t._v(" "),a("h3",{attrs:{id:"multi-head-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#multi-head-attention"}},[t._v("#")]),t._v(" Multi-Head Attention")]),t._v(" "),a("p",[t._v("原本的SDP Attention，没什么可学习的参数，作者发现，我们可以先把原本的向量，通过线性层隐射到多个低维的空间，然后再并行地进行SDP Attention操作，在concat起来，可以取得更好的效果。这类似于CNN中的多通道机制。一个向量先隐射成多个更低维的向量，相当于分成了多个视角，然后每个视角都去进行Attention，这样模型的学习能力和潜力就会大大提升，另外由于这里的降维都是参数化的，所以让模型可以根据数据来学习最有用的视角。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211129225233.png",alt:"image-20211129225233453"}})]),t._v(" "),a("h3",{attrs:{id:"point-wise-feed-forward-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#point-wise-feed-forward-networks"}},[t._v("#")]),t._v(" Point-wise Feed-forward Networks")]),t._v(" "),a("p",[t._v("这里其实就是名字比较花哨，实际上就是简单的MLP。唯一需要注意的是这个MLP的修饰词——Point-wise，它的意思是它是对每个position（词）都分开、独立地处理。我之前画过一个图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-8/1623129452764-image.png",alt:"point-wise feed-forward示意图"}})]),t._v(" "),a("p",[t._v("即MLP只是作用于最后一个维度，具体公式是：")]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mi",[t._v("F")]),a("mi",[t._v("F")]),a("mi",[t._v("N")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("x")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("r")]),a("mi",[t._v("e")]),a("mi",[t._v("l")]),a("mi",[t._v("u")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("x")]),a("msub",[a("mi",[t._v("W")]),a("mn",[t._v("1")])],1),a("mo",[t._v("+")]),a("msub",[a("mi",[t._v("b")]),a("mn",[t._v("1")])],1),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("msub",[a("mi",[t._v("W")]),a("mn",[t._v("2")])],1),a("mo",[t._v("+")]),a("msub",[a("mi",[t._v("b")]),a("mn",[t._v("2")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("FFN(x) = relu(xW_1 + b_1)W_2 +b_2\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.10903em"}},[t._v("FFN")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("x")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("re")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),a("span",{staticClass:"mord mathnormal"},[t._v("u")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("x")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.30110799999999993em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.13889em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("1")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("b")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.30110799999999993em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("1")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.30110799999999993em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.13889em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("b")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.30110799999999993em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])])])])]),t._v(" "),a("p",[t._v("沐神后面也画图总结了一下：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211215101654.png",alt:"image-20211215101654477"}})]),t._v(" "),a("p",[t._v("这个图左边是Transformer示意图，右边是RNN示意图（其中对RNN单元简化成一个MLP，本质上类似）。")]),t._v(" "),a("ul",[a("li",[t._v("Transformer是通过attention来全局地聚合序列的信息，然后通过MLP进行语义空间的转换；")]),t._v(" "),a("li",[t._v("RNN则是通过把上一时刻的信息传入下一时刻的单元，来使用序列信息，但也是通过MLP进行语义空间转换。所以二者本质区别在于如何使用序列的信息。")])]),t._v(" "),a("h3",{attrs:{id:"embedding和softmax层"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#embedding和softmax层"}},[t._v("#")]),t._v(" Embedding和softmax层")]),t._v(" "),a("p",[t._v("Transformer中使用了三处embedding：对input和output的token进行embedding，以及在softmax前面的Linear transformation中也使用跟embedding相同的权重（这样是为了能够把decoder的output通过相同的embedding给转换回token的概率，因为embedding的作用就是做token跟vector之间的转换）。三处的embedding都是同样的权重。")]),t._v(" "),a("p",[t._v("另外值得注意的点就是，作者把embedding都乘上了"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msqrt",[a("msub",[a("mi",[t._v("d")]),a("mrow",[a("mi",[t._v("m")]),a("mi",[t._v("o")]),a("mi",[t._v("d")]),a("mi",[t._v("e")]),a("mi",[t._v("l")])],1)],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sqrt{d_{model}}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.04em","vertical-align":"-0.18278000000000005em"}}),a("span",{staticClass:"mord sqrt"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.85722em"}},[a("span",{staticClass:"svg-align",staticStyle:{top:"-3em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord",staticStyle:{"padding-left":"0.833em"}},[a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("o")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("e")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-2.81722em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"hide-tail",staticStyle:{"min-width":"0.853em",height:"1.08em"}},[a("svg",{attrs:{width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice"}},[a("path",{attrs:{d:"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"}})])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.18278000000000005em"}},[a("span")])])])])])])]),t._v("。这是为了在后面跟position embedding相乘的时候能够保持差不多的scale。")]),t._v(" "),a("h3",{attrs:{id:"position-encoding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#position-encoding"}},[t._v("#")]),t._v(" Position Encoding")]),t._v(" "),a("p",[t._v("由于self-attention实际上是不考虑序列顺序的，只是单纯把各个position的信息进行聚合，无论顺序怎么改变，对于self-attention来说都是一样的。因此，这里特意设计了position encoding这种东西，来添加位置信息。")]),t._v(" "),a("p",[t._v("具体的，文章使用的是周期不同的sin和cos函数来计算得到每个position的Embedding：")]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mi",[t._v("P")]),a("mi",[t._v("E")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mn",[t._v("2")]),a("mi",[t._v("i")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("s")]),a("mi",[t._v("i")]),a("mi",[t._v("n")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("/")]),a("mn",[t._v("1000")]),a("msup",[a("mn",[t._v("0")]),a("mrow",[a("mn",[t._v("2")]),a("mi",[t._v("i")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("/")]),a("msub",[a("mi",[t._v("d")]),a("mrow",[a("mi",[t._v("m")]),a("mi",[t._v("o")]),a("mi",[t._v("d")]),a("mi",[t._v("e")]),a("mi",[t._v("l")])],1)],1)],1)],1),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mi",[t._v("P")]),a("mi",[t._v("E")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mn",[t._v("2")]),a("mi",[t._v("i")]),a("mo",[t._v("+")]),a("mn",[t._v("1")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("c")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("/")]),a("mn",[t._v("1000")]),a("msup",[a("mn",[t._v("0")]),a("mrow",[a("mn",[t._v("2")]),a("mi",[t._v("i")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("/")]),a("msub",[a("mi",[t._v("d")]),a("mrow",[a("mi",[t._v("m")]),a("mi",[t._v("o")]),a("mi",[t._v("d")]),a("mi",[t._v("e")]),a("mi",[t._v("l")])],1)],1)],1)],1),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("PE(pos, 2i) = sin(pos/10000^{2i/d_{model}}) \\\\\nPE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("PE")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord"},[t._v("2")]),a("span",{staticClass:"mord mathnormal"},[t._v("i")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.188em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("s")]),a("span",{staticClass:"mord mathnormal"},[t._v("in")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")]),a("span",{staticClass:"mord"},[t._v("/1000")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.938em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("i")]),a("span",{staticClass:"mord mtight"},[t._v("/")]),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3448em"}},[a("span",{staticStyle:{top:"-2.3487714285714287em","margin-left":"0em","margin-right":"0.07142857142857144em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.5em"}}),a("span",{staticClass:"sizing reset-size3 size1 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("o")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("e")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15122857142857138em"}},[a("span")])])])])])])])])])])])])]),a("span",{staticClass:"mclose"},[t._v(")")])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("PE")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord"},[t._v("2")]),a("span",{staticClass:"mord mathnormal"},[t._v("i")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.188em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("cos")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")]),a("span",{staticClass:"mord"},[t._v("/1000")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.938em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("i")]),a("span",{staticClass:"mord mtight"},[t._v("/")]),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3448em"}},[a("span",{staticStyle:{top:"-2.3487714285714287em","margin-left":"0em","margin-right":"0.07142857142857144em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.5em"}}),a("span",{staticClass:"sizing reset-size3 size1 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("o")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("e")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15122857142857138em"}},[a("span")])])])])])])])])])])])])]),a("span",{staticClass:"mclose"},[t._v(")")])])])])])]),t._v(" "),a("p",[t._v("其中pos是具体位置的index，i则是具体的dimension。总之，这里给每个位置，都构造了一个长尾"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("d")]),a("mrow",[a("mi",[t._v("m")]),a("mi",[t._v("o")]),a("mi",[t._v("d")]),a("mi",[t._v("e")]),a("mi",[t._v("l")])],1)],1),a("mo",[t._v("=")]),a("mn",[t._v("512")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_{model}=512")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("o")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("e")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("512")])])])]),t._v("的向量，来作为该position也就是某个具体token的位置表示。")]),t._v(" "),a("p",[t._v("最后，这个position encoding是直接跟embedding相加，输入到模型中。")]),t._v(" "),a("h2",{attrs:{id:"为啥使用self-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#为啥使用self-attention"}},[t._v("#")]),t._v(" 为啥使用Self-Attention")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 整个文章实际上对模型的解释是比较欠缺的。")])]),t._v(" "),a("p",[t._v("作者主要通过下面这个表来对比self-attention和其他结构的差别（restricted self-attention不用管，不怎么用）：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211215104354.png",alt:"image-20211215104354876"}})]),t._v(" "),a("p",[t._v("上图中，sequential operations衡量的是在处理序列的时候的并行复杂度，越小说明并行度越高；max path length则表示序列中任意两个点传递信息的最大距离。")]),t._v(" "),a("p",[t._v("Self-attention的主要优势在于并行度高（相比RNN）、信息距离短（相比RNN和CNN）。而在复杂度方面，其实没有明显优势：")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("self-attention每层的复杂度是"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("O")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("msup",[a("mi",[t._v("n")]),a("mn",[t._v("2")])],1),a("mo",[t._v("⋅")]),a("mi",[t._v("d")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(n^2 \\cdot d)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.064108em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("n")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8141079999999999em"}},[a("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("⋅")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，是因为有n个position，每个position都要计算n次attention，在计算attention的时候，是进行维度为d的内积，所以复杂度是n * n * d；")])]),t._v(" "),a("li",[a("p",[t._v("RNN层的复杂度是"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("O")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("n")]),a("mo",[t._v("⋅")]),a("msup",[a("mi",[t._v("d")]),a("mn",[t._v("2")])],1),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(n \\cdot d^2)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("n")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("⋅")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.064108em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8141079999999999em"}},[a("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，因为有n个position，每个position会经过一个线性变化（矩阵乘法），变换的复杂度是d * d，因此最终复杂度是n * d * d；")])]),t._v(" "),a("li",[a("p",[t._v("CNN实际上跟RNN类似，但因为往往涉及到多个kernel，所以多乘了一个k。但一般在NLP中k也不大，所以没有大差别。")])])]),t._v(" "),a("p",[t._v("在NLP中，往往n和d都会比较大，所以这三者的计算复杂度没有质的差别。")]),t._v(" "),a("blockquote",[a("p",[t._v("Comments by Li Mu: 看起来应该Transformer在计算效率上会比CNN、RNN更快，但我们现在体会到的Transformer的模型却不是这样的，为啥呢？实际上是因为self-attention对整个模型的假设更少，所以我们往往需要更大量的数量、更大的模型才能够训练出跟CNN、RNN同样的效果来。这就导致现在基于Transformer的模型都特别大特别贵。")])]),t._v(" "),a("p",[t._v("这里就涉及到inductive bias这个概念了，在「什么是inductive bias」文章中，我们知道合理的inductive bias可以让模型的训练加速，这里由于self-attention的inductive bias相比于CNN、RNN更少，所以训练起来也更费劲。但我猜想，对于大规模预训练来说，少一点inductive bias是不是更好？")]),t._v(" "),a("h2",{attrs:{id:"训练细节-超参数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#训练细节-超参数"}},[t._v("#")]),t._v(" 训练细节&超参数")]),t._v(" "),a("p",[t._v("英语翻译德语，使用BPE分词法构造英语德语共用的词典，使用8个P100 GPU，每个batch大概0.4s，总共训练了12小时，其实时间成本相对来说还是可承受的。")]),t._v(" "),a("p",[t._v("学习率使用了warmup，先增强后减。")]),t._v(" "),a("p",[t._v("使用了两种正则化：")]),t._v(" "),a("ol",[a("li",[t._v("Residual dropout，对每个sub-layer的输出都使用了dropout，还对embedding层也使用dropout，dropout rate=0.1")]),t._v(" "),a("li",[t._v("Label Smoothing，使用了程度为0.1的smoothing，这会损害一点的perplexity，但是nuisance提高accuracy和BLEU得分")])]),t._v(" "),a("p",[t._v("下表则是展示了不同的模型结构（超参数）的性能差别：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211215121451.png",alt:"image-20211215121451571"}})]),t._v(" "),a("p",[t._v("实际上可修改的参数不多，主要就是层数（N）、向量维度（"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("d")]),a("mrow",[a("mi",[t._v("m")]),a("mi",[t._v("o")]),a("mi",[t._v("d")]),a("mi",[t._v("e")]),a("mi",[t._v("l")])],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_{model}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("o")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("e")]),a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])])]),t._v("）、头数（h）。像"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("d")]),a("mi",[t._v("k")])],1),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("msub",[a("mi",[t._v("d")]),a("mi",[t._v("v")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_k, d_v")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.151392em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.03588em"}},[t._v("v")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])])]),t._v("都是根据"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("d")]),a("mrow",[a("mi",[t._v("m")]),a("mi",[t._v("o")]),a("mi",[t._v("d")]),a("mi",[t._v("e")])],1)],1),a("mi",[t._v("l")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_{mode}l")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[a("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("m")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("o")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("e")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])])]),t._v("和h算出来的。")]),t._v(" "),a("h2",{attrs:{id:"沐神点评"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#沐神点评"}},[t._v("#")]),t._v(" 沐神点评")]),t._v(" "),a("h3",{attrs:{id:"写作上"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#写作上"}},[t._v("#")]),t._v(" 写作上")]),t._v(" "),a("p",[t._v("这篇文章写的非常简洁，没有太多的介绍和解释，属于大佬的写作风格。不过对于我们大多数研究者来说，还是需要尽可能把背景解释清楚，在正文花足够的篇幅在把故事讲清楚，从而让读者更好理解、认识的更深入。")]),t._v(" "),a("h3",{attrs:{id:"transformer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[t._v("#")]),t._v(" Transformer")]),t._v(" "),a("p",[t._v("这个模型的最大意义在于给NLP届乃至CV、audio等其他模态的数据提供了统一的模型架构，有点类似于CNN刚提出时对CV领域的巨大变革。有了Transformer，以及现在各种基于Transformer的预训练模型、其他模态的成功运用，Transformer对于多模态学习的进步有着深远的意义。")]),t._v(" "),a("p",[t._v("然后，纵使Transformer已经提出了多年，我们对Transformer的真正原理依然缺乏理解。例如，文章的标题Attention is all you need实际上也是不对的，后续的研究者已经证明Transformer的成功对于MLP、residual connection等其他组件也是缺一不可的，attention的作用就是对序列的信息做了聚合，并不是attention一个人起了全部作用。")]),t._v(" "),a("p",[t._v("另外，self-attention相比如RNN、CNN等的优势，可能在于它所作的归纳偏置（inductive bias）更加一般化，所以经过大规模的训练，可以学习到更丰富、更一般化的知识。但代价就是它对数据的关键信息的抓取能力就下降了，我们需要更多的数据更大的模型才能训练出理想的效果。")]),t._v(" "),a("p",[t._v("但Transformer的出现，给了整个AI届新的活力，让我们发现在CNN、RNN统治的时代，我们依然可以设计出新的网络结构发光发热，因此也带动了一系列新的网络架构设计，比如纯MLP的模型等等。")]),t._v(" "),a("hr")])}),[],!1,null,null,null);s.default=e.exports}}]);