(window.webpackJsonp=window.webpackJsonp||[]).push([[65],{437:function(s,t,a){"use strict";a.r(t);var n=a(44),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("blockquote",[a("p",[a("strong",[s._v("「Huggingface🤗NLP笔记系列-第8集」")]),s._v("\nHuggingface初级教程完结撒花！🌸🌼ヽ(°▽°)ノ🌸🌺\n最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的"),a("strong",[s._v("精简+注解版")]),s._v("。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。")])]),s._v(" "),a("ul",[a("li",[s._v("官方教程网址：https://huggingface.co/course/chapter1")]),s._v(" "),a("li",[s._v("本期内容对应网址：https://huggingface.co/course/chapter3/4?fw=pt")]),s._v(" "),a("li",[s._v("本系列笔记的"),a("strong",[s._v("GitHub Notebook(可下载直接运行)")]),s._v("： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP")])]),s._v(" "),a("hr"),s._v(" "),a("h1",{attrs:{id:"更加透明的方式-使用pytorch来微调模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#更加透明的方式-使用pytorch来微调模型"}},[s._v("#")]),s._v(" 更加透明的方式——使用PyTorch来微调模型")]),s._v(" "),a("p",[s._v("这里我们不使用Trainer这个高级API，而是用pytorch来实现。")]),s._v(" "),a("h2",{attrs:{id:"_1-数据集预处理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-数据集预处理"}},[s._v("#")]),s._v(" 1. 数据集预处理")]),s._v(" "),a("p",[s._v("在Huggingface官方教程里提到，在使用pytorch的dataloader之前，我们需要做一些事情：")]),s._v(" "),a("ul",[a("li",[s._v("把dataset中一些不需要的列给去掉了，比如‘sentence1’，‘sentence2’等")]),s._v(" "),a("li",[s._v("把数据转换成pytorch tensors")]),s._v(" "),a("li",[s._v("修改列名 label 为 labels")])]),s._v(" "),a("p",[s._v("其他的都好说，但"),a("strong",[s._v("为啥要修改列名 label 为 labels，好奇怪哦！")]),s._v("\n这里探究一下：")]),s._v(" "),a("p",[s._v('首先，Huggingface的这些transformer Model直接call的时候，接受的标签这个参数是叫"labels"。\n所以不管你使用Trainer，还是原生pytorch去写，最终模型处理的时候，肯定是使用的名为"labels"的标签参数。')]),s._v(" "),a("p",[s._v('但在Huggingface的datasets中，数据集的标签一般命名为"label"或者"label_ids"，那为什么在前两集中，我们没有对标签名进行处理呢？')]),s._v(" "),a("p",[s._v("这一点在transformer的源码"),a("code",[s._v("trainer.py")]),s._v("里找到了端倪：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 位置在def _remove_unused_columns函数里")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Labels may be named label or label_ids, the default data collator handles that.")]),s._v("\nsignature_columns "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label_ids"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("这里提示了， data collator 会负责处理标签问题。然后我又去查看了"),a("code",[s._v("data_collator.py")]),s._v("中发现了一下内容：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataCollatorWithPadding")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__call__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" features"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" List"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("Dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("List"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" Dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"labels"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("del")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label_ids"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"labels"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label_ids"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("del")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"label_ids"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" batch\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("p",[s._v('这就真相大白了：不管数据集中提供的标签名叫"label"，还是"label_ids"，\nDataCollatorWithPadding 都会帮你转换成"labels"，装进batch里，再返回。')]),s._v(" "),a("p",[s._v("前面使用Trainer的时候，DataCollatorWithPadding已经帮我们自动转换了，因此我们不需要操心这个问题。")]),s._v(" "),a("p",[s._v("但这就是让我疑惑的地方：我们使用pytorch来写，其实也不用管这个，因为在pytorch的data_loader里面，有一个"),a("code",[s._v("collate_fn")]),s._v('参数，我们可以把DataCollatorWithPadding对象传进去，也会帮我们自动把"label"转换成"labels"。因此实际上，'),a("strong",[s._v("这应该是教程中的一个小错误，我们不需要手动设计")]),s._v("（前两天在Huggingface GitHub上提了issue，作者回复我确实不用手动设置）。")]),s._v(" "),a("hr"),s._v(" "),a("p",[s._v("下面开始正式使用pytorch来训练：")]),s._v(" "),a("p",[s._v("首先是跟之前一样，我们需要加载数据集、tokenizer，然后把数据集通过map的方式进行预处理。我们还需要定义一个"),a("code",[s._v("data_collator")]),s._v("方便我们后面进行批量化处理模型：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" datasets "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" load_dataset\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" DataCollatorWithPadding\n\nraw_datasets "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" load_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"glue"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"mrpc"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ncheckpoint "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bert-base-uncased"')]),s._v("\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("tokenize_function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("example"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("example"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"sentence1"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" example"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"sentence2"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" truncation"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\ntokenized_datasets "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" raw_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenize_function"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batched"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndata_collator "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" DataCollatorWithPadding"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("p",[s._v("查看一下处理后的dataset：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("column_names"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'idx'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'label'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sentence1'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sentence2'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'token_type_ids'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("huggingface datasets贴心地准备了三个方法："),a("code",[s._v("remove_columns")]),s._v(", "),a("code",[s._v("rename_column")]),s._v(", "),a("code",[s._v("set_format")])]),s._v(" "),a("p",[s._v("来方便我们为pytorch的Dataloader做准备：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("tokenized_datasets "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("remove_columns"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sentence1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sentence2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'idx'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# tokenized_datasets = tokenized_datasets.rename_column('label','labels')  # 实践证明，这一行是不需要的")]),s._v("\ntokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("set_format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'torch'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("column_names"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'label'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'token_type_ids'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("查看一下：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 经过上面的处理，它就可以直接丢进pytorch的Dataloader中了，跟pytorch中的Dataset格式已经一样了")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\nDataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    features: "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'label'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'token_type_ids'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n    num_rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3668")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("定义我们的"),a("strong",[s._v("pytorch dataloaders")]),s._v("：")]),s._v(" "),a("p",[s._v("在pytorch的"),a("code",[s._v("DataLoader")]),s._v("里，有一个"),a("code",[s._v("collate_fn")]),s._v('参数，其定义是："merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset." 我们可以直接把Huggingface的'),a("code",[s._v("DataCollatorWithPadding")]),s._v("对象传进去，用于对数据进行padding等一系列处理：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" DataLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" Dataset\ntrain_dataloader "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" DataLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" shuffle"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" collate_fn"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("data_collator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 通过这里的dataloader，每个batch的seq_len可能不同")]),s._v("\neval_dataloader "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" DataLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'validation'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" collate_fn"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("data_collator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查看一下train_dataloader的元素长啥样")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" batch "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" train_dataloader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("break")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 可见都是长度为72，size=8的batch")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'attention_mask'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" torch.Size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("72")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'input_ids'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" torch.Size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("72")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'token_type_ids'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" torch.Size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("72")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'labels'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" torch.Size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("观察一下经过DataLoader处理后的数据，我们发现，标签那一列的列名，已经从"),a("code",[s._v('"label"')]),s._v("变为"),a("code",[s._v('"labels"')]),s._v("了！")]),s._v(" "),a("h2",{attrs:{id:"_2-模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-模型"}},[s._v("#")]),s._v(" 2. 模型")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AutoModelForSequenceClassification\n\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_labels"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("前面dataloader出来的batch可以直接丢进模型处理：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\nSequenceClassifierOutput"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.7563")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("grad_fn")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("NllLossBackward"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("logits")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2171, -0.4416"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2248, -0.4694"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2440, -0.4664"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2421, -0.4510"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2273, -0.4545"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2339, -0.4515"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2334, -0.4387"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("-0.2362, -0.4601"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("grad_fn")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("AddmmBackward"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("hidden_states")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("None, "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("attentions")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("None"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("h2",{attrs:{id:"定义-optimizer-和-learning-rate-scheduler"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#定义-optimizer-和-learning-rate-scheduler"}},[s._v("#")]),s._v(" 定义 optimizer 和 learning rate scheduler")]),s._v(" "),a("p",[s._v("按道理说，Huggingface这边提供Transformer模型就已经够了，具体的训练、优化，应该交给pytorch了吧。但鉴于Transformer训练时，最常用的优化器就是AdamW，这里Huggingface也直接在"),a("code",[s._v("transformers")]),s._v("库中加入了"),a("code",[s._v("AdamW")]),s._v("这个优化器，还贴心地配备了lr_scheduler，方便我们直接使用。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AdamW"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" get_scheduler\n\noptimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" AdamW"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("parameters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" lr"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nnum_epochs "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v("\nnum_training_steps "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" num_epochs "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_dataloader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# num of batches * num of epochs")]),s._v("\nlr_scheduler "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" get_scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'linear'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# scheduler是针对optimizer的lr的")]),s._v("\n    num_warmup_steps"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    num_training_steps"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("num_training_steps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num_training_steps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[s._v("1377\n")])])]),a("h2",{attrs:{id:"_3-training"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-training"}},[s._v("#")]),s._v(" 3. Training")]),s._v(" "),a("p",[s._v("首先，我们设置cuda device，然后把模型给移动到cuda上：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n\ndevice "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("device"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'cuda'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cuda"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("is_available"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'cpu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("device"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("h2",{attrs:{id:"编写pytorch-training-loops"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写pytorch-training-loops"}},[s._v("#")]),s._v(" 编写pytorch training loops:")]),s._v(" "),a("p",[s._v("这里也很简单，思路就是这样：")]),s._v(" "),a("ol",[a("li",[s._v("for每一个epoch")]),s._v(" "),a("li",[s._v("从dataloader里取出一个个batch")]),s._v(" "),a("li",[s._v("把batch喂给model（先把batch都移动到对应的device上）")]),s._v(" "),a("li",[s._v("拿出loss，进行反向传播backward")]),s._v(" "),a("li",[s._v("分别把optimizer和scheduler都更新一个step")])]),s._v(" "),a("p",[s._v("最后别忘了每次更新都要清空grad，即对optimizer进行zero_grad()操作。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tqdm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" tqdm\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" epoch "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num_epochs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" batch "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" tqdm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_dataloader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 要在GPU上训练，需要把数据集都移动到GPU上：")]),s._v("\n        batch "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("device"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n        loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("loss\n        loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        lr_scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("zero_grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("%"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("██████████"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("459")]),s._v("/459 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("01:5"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("4")]),s._v("<")]),s._v("00:00,  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v(".01it/s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("%"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("██████████"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("459")]),s._v("/459 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("01:5"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("5")]),s._v("<")]),s._v("00:00,  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(".98it/s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("%"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("██████████"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("459")]),s._v("/459 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("01:5"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("5")]),s._v("<")]),s._v("00:00,  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(".96it/s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h2",{attrs:{id:"_4-evaluation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-evaluation"}},[s._v("#")]),s._v(" 4. Evaluation")]),s._v(" "),a("p",[s._v("这里跟train loop还是挺类似的，一些细节见注释即可：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" datasets "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" load_metric\n\nmetric"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" load_metric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"glue"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"mrpc"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" batch "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" eval_dataloader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    batch "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("device"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("no_grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# evaluation的时候不需要算梯度")]),s._v("\n        outputs "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \n    logits "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" outputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("logits\n    predictions "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("logits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" dim"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 由于dataloader是每次输出一个batch，因此我们要等着把所有batch都添加进来，再进行计算")]),s._v("\n    metric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("predictions"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" references"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"labels"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nmetric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("compute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8651960784313726")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'f1'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.9050086355785838")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("hr"),s._v(" "),a("p",[s._v("至此，Huggingface Transformer初级教程就完结撒花了！")]),s._v(" "),a("center",[s._v("🌸🌼ヽ(°▽°)ノ🌸🌺")]),s._v(" "),a("p",[s._v("更高级的教程，Huggingface也还没出😂，所以咱们敬请期待吧！不过，学完了这个初级教程，我们基本是也可以快乐地操作各种各样Transformer-based模型自由玩耍啦！")])],1)}),[],!1,null,null,null);t.default=e.exports}}]);