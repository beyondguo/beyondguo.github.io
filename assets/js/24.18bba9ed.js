(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{395:function(t,_,v){"use strict";v.r(_);var n=v(44),i=Object(n.a)({},(function(){var t=this,_=t.$createElement,v=t._self._c||_;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h1",{attrs:{id:"「杂谈」神经网络中的优化算法"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#「杂谈」神经网络中的优化算法"}},[t._v("#")]),t._v(" 「杂谈」神经网络中的优化算法")]),t._v(" "),v("blockquote",[v("p",[t._v("上一篇文章讲了神经网络的参数初试化，可以看到不同的初始化方法对我们的学习效果有很大影响。本文继续讨论如何进一步"),v("strong",[t._v("优化我们的梯度下降算法")]),t._v("，使得我们的学习更快，学习效果更佳。")])]),t._v(" "),v("p",[t._v("首先，我们惯用的方法是“梯度下降法”，称为Gradient Decent，"),v("strong",[t._v("就是把所有的训练样本丢进去训练一次之后，把W和b更新一次")]),t._v("，然后重复这个过程，具体重复多少次就看我们的“迭代次数”是多少。")]),t._v(" "),v("p",[t._v("【为了下面叙述的方便，我们称“把所有训练样本过一遍”为一个"),v("strong",[t._v("epoch")]),t._v("，另外 gradient decent也就是梯度下降我们就简称为"),v("strong",[t._v("GD")]),t._v("。】")]),t._v(" "),v("p",[t._v("我们传统的GD，也可以称为Batch GD，这个batch就是指一整批，就是指我们所有样本。")]),t._v(" "),v("p",[v("strong",[t._v("今天主要介绍的有如下方法，均是对传统batch gradient decent的改进：")])]),t._v(" "),v("ol",[v("li",[t._v("Mini-batch GD（小批量梯度下降法）")]),t._v(" "),v("li",[t._v("GD with Momentum（动量法）")]),t._v(" "),v("li",[t._v("Adam算法（Momentum与RMSprop算法结合）")])]),t._v(" "),v("h2",{attrs:{id:"一、mini-batch-gd"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#一、mini-batch-gd"}},[t._v("#")]),t._v(" 一、Mini-batch GD")]),t._v(" "),v("p",[t._v("mini-batch是相对于batch来说的，后者我们前面说过，是"),v("strong",[t._v("走完一个epoch才把参数更新一次，这样其实很慢")]),t._v("。")]),t._v(" "),v("p",[t._v("样本数量不多还好，但是样本多了就会明显耗费大量时间，比如5百万样本，这在深度学习里面是很平常的事情，但是使用传统的方法，即batch GD，意味着我们需要把5,000,000个样本跑一遍才能更新一次W和b。")]),t._v(" "),v("p",[t._v("mini-batch就是为了解决这个问题的，我们可以把5百万的样本分成一块一块的，比如1000个样本一块，称为一个mini-batch，这样我们就有5000个mini-batch。我们训练的时候，"),v("strong",[t._v("跑完一个mini-batch就把W和b更新一次，这样的的话，在一个epoch中，我们就已经把参数更新了5000次了")]),t._v("！虽然每一步没有batch GD的一步走的准，但是我5000步加起来，怎么样也比你走一步的效果好的多，因此可以加快训练速度，更快到达最值点。")]),t._v(" "),v("p",[t._v("这就是mini-batch GD方法。")]),t._v(" "),v("p",[v("strong",[t._v("对于不同的mini-batch的大小（size），也有不一样的效果：")])]),t._v(" "),v("ul",[v("li",[t._v("size=样本数 —> Batch GD")]),t._v(" "),v("li",[t._v("size=1 —> "),v("strong",[t._v("Stochastic GD（随机梯度下降）")])])]),t._v(" "),v("p",[t._v("有人可能会问，那"),v("strong",[t._v("size=1")]),t._v("的时候，也就是来一个样本就更新一次，岂不是会更快？")]),t._v(" "),v("p",[v("strong",[t._v("不是的")]),t._v("，它会有两个问题：")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("震动")]),t._v("太剧烈，误差会灰常大，可能根本无法到达最低点")]),t._v(" "),v("li",[t._v("每次只计算一个样本，就**失去了我们前面提到的“Vectorization（矢量化）”**的优势，因此计算效率反而不高")])]),t._v(" "),v("p",[t._v("因此，我们通常取一个中间的值，这样，既利用了Vectorization来加速计算，又可以在一个epoch中多次更新，速度就可以最快。")]),t._v(" "),v("p",[t._v("有意思的是，据吴恩达说，"),v("strong",[t._v("mini-batch size 通常取2的指数")]),t._v("，主要是16，32，64，128，256，512，1024这几个值，因为计算机是二进制，这样的数字计算起来效率会更高一些。")]),t._v(" "),v("p",[t._v("口说无凭，现在我做个实验验证一下是不是mini-batch 更好：")]),t._v(" "),v("p",[v("strong",[t._v("实验条件：")])]),t._v(" "),v("p",[t._v("三层神经网络，learning-rate=0.0007，batch size=300，mini-batch size=64，迭代次数=40000")]),t._v(" "),v("p",[t._v("数据集形状如下：")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599508200-image.png",alt:""}})]),t._v(" "),v("p",[v("strong",[t._v("猜想：")])]),t._v(" "),v("p",[t._v("①mini-batch GD效果要比batch GD更好")]),t._v(" "),v("p",[t._v("②mini-batch GD的cost曲线会比batch波动更大，但是最终cost却更低")]),t._v(" "),v("p",[t._v("实验代码和过程就不放了，直接看我们运行的"),v("strong",[t._v("结果")]),t._v("：")]),t._v(" "),v("p",[v("em",[v("strong",[t._v("batch GD：")])])]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599519345-image.png",alt:""}})]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599524931-image.png",alt:""}})]),t._v(" "),v("p",[v("strong",[t._v("耗时105s，准确率只有0.76，看图明显就知道还没训练好。")])]),t._v(" "),v("p",[t._v("再看***mini-batch GD：***")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599534871-image.png",alt:""}})]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599540334-image.png",alt:""}})]),t._v(" "),v("p",[t._v("哇塞，"),v("strong",[t._v("效果不错！准确率提高到了91.7%，而且从cost曲线上看，确实如我所料有很大的波动，但是最终的cost显著低于batch GD，因此学习的模型就更好。")])]),t._v(" "),v("p",[t._v("出乎我意料的是，**时间居然缩短了一半以上！**一开始我估计时间应该差不多，因为我们迭代的40000次相当于40000个epoch，我前面只是说一个epoch中mini-batch更新的次数更多，没想到居然也更快（因为我觉得一个epoch中的操作变多了，可能会变慢一点）。")]),t._v(" "),v("p",[t._v("想了想，觉得应该是这样："),v("strong",[t._v("因为mini-batch在一个epoch中走的步子多，因此可以迅速地找到“最佳下坡路”")]),t._v("，找到了之后，就跟坐滑滑梯一样，越溜越快，因此比batch GD更快。")]),t._v(" "),v("h2",{attrs:{id:"二、momentum-动量法"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#二、momentum-动量法"}},[t._v("#")]),t._v(" 二、Momentum 动量法")]),t._v(" "),v("p",[t._v("上面的mini-batch GD给了我们一些惊喜了，但是似乎还可以更好，毕竟还有不少点都分类错误。")]),t._v(" "),v("p",[t._v("主要的原因就是因为，"),v("strong",[t._v("使用mini-batch之后，稳定性降低了，在梯度下降的时候会有较为剧烈的振动，这样可能导致在最低点附近瞎晃悠，因此效果会受影响。")])]),t._v(" "),v("p",[t._v("动量法就是解决mini-batch的这个问题的，它让梯度下降不那么抖动。")]),t._v(" "),v("p",[t._v("看看Momentum的参数更新过程：")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599551513-image.png",alt:""}})]),t._v(" "),v("p",[t._v("如果你熟悉神经网络梯度下降的过程，就知道，我们"),v("strong",[t._v("一般的梯度下降的更新过程（以W为例）是：W = W -αdW。")])]),t._v(" "),v("p",[t._v("动量法相当于多了一个V_dW，"),v("strong",[t._v("它考虑了前面若干个dW")]),t._v("，（实际上，V_dW约等于前1/(1-β)个dW的平均值，数学上称为**“指数加权平均”**）这样，dW的方向就会受到前面若干个dW的冲击，于是整体就变得更平缓。")]),t._v(" "),v("p",[t._v("可能画个示意图才能更好地说明吧：")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599562112-image.png",alt:""}})]),t._v(" "),v("p",[t._v("mini-batch是上下起伏不定的箭头，但是"),v("strong",[t._v("把若干个的方向平均一下，就变得平缓多了")]),t._v("，相当于抵消掉了很多的方向相反的误差。")]),t._v(" "),v("p",[t._v("我们也在做实验验证一下，实验条件跟上面一样，我们在mini-batch size=64的基础上，使用Momentum方法来更新参数，得到的效果如下：")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599571731-image.png",alt:""}})]),t._v(" "),v("p",[t._v("这个超参数β调了我半天，最开始0.9，然后0.95，再0.99，再0.999，终于有了上面的这么一点点的提升，"),v("strong",[t._v("准确率到了92%")]),t._v("。可见momentum有一些效果，但是此处还不太明显。")]),t._v(" "),v("h2",{attrs:{id:"三、adam算法"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#三、adam算法"}},[t._v("#")]),t._v(" 三、Adam算法")]),t._v(" "),v("p",[t._v("这个方法，"),v("strong",[t._v("对momentum再进一步改进，结合了RMSprop算法")]),t._v("（是另一种减小梯度下降振动的方法），更新过程如下：")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599580375-image.png",alt:""}})]),t._v(" "),v("p",[t._v("不仅有了我们刚刚的"),v("strong",[t._v("V_dW")]),t._v(", 还有一个"),v("strong",[t._v("S_dW")]),t._v("(就把它理解为跟V_dW的原理类似就行了)，然后，再对这两个玩意儿都进行了一个"),v("strong",[t._v("修正（corrected）")]),t._v("，最后参数更新是综合了这二者，结果效果就出奇的好：")]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599587617-image.png",alt:""}})]),t._v(" "),v("p",[v("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-25/1624599593343-image.png",alt:""}})]),t._v(" "),v("p",[v("strong",[t._v("准确率有了明显提升，达到94%")]),t._v("，拟合的形状明显是那么一回事了。")]),t._v(" "),v("p",[t._v("对于Momentum的效果不那么明显的现在，吴恩达的解释是"),v("strong",[t._v("在learning-rate太小以及数据集比较简单的情况下，momentum发挥不了太大的作用")]),t._v("，因此本实验中我们看不出惊人的效果。但在其他场景下也许就有很好的改善了。\n当然，既然有了Adam算法，我们自然会使用"),v("strong",[t._v("Adam这个最佳实践")]),t._v("了。")]),t._v(" "),v("h2",{attrs:{id:"总结一下"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结一下"}},[t._v("#")]),t._v(" 总结一下：")]),t._v(" "),v("ul",[v("li",[t._v("Mini-batch GD比传统GD效果更好，训练更快")]),t._v(" "),v("li",[t._v("Momentum动量法可以减小Mini-batch带来的振动")]),t._v(" "),v("li",[t._v("梯度下降的最佳优化方法是Adam算法")]),t._v(" "),v("li",[t._v("Adam算法中的"),v("strong",[t._v("超参数β1和β2以及learning-rate也会显著影响模型")]),t._v("，因此需要我们反复调试")])])])}),[],!1,null,null,null);_.default=i.exports}}]);