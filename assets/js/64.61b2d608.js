(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{433:function(t,s,a){"use strict";a.r(s);var n=a(44),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("blockquote",[a("p",[a("strong",[t._v("「Huggingface🤗NLP笔记系列-第7集」")]),t._v("\n最近跟着Huggingface上的NLP tutorial走了一遍，惊叹居然有如此好的讲解Transformers系列的NLP教程，于是决定记录一下学习的过程，分享我的笔记，可以算是官方教程的"),a("strong",[t._v("精简+注解版")]),t._v("。但最推荐的，还是直接跟着官方教程来一遍，真是一种享受。")])]),t._v(" "),a("ul",[a("li",[t._v("官方教程网址：https://huggingface.co/course/chapter1")]),t._v(" "),a("li",[t._v("本期内容对应网址：https://huggingface.co/course/chapter3/3?fw=pt")]),t._v(" "),a("li",[t._v("本系列笔记的"),a("strong",[t._v("GitHub")]),t._v("： https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP")])]),t._v(" "),a("hr"),t._v(" "),a("h1",{attrs:{id:"使用trainer-api来微调模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#使用trainer-api来微调模型"}},[t._v("#")]),t._v(" 使用Trainer API来微调模型")]),t._v(" "),a("h2",{attrs:{id:"_1-数据集准备和预处理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-数据集准备和预处理"}},[t._v("#")]),t._v(" 1. 数据集准备和预处理：")]),t._v(" "),a("p",[t._v("这部分就是回顾上一集的内容：")]),t._v(" "),a("ul",[a("li",[t._v("通过dataset包加载数据集")]),t._v(" "),a("li",[t._v("加载预训练模型和tokenizer")]),t._v(" "),a("li",[t._v("定义Dataset.map要使用的预处理函数")]),t._v(" "),a("li",[t._v("定义DataCollator来用于构造训练batch")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" DataCollatorWithPadding\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" datasets\ncheckpoint "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bert-base-cased'")]),t._v("\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nraw_datasets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glue'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mrpc'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("tokenize_function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sample"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sample"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sample"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truncation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokenized_datasets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenize_function"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batched"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndata_collator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataCollatorWithPadding"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("h2",{attrs:{id:"_2-加载我们要fine-tune的模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-加载我们要fine-tune的模型"}},[t._v("#")]),t._v(" 2. 加载我们要fine-tune的模型：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoModelForSequenceClassification\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_labels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("warnings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.predictions.bias'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.predictions.transform.dense.weight'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.predictions.transform.dense.bias'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.predictions.decoder.weight'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.seq_relationship.weight'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.seq_relationship.bias'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.predictions.transform.LayerNorm.weight'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cls.predictions.transform.LayerNorm.bias'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n- This IS expected "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(".\n- This IS NOT expected "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" you are initializing BertForSequenceClassification from the checkpoint of a model that you "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("expect")]),t._v(" to be exactly identical "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("initializing a BertForSequenceClassification model from a BertForSequenceClassification model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(".\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'classifier.weight'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'classifier.bias'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nYou should probably TRAIN this model on a down-stream task to be able to use it "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" predictions and inference.\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("不得不说，这个Huggingface很贴心，这里的warning写的很清楚。这里我们使用的是带"),a("code",[t._v("ForSequenceClassification")]),t._v("这个Head的模型，但是我们的"),a("code",[t._v("bert-baed-cased")]),t._v("虽然它本身也有自身的Head，但跟我们这里的二分类任务不匹配，所以可以看到，它的Head被移除了，使用了一个随机初始化的"),a("code",[t._v("ForSequenceClassification")]),t._v("Head。")]),t._v(" "),a("p",[t._v('所以这里提示还说："You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."')]),t._v(" "),a("h2",{attrs:{id:"_3-使用trainer来训练"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-使用trainer来训练"}},[t._v("#")]),t._v(" 3. 使用"),a("code",[t._v("Trainer")]),t._v("来训练")]),t._v(" "),a("p",[a("code",[t._v("Trainer")]),t._v("是Huggingface transformers库的一个高级API，可以帮助我们快速搭建训练框架：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainingArguments\n\ntraining_args "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainingArguments"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_dir"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test_trainer'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 指定输出文件夹，没有会自动创建")]),t._v("\n\ntrainer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    eval_dataset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    data_collator"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("data_collator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在定义了tokenizer之后，其实这里的data_collator就不用再写了，会自动根据tokenizer创建")]),t._v("\n    tokenizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("p",[t._v("我们看看"),a("code",[t._v("TrainingArguments")]),t._v("和"),a("code",[t._v("Trainer")]),t._v("的参数都有些啥：")]),t._v(" "),a("ul",[a("li",[t._v("https://huggingface.co/transformers/master/main_classes/trainer.html")]),t._v(" "),a("li",[t._v("https://huggingface.co/transformers/master/main_classes/trainer.html#trainingarguments")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("TrainingArguments"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    overwrite_output_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    do_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    do_eval"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    do_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    evaluation_strategy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainer_utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("EvaluationStrategy "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'no'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    prediction_loss_only"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    per_device_train_batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 默认的batch_size=8")]),t._v("\n    per_device_eval_batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    per_gpu_train_batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    per_gpu_eval_batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    gradient_accumulation_steps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    eval_accumulation_steps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    learning_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    weight_decay"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    adam_beta1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    adam_beta2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.999")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    adam_epsilon"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("08")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    max_grad_norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_train_epochs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 默认跑3轮")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("modeling_utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PreTrainedModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("modules"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("training_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TrainingArguments "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    data_collator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("DataCollator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    eval_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ForwardRef"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'PreTrainedTokenizerBase'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    model_init"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Callable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("modeling_utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PreTrainedModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    compute_metrics"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Callable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainer_utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("EvalPrediction"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    callbacks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Union"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("List"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainer_callback"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TrainerCallback"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NoneType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    optimizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Tuple"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LambdaLR"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 默认会使用AdamW")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nDocstring"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("     \nTrainer "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" a simple but feature"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("complete training "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),t._v(" loop "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" PyTorch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimized "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" 🤗 Transformers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br")])]),a("p",[t._v("可见，这个"),a("code",[t._v("Trainer")]),t._v("把所有训练中需要考虑的参数、设计都包括在内了，我们可以在这里指定训练验证集、data_collator、metrics、optimizer，并通过"),a("code",[t._v("TrainingArguments")]),t._v("来提供各种超参数。")]),t._v(" "),a("p",[t._v("默认情况下，"),a("code",[t._v("Trainer")]),t._v("和"),a("code",[t._v("TrainingArguments")]),t._v("会使用：")]),t._v(" "),a("ul",[a("li",[t._v("batch size=8")]),t._v(" "),a("li",[t._v("epochs = 3")]),t._v(" "),a("li",[t._v("AdamW优化器")])]),t._v(" "),a("p",[t._v("定义好之后，直接使用"),a("code",[t._v(".train()")]),t._v("来启动训练：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("输出：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20210927151316.png",alt:"image-20210927151316532"}})]),t._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("TrainOutput"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("global_step"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1377")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("training_loss")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.35569445984728887")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("metrics")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train_runtime'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("383.0158")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train_samples_per_second'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.595")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'total_flos'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("530185443455520")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epoch'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("然后我们用"),a("code",[t._v("Trainer")]),t._v("来预测：")]),t._v(" "),a("p",[a("code",[t._v("trainer.predict()")]),t._v("函数处理的结果是一个"),a("code",[t._v("named_tuple")]),t._v("（一种可以直接通过key来取值的tuple），类似一个字典，包含三个属性：predictions, label_ids, metrics")]),t._v(" "),a("p",[t._v("注意，这里的三个属性：")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("predictions")]),t._v("实际上就是logits")]),t._v(" "),a("li",[a("code",[t._v("label_ids")]),t._v("不是预测出来的id，而是数据集中自带的ground truth的标签，因此如果输入的数据集中没给标签，这里也不会输出")]),t._v(" "),a("li",[a("code",[t._v("metrics")]),t._v("，也是只有输入的数据集中提供了"),a("code",[t._v("label_ids")]),t._v("才会输出metrics，包括loss之类的指标")])]),t._v(" "),a("p",[t._v("其中"),a("code",[t._v("metrics")]),t._v("中还可以包含我们自定义的字段，我们需要在定义"),a("code",[t._v("Trainer")]),t._v("的时候给定"),a("code",[t._v("compute_metrics")]),t._v("参数。")]),t._v(" "),a("p",[t._v("文档参考： https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer.predict")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("predictions "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'validation'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# logits")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# array([[-2.7887206,  3.1986978],")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#       [ 2.5258656, -1.832253 ], ...], dtype=float32)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("metrics"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("输出：")]),t._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("51")]),t._v("/51 00:03"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("408")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("408")]),t._v(","),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eval_loss'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7387174963951111")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eval_runtime'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.2872")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eval_samples_per_second'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("124.117")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br")])]),a("p",[t._v("然后就可以用preds和labels来计算一些相关的metrics了。")]),t._v(" "),a("p",[t._v("Huggingface "),a("code",[t._v("datasets")]),t._v("里面可以直接导入跟数据集相关的metrics：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" datasets "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" load_metric\n\npreds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmetric "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_metric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glue'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mrpc'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmetric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" references"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8455882352941176")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f1'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8911917098445595")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("p",[t._v("看看这里的metric（glue type）的文档：")]),t._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("Args:\n    predictions: list of predictions to score.\n        Each translation should be tokenized into a list of tokens.\n    references: list of lists of references "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" each translation.\n        Each reference should be tokenized into a list of tokens.\nReturns: depending on the GLUE subset, one or several of:\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"accuracy"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" Accuracy\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"f1"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" F1 score\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pearson"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" Pearson Correlation\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spearmanr"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" Spearman Correlation\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"matthews_correlation"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" Matthew Correlation\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("h2",{attrs:{id:"_4-构建trainer中的compute-metrics函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-构建trainer中的compute-metrics函数"}},[t._v("#")]),t._v(" 4.构建"),a("code",[t._v("Trainer")]),t._v("中的"),a("code",[t._v("compute_metrics")]),t._v("函数")]),t._v(" "),a("p",[t._v("前面我们注意到"),a("code",[t._v("Trainer")]),t._v("的参数中，可以提供一个"),a("code",[t._v("compute_metrics")]),t._v("函数，用于输出我们希望有的一些指标。")]),t._v(" "),a("p",[t._v("这个"),a("code",[t._v("compute_metrics")]),t._v("有一些输入输出的要求：")]),t._v(" "),a("ul",[a("li",[t._v("输入：是一个"),a("code",[t._v("EvalPrediction")]),t._v("对象，是一个named tuple，需要有至少"),a("code",[t._v("predictions")]),t._v("和"),a("code",[t._v("label_ids")]),t._v("两个字段；经过查看源码，这里的predictions，"),a("strong",[t._v("就是logits")])]),t._v(" "),a("li",[t._v("输出：一个字典，包含各个metrics和对应的数值。")])]),t._v(" "),a("p",[t._v("源码地址： https://huggingface.co/transformers/master/_modules/transformers/trainer.html#Trainer")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" datasets "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" load_metric\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("compute_metrics")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("eval_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    metric "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_metric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"glue"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mrpc"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    logits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" eval_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eval_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label_ids\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 上一行可以直接简写成：")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# logits, labels = eval_preds  因为它相当于一个tuple")]),t._v("\n    predictions "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" metric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" references"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("h3",{attrs:{id:"总结一下这个过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结一下这个过程"}},[t._v("#")]),t._v(" 总结一下这个过程：")]),t._v(" "),a("ul",[a("li",[t._v("首先我们定义了一个"),a("code",[t._v("compute_metrics")]),t._v("函数，交给"),a("code",[t._v("Trainer")]),t._v("；")]),t._v(" "),a("li",[a("code",[t._v("Trainer")]),t._v("训练模型，模型会对样本计算，产生 predictions (logits)；")]),t._v(" "),a("li",[a("code",[t._v("Trainer")]),t._v("再把 predictions 和数据集中给定的 label_ids 打包成一个对象，发送给"),a("code",[t._v("compute_metrics")]),t._v("函数；")]),t._v(" "),a("li",[a("code",[t._v("compute_metrics")]),t._v("函数计算好相应的 metrics 然后返回。")])]),t._v(" "),a("h2",{attrs:{id:"看看带上了-compute-metrics-之后的训练"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#看看带上了-compute-metrics-之后的训练"}},[t._v("#")]),t._v(" 看看带上了 compute_metrics 之后的训练：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("training_args "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainingArguments"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_dir"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test_trainer'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" evaluation_strategy"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epoch'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_labels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# new model")]),t._v("\ntrainer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    eval_dataset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenized_datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    data_collator"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("data_collator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在定义了tokenizer之后，其实这里的data_collator就不用再写了，会自动根据tokenizer创建")]),t._v("\n    tokenizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    compute_metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("compute_metrics\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br")])]),a("p",[t._v("输出：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20210927151222.png",alt:"image-20210927151216236"}})]),t._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("TrainOutput"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("global_step"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1377")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("training_loss")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.32063739751678666")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("metrics")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train_runtime'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("414.1719")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train_samples_per_second'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.325")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'total_flos'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("530351810395680")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epoch'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("可见，带上了"),a("code",[t._v("compute_metircs")]),t._v("函数之后，在Trainer训练过程中，会把增加的metric也打印出来，方便我们时刻了解训练的进展。")])])}),[],!1,null,null,null);s.default=e.exports}}]);